
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>第03章：深入浅出ML之Tree-Based家族 | 计算广告与机器学习</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="ZhouYong">
    

    
    <meta name="description" content="author: zhouyongsdzh@foxmail.com
date: 2015-09-15
sina weibo: @周永_52ML


内容列表

写在前面
决策树学习过程
特征选择方法
决策树解读
分类与回归树
随机森林


写在前面
本章我想以一个例子作为直观引入，来介绍决策树的结构、学习过程以及具体方法在学习过程中的差异。（注：构造下面的成绩示例数据，来说明决策树的构造过程）

假">
<meta property="og:type" content="article">
<meta property="og:title" content="第03章：深入浅出ML之Tree-Based家族">
<meta property="og:url" content="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/index.html">
<meta property="og:site_name" content="计算广告与机器学习">
<meta property="og:description" content="author: zhouyongsdzh@foxmail.com
date: 2015-09-15
sina weibo: @周永_52ML


内容列表

写在前面
决策树学习过程
特征选择方法
决策树解读
分类与回归树
随机森林


写在前面
本章我想以一个例子作为直观引入，来介绍决策树的结构、学习过程以及具体方法在学习过程中的差异。（注：构造下面的成绩示例数据，来说明决策树的构造过程）

假">
<meta property="og:image" content="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_3_0_1_decision_tree_graph.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_3_1_1_decision_tree_info_gain.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第03章：深入浅出ML之Tree-Based家族">
<meta name="twitter:description" content="author: zhouyongsdzh@foxmail.com
date: 2015-09-15
sina weibo: @周永_52ML


内容列表

写在前面
决策树学习过程
特征选择方法
决策树解读
分类与回归树
随机森林


写在前面
本章我想以一个例子作为直观引入，来介绍决策树的结构、学习过程以及具体方法在学习过程中的差异。（注：构造下面的成绩示例数据，来说明决策树的构造过程）

假">

    
    <link rel="alternative" href="/atom.xml" title="计算广告与机器学习" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="计算广告与机器学习" title="计算广告与机器学习"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="计算广告与机器学习">计算广告与机器学习</a></h1>
				<h2 class="blog-motto">Computational Advertising and Machine Learning</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/home">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:www.52caml.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter3-tree-based-family/" title="第03章：深入浅出ML之Tree-Based家族" itemprop="url">第03章：深入浅出ML之Tree-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-09-28T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-09-28</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#写在前面"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树学习过程"><span class="toc-number">2.</span> <span class="toc-text">决策树学习过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征选择方法"><span class="toc-number">3.</span> <span class="toc-text">特征选择方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#信息增益－ID3"><span class="toc-number">3.1.</span> <span class="toc-text">信息增益－ID3</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#增益率"><span class="toc-number">3.2.</span> <span class="toc-text">增益率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基尼指数"><span class="toc-number">3.3.</span> <span class="toc-text">基尼指数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类与回归树"><span class="toc-number">4.</span> <span class="toc-text">分类与回归树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Next"><span class="toc-number">5.</span> <span class="toc-text">Next</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考资料"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol>
		
		</div>
		
		<ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-09-15</li>
<li>sina weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>决策树学习过程</li>
<li>特征选择方法</li>
<li>决策树解读</li>
<li>分类与回归树</li>
<li>随机森林</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<p>本章我想以一个例子作为直观引入，来介绍决策树的结构、学习过程以及具体方法在学习过程中的差异。（注：构造下面的成绩示例数据，来说明决策树的构造过程）</p>
<blockquote>
<p>假设某次学生的考试成绩，第一列表示学生编号，第2列表示成绩，第3、4列分别划分两个不同的等级。数据如下表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">Score</th>
<th style="text-align:center">等级1</th>
<th style="text-align:center">等级2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">82</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">74</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">68</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">91</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">88</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">53</td>
<td style="text-align:center">较差</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">76</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">62</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">58</td>
<td style="text-align:center">较差</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">97</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">过关</td>
</tr>
</tbody>
</table>
<p>定义划分等级的标准：</p>
<p>“等级1”把数据划分为4个区间：</p>
<table>
<thead>
<tr>
<th style="text-align:center">分数区间</th>
<th style="text-align:center">[90, 100]</th>
<th style="text-align:center">[75, 90)</th>
<th style="text-align:center">[60, 75)</th>
<th style="text-align:center">[0, 60)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">等级1</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">较差</td>
</tr>
</tbody>
</table>
<p>“等级2”的划分 假设这次考试，成绩超过75分算过关；小于75分不过关。得到划分标准如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">分数区间</th>
<th style="text-align:center">\(score \ge 75\)</th>
<th style="text-align:center">\(0 \le score \lt 75\)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">等级2</td>
<td style="text-align:center">过关</td>
<td style="text-align:center">不过关</td>
</tr>
</tbody>
</table>
</blockquote>
<p>我们按照树结构展示出来，如下图所示：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_3_0_1_decision_tree_graph.png" width="600" height="500" alt="Decision Tree"></p>
<p>如果按照“等级1”作为划分标准，取值<code>“优秀”，“良好”，“中等”和“较差”</code>分别对应4个分支，如图4.1所示。由于只有一个划分特征，它对应的是一个单层决策树，亦称作“决策树桩”（Decision Stump）。</p>
<blockquote>
<p>决策树桩的特点是：只有一个非叶节点，或者说它的根节点等于内部节点（我们在下面介绍决策树多层结构时再介绍）。</p>
</blockquote>
<p>“等级1”取值类型是category，而在实际数据中，一些特征取值可能是连续值（如这里的score特征）。如果用决策树模型解决一些回归或分类问题的化，在学习的过程中就需要有将连续值转化为离散值的方法在里面，在特征工程中称为特征离散化。</p>
<blockquote>
<p>在图4.2中，我们把<strong>连续值划分为两个区域</strong>，分别是\(score \ge 75\) 和 \(0 \le score \lt 75\) </p>
</blockquote>
<p>图4.3和图4.4属于CART（Classification and Regression Tree，分类与回归树）模型。<strong>CART假设决策树是二叉树</strong>，根节点和内部节点的特征取值为”是”或”否”，节点的左分支对应”是”，右分支对应“否”，<strong>每一次划分特征选择都会把当前特征对应的样本子集划分到两个区域。</strong></p>
<blockquote>
<p>在CART学习过程中，不论特征原有取值是连续值（如图4.2）或离散值（图4.3，图4.4），也要转化为离散二值形式。</p>
</blockquote>
<p>直观上看，回归树与分类树的区别取决于实际的应用场景（回归问题还是分类问题）以及对应的“Label”取值类型。</p>
<blockquote>
<p><code>Label</code>是连续值，通常对应的是回归树；当<code>Label</code>是category时，对应分类树模型；</p>
</blockquote>
<p>后面会提到，CART学习的过程中最核心的是<strong>通过遍历</strong>选择最优划分特征及对应的特征值。那么二者的区别也体现在具体最优划分特征的方法上。</p>
<p>同样，为了直观了解本节要介绍的内容，这里用一个表格来说明：</p>
<table>
<thead>
<tr>
<th style="text-align:center">决策树算法</th>
<th style="text-align:center">特征选择方法</th>
<th style="text-align:center">作者信息</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ID3</td>
<td style="text-align:center">信息增益</td>
<td style="text-align:center">Quinlan. 1986. <br>(Iterative Dichotomiser 迭代二分器)</td>
</tr>
<tr>
<td style="text-align:center">C4.5</td>
<td style="text-align:center">增益率</td>
<td style="text-align:center">Quinlan. 1993. </td>
</tr>
<tr>
<td style="text-align:center">CART</td>
<td style="text-align:center">回归树： 最小二乘<br> 分类树： 基尼指数</td>
<td style="text-align:center">Breiman. 1984. <br> (Classification and Regression Tree 分类与回归树)</td>
</tr>
</tbody>
</table>
<p>除了具体介绍这3个具体算法对应的特征选择方法外，还会简要的介绍决策树学习过程出现的模型和数据问题，如过拟合问题，连续值和缺失值问题等。</p>
<p><br></p>
<h3 id="决策树学习过程">决策树学习过程</h3><hr>
<p>图4.1~图4.4给出的仅仅是单层决策树，只有一个非叶节点（对应一个特征）。那么对于含有多个特征的分类问题来说，决策树的学习过程通常是一个通过递归选择最优划分特征，并根据该特征的取值情况对训练数据进行分割，使得切割后对应的<strong>数据子集有一个较好的分类</strong>的过程。</p>
<blockquote>
<p>为了更直观的解释决策树的学习过程，这里参考《数据挖掘－实用机器学习技术》一书中P69页提供的天气数据，根据天气情况决定是否出去玩，数据信息如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">ID</th>
<th>阴晴</th>
<th>温度</th>
<th>湿度</th>
<th>刮风</th>
<th style="text-align:center">玩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td>overcast</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td>rainy</td>
<td>mild</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td>overcast</td>
<td>cool</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td>sunny</td>
<td>mild</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td>sunny</td>
<td>cool</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td>rainy</td>
<td>mild</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td>sunny</td>
<td>mild</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td>overcast</td>
<td>mild</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td>overcast</td>
<td>hot</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td>rainy</td>
<td>mild</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
</blockquote>
<p>利用ID3算法中的<strong>信息增益</strong>特征选择方法，递归的学习一棵决策树，得到树结构，如图4.5所示：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_3_1_1_decision_tree_info_gain.png" width="550" height="400" alt="ID3-决策树示意图"></p>
<p>假设 训练数据集\(D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(m)},y^{(m)}) \} \) (特征用离散值表示)，候选特征集合\(F=\{f^1, f^2, \cdots, f^n\} \)。开始，建立根节点，将所有训练数据都置于根节点（\(m\)条样本）。从特征集合\(F\)中选择一个最优特征\(f^{\ast}\)，按照\(f^{\ast}\)取值讲训练数据集切分成若干子集，使得各个自己有一个在当前条件下最好的分类。</p>
<p>如果子集中样本类别基本相同，那么构建叶节点，并将数据子集划分给对应的叶节点；如果子集中样本类别差异较大，不能被基本正确分类，需要在剩下的特征集合（\(F-\{f^{\ast}\}\)）中选择新的最优特征，构建响应的内部节点，继续对数据子集进行切分。如此递归地进行下去，直至所有数据自己都能被基本正确分类，或者没有合适的最优特征为止。</p>
<p>这样最终结果是每个子集都被分到叶节点上，对应着一个明确的类别。那么，递归生成的层级结构即为一棵决策树。我们将上面的文字描述用伪代码形式表达出来，即为：</p>
<p>\(<br>\{ \\<br>\quad输入: 训练数据集D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(m)},y^{(m)}) \} \; (特征用离散值表示); \\<br>\qquad\quad\; 候选特征集F=\{f^1, f^2, \cdots, f^n\} \\<br>\quad输出：一颗决策树T(D,F) \\<br>\quad学习过程：\\<br>\qquad 01. \;\; 创建节点node; \\<br>\quad\;\;\;02. \;\; if \; D中样本全属于同一类别C； \; then \\<br>\qquad 03. \qquad 将node作为叶节点，用类别C标记，返回； \\<br>\qquad 04. \; endif \\<br>\qquad 05. \;\; if \; F为空（F=\emptyset）or \; D中样本在F的取值相同；\; then \\<br>\qquad 06. \qquad 将node作为叶节点，其类别标记为D中样本数最多的类（多数表决），返回； \\<br>\qquad 07. \;\underline{ 选择F中最优特征，得到f^{\ast}(f^{\ast} \in F) }； \\<br>\qquad 08. \;标记节点node为f^{\ast} \\<br>\qquad 09. \;\; for \; f^{\ast} \;中的每一个已知值f_{i}^{\ast}; \; do \\<br>\qquad 10. \quad\;\; 为节点node生成一个分支；令D_i表示D中在特征f^{\ast}上取值为f_i^{\ast}的样本子集； \; //划分子集 \\<br>\qquad 11. \quad\;\; if \; D_i为空；\; then \\<br>\qquad 12. \qquad\quad 将分支节点标记为叶节点，其类别标记为D_i中样本最多的类；\; then \\<br>\qquad 13. \quad\;\; else \\<br>\qquad 14. \qquad\quad 以T(D_i, F-\{f^{\ast}\})为分支节点；\quad // 递归过程 \\<br>\qquad 15. \quad\;\; endif \\<br>\qquad 16. \; done<br>\\\}<br>\)</p>
<p>决策树学习过程中递归的每一步，在选择最优特征后，根据特征取值切割当前节点的数据集，得到若干数据子集。由于决策树学习过程是递归的选择最优特征，因此可以理解为这是一个特征空间划分的过程。每一个特征子空间对应决策树中的一个叶子节点，特征子空间相应的类别就是叶子节点对应数据子集中样本数最多的类别。</p>
<p><br></p>
<h3 id="特征选择方法"><strong>特征选择方法</strong></h3><hr>
<p>上面多次提到递归地选择最优特征，根据特征取值切割数据集，使得对应的数据子集有一个较好的分类。从伪代码中也可以看出，在决策树学习过程中，最重要的是第07行，即如何选择最优特征？也就是我们常说的特征选择选择问题。</p>
<p>顾名思义，特征选择就是将特征的重要程度量化之后再进行选择，而如何量化特征的重要性，就成了各种方法间最大的区别。</p>
<blockquote>
<p>例如卡方检验、斯皮尔曼法（Spearman）、<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/#互信息" target="_blank" rel="external">互信息</a>等使用<code>&lt;feature, label&gt;</code>之间的<strong>关联性</strong>来进行量化<code>feature</code>的重要程度。关联性越强，特征得分越高，该特征越应该被优先选择。</p>
</blockquote>
<p>在这里，希望随着特征选择过程地不断进行，决策树的分支节点所包含的样本尽可能属于<br>同一类别，即希望节点的”纯度（purity）”越来越高。</p>
<blockquote>
<p>如果子集中的样本都属于同一个类别，当然是最好的结果；如果说大多数的样本类型相同，只有少部分样本不同，也可以接受。</p>
</blockquote>
<p>那么如何才能做到选择的特征对应的样本子集纯度最高呢？</p>
<p>ID3算法用<strong>信息增益</strong>来刻画样例集的纯度;C4.5算法采用<strong>增益率</strong>;</p>
<ul>
<li>CART算法采用<strong>基尼指数</strong>来刻画样例集纯度</li>
</ul>
<p><br></p>
<h4 id="信息增益－ID3"><strong>信息增益－ID3</strong></h4><hr>
<p>信息增益（Information Gain，简称IG）衡量特征的重要性是根据<strong>当前特征为划分带来多少信息量，带来的信息越多，该特征就越重要，此时节点的”纯度”也就越高。</strong></p>
<p>分类系统的信息熵，我们在<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/#熵与信息熵" target="_blank" rel="external">第02章：熵与信息熵</a>部分已经给出计算公式，这里再复习一下：</p>
<blockquote>
<p>对一个分类系统来说，假设类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别总数），每一个类别出现的概率分别是\(p(c_1),p(c_2), \cdots, p(c_k)\)。此时，分类系统的熵可以表示为:</p>
<p>$$<br>H(C) = - \sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) \qquad (n.ml.1.3.1)<br>$$</p>
<p>分类系统的作用就是输出一个特征向量（文本特征、ID特征、特征特征等）属于哪个类别的值，而这个值可能是\(c_1, c_2, \cdots, c_k\)，因此这个值所携带的信息量就是公式\((n.ml.1.3.1)\)这么多。</p>
</blockquote>
<p>假设离散特征\(t\)的取值有\(I\)个，\(H(C|t=t_i)\) 表示特征\(t\)被取值为\(t_i\)时的条件熵；\(H(C|t)\)是指特征\(t\)被固定时的条件熵。二者之间的关系是：</p>
<blockquote>
<p>$$<br>\begin{align}<br>H(C|t) &amp; = p_1 \cdot H(C|t=t_1) + p_2 \cdot H(C|t=t_2) + \cdots + p_k \cdot H(C|t=t_{n}) \\<br>&amp; = \sum_{i=1}^{I} p_i \cdot H(C|t=t_i)<br>\end{align}        \quad (n.ml.1.3.2)<br>$$</p>
<p>假设总样本数有\(m\)条，特征\(t=t_i\)时的样本数\(m_i\)，\(p_i=\frac{m_i}{m}\).</p>
</blockquote>
<p>接下来，如何求\(P(C|T=t_i)？\)</p>
<blockquote>
<p>以二分类为例（正例为1，负例为0），总样本数为\(m\)条，特征\(t\)的取值为\(I\)个，其中特征\(t=t_i\)对应的样本数为\(m_i\)条，其中正例\(m_{i1}\)条，负例\(m_{i0}\)条（即\(m_i=m_{i0} + m_{i1}\)）。那么有：</p>
<p>$$<br>\begin{align}<br>P(C|T=t_i) &amp; = - \frac{m_{i1}}{m_i} \cdot log_{2} \frac{m_{i1}}{m_i} - \frac{m_{i0}}{m_i} \cdot log_{2} \frac{m_{i0}}{m_i} \\<br>&amp; = -\sum_{j=0}^{k-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i}<br>\end{align} \qquad (n.ml.1.3.3)<br>$$</p>
<p>这里\(k=2\)表示分类的类别数，公式\(\frac{m_{ij}}{m_i}\)物理含义是当\(t=t_i\)且\(C=c_j\)的概率，即条件概率\(p(c_j|t_i)\)。</p>
<p>因此，条件熵计算公式为：</p>
<p>$$<br>\begin{align}<br>H(C|t) &amp; = \sum_{i=1}^{I} p(t_i) \cdot H(C|t=t_i) \\<br>&amp; = - \sum_{i=1}^{I} p(t_i) \cdot \underline { \sum_{j=0}^{k-1} p(c_j|t_i) \cdot log_2 p(c_j|t_i) } \\<br>&amp; = - \sum_{i=1}^{I} \sum_{j=o}^{k-1} p(c_j,t_i) \cdot log_2 p(c_j|t_i)<br>\end{align} \qquad (n.ml.1.3.4)<br>    $$</p>
</blockquote>
<p>特征\(t\)给系统带来的信息增益等于<strong>系统原有的熵与固定特征\(t\)后的条件熵之差</strong>，公式表示如下:</p>
<p>$$<br>\begin{align}<br>IG(T) &amp; = H(C) - H(C|T) \\<br>&amp; = -\sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) + \sum_{i=1}^{n} \sum_{j=1}^{k} p(c_j,t_i) \cdot \log_2 p(c_j|t_i)<br>\end{align}  \qquad(ml.1.3.1)<br>    $$</p>
<p>\(n表示特征t取值个数，k表示类别C个数，\sum_{j=0}^{n-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i}表示每一个类别对应的熵。\)</p>
<p>下面以天气数据为例，介绍通过信息增益选择最优特征的工作过程：</p>
<blockquote>
<p>根据阴晴、温度、湿度和刮风来决定是否出去玩。样本中总共有14条记录，取值为“是”和“否”的yangebnshu分别是9和5，即9个正样本、5个负样本，用\(S(9+,5-)\)表示，S表示样本(sample)的意思。<br> <br></p>
<p>(1). 分类系统的熵:</p>
<p>$$<br>Entropy(S) = info(9,5) = -\frac{9}{14} _ log_2 (\frac{9}{14}) - \frac{5}{14} _ log_2 (\frac{5}{14}) = 0.940位    \quad(exp.1.3.1)<br>$$</p>
<p>(2). 如果以特征”阴晴”作为根节点。“阴晴”取值为{sunny, overcast, rainy}, 分别对应的正负样本数分别为(2+,3-), (4+,0-), (3+,2-)，那么在这三个节点上的信息熵分别为：</p>
<p>$$<br>\begin{align}<br>&amp; Entropy(S| “阴晴”=sunny) = info(2,3) = 0.971位  \quad(exp.1.3.2.1) \\<br>&amp; Entropy(S| “阴晴”=overcast) = info(4,0) = 0位  \;\;\quad(exp.1.3.2.2) \\<br>&amp; Entropy(S| “阴晴”=rainy) = info(3,2) = 0.971位  \;\quad(exp.1.3.2.3)<br>\end{align}<br>$$</p>
<p>以特“阴晴”为根节点，平均信息值（即条件熵）为：<br></p>
<p>$$<br>Entropy(S| “阴晴”) = \frac{5}{14} _ 0.971 + \frac{4}{14} _ 0 + \frac{5}{14} * 0.971 = 0.693位 \quad (exp.1.3.2)<br>$$</p>
<p>以特征\( “阴晴”\)为条件，计算得到的条件熵代表了期望的信息总量，即对于一个新样本判定其属于哪个类别所必需的信息量。<br><br><br>(3). 计算特征\( “阴晴”\)对应的信息增益:</p>
<p>$$<br>IG( “阴晴”) = Entropy(S) - Entropy(S| “阴晴”) = 0.247位 \quad(exp.1.3.3.1)<br>$$</p>
<p>同样的计算方法，可得每个特征对应的信息增益，即</p>
<p>$$<br>IG(“刮风”) = Entropy(S) - Entropy(S|“刮风”) = 0.048位 \qquad\qquad(exp.1.3.3.2) \\<br>IG(“湿度”) = Entropy(S) - Entropy(S|“湿度”) = 0.152位 \qquad\qquad(exp.1.3.3.3) \\<br>IG(“温度”) = Entropy(S) - Entropy(S|“温度”) = 0.029位 \qquad\qquad(exp.1.3.3.4)<br>$$</p>
</blockquote>
<p>显然，特征“阴晴”的信息增益最大，于是把它作为划分特征。基于“阴晴”对根节点进行划分的结果，如图4.5所示（决策树学习过程部分）。决策树学习算法对子节点进一步划分，重复上面的计算步骤。</p>
<p>用信息增益选择最优特征，并不是完美的，存在问题或缺点主要有以下两个：</p>
<ul>
<li><p><strong>倾向于选择拥有较多取值的特征</strong></p>
<blockquote>
<p>尤其特征集中包含ID类特征时，ID类特征会最先被选择为分裂特征，但在该类特征上的分支对预测未知样本的类别并无意义，降低了决策树模型的泛化能力，也容易使模型易发生过拟合。</p>
</blockquote>
</li>
<li><p><strong>只能考察特征对整个系统的贡献，而不能具体到某个类别上</strong></p>
<blockquote>
<p>信息增益只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而无法做“本地”的特征选择（对于文本分类来讲，每个类别有自己的特征集合，因为有的词项（word item）对一个类别很有区分度，对另一个类别则无足轻重）。</p>
</blockquote>
</li>
</ul>
<p>为了弥补信息增益这一缺点，一个被称为<strong>增益率（Gain Ratio）</strong>的修正方法被用来做最优特征选择。</p>
<p><br>     </p>
<h4 id="增益率"><strong>增益率</strong></h4><hr>
<p>与信息增益不同，信息增益率的计算考虑了<strong>特征分裂数据集后所产生的子节点的数量和规模，而忽略任何有关类别的信息</strong>。</p>
<blockquote>
<p>以信息增益示例为例，按照特征“阴晴”将数据集分裂成3个子集，规模分别为5、4和5，因此不考虑子集中所包含的类别，产生一个分裂信息为：</p>
<p>$$<br>SplitInfo(“阴晴”) = info(5,4,5) = 1.577位 \qquad(exp.1.3.4)<br>$$</p>
<p>分裂信息熵（Split Information）可简单地理解为<strong>表示信息分支所需要的信息量</strong>。 <br><br>那么信息增益率：</p>
<p>$$<br>IG_{ratio}(T) = \frac{IG(T)}{SplitInfo(T)} \qquad(n.ml.1.3.3)<br>$$</p>
<p>在这里，特征 “阴晴”的信息增益率为\(IG_{ratio}( “阴晴”)=\frac{0.247}{1.577} = 0.157\)。减少信息增益方法对取值数较多的特征的影响。</p>
</blockquote>
<p><br></p>
<h4 id="基尼指数"><strong>基尼指数</strong></h4><hr>
<p>基尼指数（Gini Index）是另外一种数据不纯度的度量方法。</p>
<ul>
<li><p>基尼指数定义：</p>
<p>  这里同样以分类系统为例，理解基尼指数的含义：</p>
<blockquote>
<p>假设分类系统中，类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别总数），一个样本属于类别\(c_i\)的概率为\(p(c_i)\)。</p>
</blockquote>
<p>  那么基尼指数公式表示为：</p>
<p>  $$<br>  Gini(C) = 1 - \sum_{i=1}^{k} {p_i}^2    \qquad(ml.1.3.4)<br>  $$</p>
<blockquote>
<p>其中\(p(c_i) = \frac{类别属于c_i的样本数}{总样本数n}\)。可以看出，如果所有的样本都属于同一个类别，则\(p_1=1,Gini(C)=0\)，此时数据不纯度最低。</p>
<p>在下面介绍的CART(Classification And Regression Tree，分类与回归树)算法中利用基尼指数都早二叉决策树，对每个特征都会枚举其特征的非空真子集，以特征 “阴晴”分裂后的基尼指数为：</p>
<p>  $$<br>  Gini_{ “阴晴”}(C) = \frac{|C_1|}{|C|} Gini(C1) + \frac{|C_2|}{|C|} Gini(C_2) + \frac{|C_3|}{|C|} Gini(C_3) \qquad(n.ml.1.3.4)<br>  $$</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h3 id="分类与回归树">分类与回归树</h3><hr>
<p><br></p>
<h3 id="Next">Next</h3><hr>
<ul>
<li>随机森林</li>
</ul>
<p><br></p>
<h3 id="参考资料">参考资料</h3><ul>
<li><a href="http://www.chawenti.com/articles/18892.html" target="_blank" rel="external">http://www.chawenti.com/articles/18892.html</a></li>
<li><a href="http://wenku.baidu.com/link?url=xMRoqqThIY5xC01z9AjL7VtAUYKlHHi7zEMl9gF4b-rbu-cxjLJDfQ0zlnwWT4maOnZp03Q4l1ioaB319vEQWb81YvB8udk9axZacVS26Qi" target="_blank" rel="external">http://wenku.baidu.com/link?url=xMRoqqThIY5xC01z9AjL7VtAUYKlHHi7zEMl9gF4b-rbu-cxjLJDfQ0zlnwWT4maOnZp03Q4l1ioaB319vEQWb81YvB8udk9axZacVS26Qi</a></li>
</ul>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/C4-5/">C4.5</a><a href="/tags/CART/">CART</a><a href="/tags/Decision-Stump/">Decision Stump</a><a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/ID3/">ID3</a><a href="/tags/IG/">IG</a><a href="/tags/Random-Forest/">Random Forest</a><a href="/tags/信息增益/">信息增益</a><a href="/tags/决策树/">决策树</a><a href="/tags/决策树桩/">决策树桩</a><a href="/tags/分类与回归树/">分类与回归树</a><a href="/tags/随机森林/">随机森林</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/" data-title="第03章：深入浅出ML之Tree-Based家族 | 计算广告与机器学习" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/head_first_ml/ml-chapter10-clustering-family/" title="第10章：深入浅出ML之Clustering家族">
  <strong>上一篇：</strong><br/>
  <span>
  第10章：深入浅出ML之Clustering家族</span>
</a>
</div>


<div class="next">
<a href="/stats/beta-gamma-dirichlet-function/"  title="概率与统计-chapter0-三个重要函数">
 <strong>下一篇：</strong><br/> 
 <span>概率与统计-chapter0-三个重要函数
</span>
</a>
</div>

</nav>

	

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#写在前面"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树学习过程"><span class="toc-number">2.</span> <span class="toc-text">决策树学习过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征选择方法"><span class="toc-number">3.</span> <span class="toc-text">特征选择方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#信息增益－ID3"><span class="toc-number">3.1.</span> <span class="toc-text">信息增益－ID3</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#增益率"><span class="toc-number">3.2.</span> <span class="toc-text">增益率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基尼指数"><span class="toc-number">3.3.</span> <span class="toc-text">基尼指数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类与回归树"><span class="toc-number">4.</span> <span class="toc-text">分类与回归树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Next"><span class="toc-number">5.</span> <span class="toc-text">Next</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考资料"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/概率与统计/" title="概率与统计">概率与统计<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/深入浅出机器学习/" title="深入浅出机器学习">深入浅出机器学习<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/连续随机变量/" title="连续随机变量">连续随机变量<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Boosting/" title="Boosting">Boosting<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaBoost/" title="AdaBoost">AdaBoost<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Gradient-Boosting/" title="Gradient Boosting">Gradient Boosting<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/BGDT/" title="BGDT">BGDT<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/XGBoost/" title="XGBoost">XGBoost<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Kernel/" title="Kernel">Kernel<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/对偶优化/" title="对偶优化">对偶优化<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/SVM/" title="SVM">SVM<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/支持向量机/" title="支持向量机">支持向量机<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/KKT/" title="KKT">KKT<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/VC维/" title="VC维">VC维<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Decision-Tree/" title="Decision Tree">Decision Tree<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/决策树/" title="决策树">决策树<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/ID3/" title="ID3">ID3<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/C4-5/" title="C4.5">C4.5<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/信息增益/" title="信息增益">信息增益<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/IG/" title="IG">IG<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/CART/" title="CART">CART<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello, Welcome to CAML technology sharing platform.  <br/>
			I&#39;m Zhou Yong, engaged in algorithms work on computational advertising and machine learning.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1707438033" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/ComputationalAds" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:zhouyongsdzh@foxmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2016 
		
		<a href="/about" target="_blank" title="ZhouYong">ZhouYong</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>









<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
