
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>计算广告与机器学习</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="ZhouYong">
    

    
    <meta name="description" content="基于关键词的召回算法与系统重要技术方向



方向
目标
主要工作
相关指标




查询理解
理解用户查询意图，优化召回与排序结果的相关性
1. Query分析：分词、词权重、纠错、成分分析；




Query理解：意图识别、类目识别、地址识别 | 1. 离线指标：在测试集上的准召率、F1
线上指标：CTR 
关注指标：覆盖率、密度、RPS等；|| 召回模型 | 在保证相关性的前提下，提升召回">
<meta property="og:type" content="article">
<meta property="og:title" content="计算广告与机器学习">
<meta property="og:url" content="http://www.52caml.com/deep_learning/dl-chapter9-recall-algorithm-dev/index.html">
<meta property="og:site_name" content="计算广告与机器学习">
<meta property="og:description" content="基于关键词的召回算法与系统重要技术方向



方向
目标
主要工作
相关指标




查询理解
理解用户查询意图，优化召回与排序结果的相关性
1. Query分析：分词、词权重、纠错、成分分析；




Query理解：意图识别、类目识别、地址识别 | 1. 离线指标：在测试集上的准召率、F1
线上指标：CTR 
关注指标：覆盖率、密度、RPS等；|| 召回模型 | 在保证相关性的前提下，提升召回">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="计算广告与机器学习">
<meta name="twitter:description" content="基于关键词的召回算法与系统重要技术方向



方向
目标
主要工作
相关指标




查询理解
理解用户查询意图，优化召回与排序结果的相关性
1. Query分析：分词、词权重、纠错、成分分析；




Query理解：意图识别、类目识别、地址识别 | 1. 离线指标：在测试集上的准召率、F1
线上指标：CTR 
关注指标：覆盖率、密度、RPS等；|| 召回模型 | 在保证相关性的前提下，提升召回">

    
    <link rel="alternative" href="/atom.xml" title="计算广告与机器学习" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="计算广告与机器学习" title="计算广告与机器学习"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="计算广告与机器学习">计算广告与机器学习</a></h1>
				<h2 class="blog-motto">Computational Advertising and Machine Learning</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/home">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:www.52caml.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/deep_learning/dl-chapter9-recall-algorithm-dev/" title="" itemprop="url"></a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2020-08-28T14:02:13.000Z" itemprop="datePublished"> 发表于 2020-08-28</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#基于关键词的召回算法与系统"><span class="toc-number">1.</span> <span class="toc-text">基于关键词的召回算法与系统</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0.召回算法系统综述"><span class="toc-number">2.</span> <span class="toc-text">召回算法系统综述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0.1.召回的数学模型"><span class="toc-number">2.1.</span> <span class="toc-text">召回的数学模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0.2.召回算法逻辑思路"><span class="toc-number">2.2.</span> <span class="toc-text">召回算法逻辑思路</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1.查询理解"><span class="toc-number">3.</span> <span class="toc-text">查询理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1.1.Query分析"><span class="toc-number">3.1.</span> <span class="toc-text">Query分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1.2.意图识别"><span class="toc-number">3.2.</span> <span class="toc-text">Query理解</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2.查询改写"><span class="toc-number">4.</span> <span class="toc-text">查询改写</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2.9.改写词全局优化"><span class="toc-number">4.1.</span> <span class="toc-text">改写词全局优化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3.索引挖掘"><span class="toc-number">5.</span> <span class="toc-text">索引挖掘</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3.1.知识图谱"><span class="toc-number">5.1.</span> <span class="toc-text">知识图谱</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3.1.1.知识图谱应用"><span class="toc-number">5.1.1.</span> <span class="toc-text">知识图谱应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3.2.索引词优化"><span class="toc-number">5.2.</span> <span class="toc-text">索引词优化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4.召回匹配"><span class="toc-number">6.</span> <span class="toc-text">召回匹配</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4.1.精确匹配"><span class="toc-number">6.1.</span> <span class="toc-text">精确匹配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4.2.相关性匹配"><span class="toc-number">6.2.</span> <span class="toc-text">相关性匹配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5.2.向量召回"><span class="toc-number">6.3.</span> <span class="toc-text">向量召回</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6.相关性与粗排"><span class="toc-number">7.</span> <span class="toc-text">相关性与粗排</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6.1.相关性模型"><span class="toc-number">7.1.</span> <span class="toc-text">相关性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6.1.1.文本及语义相关性"><span class="toc-number">7.1.1.</span> <span class="toc-text">文本及语义相关性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6.1.2.业务相关性"><span class="toc-number">7.1.2.</span> <span class="toc-text">业务相关性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6.1.3.相关性模型"><span class="toc-number">7.1.3.</span> <span class="toc-text">相关性模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6.2.粗排模块"><span class="toc-number">7.2.</span> <span class="toc-text">粗排模块</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9.工程建设"><span class="toc-number">8.</span> <span class="toc-text">工程建设</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9.1.索引系统"><span class="toc-number">8.1.</span> <span class="toc-text">索引系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9.2.工具积累"><span class="toc-number">8.1.1.</span> <span class="toc-text">工具积累</span></a></li></ol></li></ol></li></ol>
		
		</div>
		
		<h1 id="基于关键词的召回算法与系统">基于关键词的召回算法与系统</h1><p>重要技术方向</p>
<table>
<thead>
<tr>
<th>方向</th>
<th>目标</th>
<th>主要工作</th>
<th>相关指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>查询理解</td>
<td>理解用户查询意图，优化召回与排序结果的相关性</td>
<td>1. Query分析：分词、词权重、纠错、成分分析；</td>
</tr>
</tbody>
</table>
<ol>
<li>Query理解：意图识别、类目识别、地址识别 | 1. 离线指标：在测试集上的准召率、F1<ol>
<li>线上指标：CTR </li>
<li>关注指标：覆盖率、密度、RPS等；|<br>| 召回模型 | 在保证相关性的前提下，提升召回结果 | 1. 查询改写：改写词召回；</li>
<li>相关性校验：改写词排序；</li>
<li>向量召回：双塔、Graph Embedding等；</li>
<li>粗排：Q-P相关性 | 1. 离线指标：AUC；</li>
<li>线上指标：召回率、覆盖率、密度；</li>
<li>关注指标：CTR、RPS、访购率；|<br>| 知识图谱 | 1. 保证准确的前提下，扩充广告node数量；</li>
<li>保证相关性的前提下，提升召回结果 | 1. 实体词、标签词挖掘；</li>
<li>同义、上下位关系挖掘 | 1. 离线指标：在测试集上的准召率、F1；</li>
<li>线上指标：召回率、覆盖率、密度；</li>
<li>线上指标：CTR、RPS、访购率；|<br>| 创意优化 | 1. 通过优化图像、文本的质量，提升CTR | 1. 图片召回：数据源、图片候选模型(pairwise、end2end)；</li>
<li>图片排序：图片特征挖掘；</li>
<li>按搜出图文：类目、标签体系与图文的关系挖掘；</li>
<li>|</li>
</ol>
</li>
</ol>
<ul>
<li><a href="#0.召回算法系统综述">召回算法系统综述</a><ul>
<li><a href="#0.1.召回的数学模型">召回的数学模型</a></li>
<li><a href="#0.2.召回算法逻辑思路">召回算法逻辑思路</a></li>
</ul>
</li>
<li><a href="#1.查询理解">查询理解</a></li>
<li><a href="#2.查询改写">查询改写</a><ul>
<li><a href="#2.9.改写词全局优化">改写词全局优化</a></li>
</ul>
</li>
<li><a href="#3.索引挖掘">索引挖掘</a><ul>
<li><a href="#3.1.知识图谱">知识图谱</a><ul>
<li><a href="#3.2.索引词优化">索引词优化</a> </li>
</ul>
</li>
</ul>
</li>
<li><a href="#4.召回匹配">召回匹配</a></li>
<li><a href="#5.向量召回">向量召回</a></li>
<li><a href="#6.相关性与粗排">相关性与粗排</a><ul>
<li><a href="#6.1.相关性模型">相关性模型</a> </li>
<li><a href="#6.2.粗排模块">粗排模块</a></li>
</ul>
</li>
<li><a href="#9.工程建设">工程建设</a><ul>
<li>[索引建设]</li>
<li>[向量索引]</li>
<li>[case分析链路]</li>
</ul>
</li>
</ul>
<h1 id="0.召回算法系统综述">召回算法系统综述</h1>

<h2 id="0.1.召回的数学模型">召回的数学模型</h2>

<p>召回就是<strong>给定用户Query（q），寻找后验概率最大（相关）的Item。</strong></p>
<p>如果假设索引词node能表示广告（node-item相关性），给定query能召回某个node词（query-node相关性），则该过程可以用条件概率公式描述如下：</p>
<p>$$<br>P(i|q) = \sum_{k} P(n_k | q) \cdot P(i | n_k, q)<br>$$</p>
<p>如果概率分布已知，就可以进行参数估计。不过实际该过程会进行若干近似，以应对常见的<strong>分段式召回</strong>流程，即简单认为最大化$P(n_k|q)$相应的$P(i|n_k)$来最大化$P(i|q)$。分段式召回流程表示如下：</p>
<p>$$<br>\text{Query} \rightarrow<br>\underbrace{<br>\color{red}{Query Rewrite}<br>}_{\text{Matching过程}\; P(n_k | q)}<br>\rightarrow \text{Nodes} \rightarrow<br>\underbrace{<br>\overbrace{<br>\text{Node-Items}<br>}^{\color{red}{\text{Node挖掘}\; P(i|n_k)}}<br>\rightarrow \text{Items Candidates} \rightarrow 粗排<br>}_{\color{red}{\text{Recall Service}\; P(i|q)}}<br>\quad \rightarrow \text{召回的Items}<br>$$</p>
<p>$$<br>\text{Query} \rightarrow<br>\underbrace{ \boxed{Query \; Rewrite}<br>}_{\text{Matching过程}\; P(n_k | q)}<br>\rightarrow \text{Nodes} \rightarrow<br>\underbrace{<br>\overbrace{<br>\boxed{Node-Ads} }^{\color{red}{\text{Node挖掘}\; P(i|n_k)}}<br>\rightarrow \text{Ads Candidates} \rightarrow \boxed{粗排}<br>}_{\color{red}{\text{Recall Service}\; P(i|q)}} \quad \rightarrow \text{召回的Ads}<br>$$</p>
<p>这里涉及到召回算法两个重要的工作方向：查询改写和索引构建。</p>
<p><strong>查询改写（Query Rewrite）</strong></p>
<p>SMT（统计翻译模型）的原理如下：</p>
<p>$$<br>\hat{e} = \arg \max_{e} P(e|f) = \arg \max_{e} P(e) P(f|e)<br>$$</p>
<p>上式的物理意义是求是的$P(e|f)$最大化的$e$。其中，$P(f|e)$是翻译模型，可以用来求解$P(n|q)$。</p>
<p><strong>索引构建</strong></p>
<p>索引构建的主要工作是索引词挖掘（Node挖掘），索引的粒度可以分为Term粒度和Node粒度。</p>
<p>_Term粒度_</p>
<p>比如Query=日式烧烤，Term粒度词：{日式，烧烤}，得到两个词对应索引Item分别是：</p>
<ul>
<li>$日式 \longrightarrow item1, item2, item3, \cdots, itemN$；</li>
<li>$烧烤 \longrightarrow item1, item3, item5, \cdots, itemN$；</li>
</ul>
<p>最后得到的召回结果取交集并按照ecpm排序，记为：$item1, item3, \cdots, itemN$ (sorted by bid*score)。</p>
<blockquote>
<p>优点：根据索引词即可完成相关性检索，可以利用复杂语法树完成多域查询，容易支持多模式匹配；</p>
<p>缺点：倒排链无法与线按照ecpm排序，因为bid粒度在node而非term；需要merge倒排链之后才能排序，如果改写词太多，会对性能有影响。</p>
</blockquote>
<p>_Node粒度_</p>
<ul>
<li>$日式烧烤 \longrightarrow item1, item3, \cdots, itemN$；(sorted by bid*score)</li>
</ul>
<blockquote>
<p>优点：倒排链可以直接按照ecpm排序，对长链很容易截断；</p>
<p>缺点：支持多模式匹配比较复杂，需要在业务层添加额外逻辑；对改写层要求比较高，需要精确改写到node词；</p>
</blockquote>
<p>_索引词方案_</p>
<p>不同的索引粒度，本质上还是召回数量与质量之间的权衡。大多会是两种方式的种种，一般会构建多粒度索引，甚至是字粒度，以适应不同的改写方式。</p>
<p><strong>分段式召回的问题</strong></p>
<ul>
<li>为了性能考虑，改写词会控制在几十个左右，如K=10。改写词的截断会损失一部分搜索空间，尤其在top query会愈加明显。</li>
</ul>
<p>$$<br>P(i|q) = \sum_{k=1}^K P(n_k|q) P(i|n_k, q) + P(n_{K+1}|q) P(i|n_{K+1}, q) + \cdots<br>$$</p>
<ul>
<li>索引层计算node-item score部分，条件独立假设item与query无关，仅与索引词node有关，即：</li>
</ul>
<p>$$P(i|n_k) = P(i|n_k, q)$$</p>
<p>信息损失同样会影响部分准确率，倒排链的阶段会受到node-item score的影响。</p>
<p>以上分析的<strong>分段式召回</strong>的数学模型，下面分析另一种召回方法：<strong>端到端召回</strong>。</p>
<p><strong>端到端召回</strong></p>
<p>为了性能考虑，业界常见做法是借助最近邻搜索（如FAISS），需要计算向量的内积。公式表示：</p>
<p>$$<br>P(i|q) := softmax(\text{Vector}_{q} \cdot \text{Vector}_{i})<br>$$</p>
<p>业界常用的两种方案：</p>
<ol>
<li>建模为超大规模多分类问题，如YoutubeNet。<strong>但$\text{Vector}_{i}$只是softmax矩阵的一维向量，难以引入更多的item特征</strong>；</li>
<li>建模为二分类问题，如利用FM，DSSM或RNN类模型，直接编码query和item侧的特征，即常见的双塔模型。</li>
</ol>
<p>相比方法1，方法2对于item vector的刻画会更加精细化，重点是对于query和item encoder的设计，衍生出较多的模型，如《深度学习》第6章总结的Graph Embedding系列模型。(TODO 添加引用链接)</p>
<h2 id="0.2.召回算法逻辑思路">召回算法逻辑思路</h2>

<p>召回算法逻辑思路可以归纳为：</p>
<table>
<thead>
<tr>
<th>召回思路</th>
<th>逻辑</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>传统召回</strong></td>
<td>类bidword检索 $\rightarrow$ 索引词挖掘&amp;&amp;查询理解与改写 $\rightarrow$ 多种召回策略</td>
<td>长尾Query覆盖较弱，分段式召回；</td>
</tr>
<tr>
<td><strong>向量召回</strong></td>
<td>使用更多特征和相关性信息&amp;&amp;基于候选全集&amp;&amp;end2end架构统一优化</td>
<td>长尾覆盖较好</td>
</tr>
</tbody>
</table>
<p>广告召回算法的主要目标是理解用户行为意图（实时行为分析或Query分析），从广告候选中召回相关性较高的广告集合，经过粗排截断后，给下游进行精排并最终排序。目前，搜索广告召回采用<strong>类bidword检索</strong>的传统召回方案，主要工作有两方面：</p>
<ul>
<li><p><strong>供给端-索引词挖掘</strong>：搜索广告场景，由于广告主通投，由平台主动挖掘bidword作为node索引词，建入倒排索引。涉及的技术主要有实体词、标签词挖掘，基于知识图谱的关系挖掘等；</p>
</li>
<li><p><strong>需求端-查询理解和改写</strong>：通过query理解，改写出不同相关性的改写词，然后去倒排索引中检索出相关的广告。涉及的技术主要有Query分析、Query理解，查询改写，生成不同的改写策略等；</p>
</li>
</ul>
<p>基于类bidword的检索方案优点：可解释性好，便于分模块优化。但是也存在一些问题，比如：</p>
<ul>
<li>召回结果依赖于挖掘和改写两部分工作，它们各自的准确率和召回率，会影响到最终的触发效果；</li>
<li>信息利用不全。传统检索为了性能考虑，一般在粗排中只能使用简单的线性公式，难以表达query和ad的特征，并计算它们的相关性。</li>
</ul>
<p>随着深度学习进展，我们进一步探索了向量召回进一步扩展了召回的边界。对比传统检索方案，向量召回具备这样一些优势：</p>
<ul>
<li>利用异构图可以把query、ad的更多特征以及它们之间的关系数据融入其中，更好的刻画多种相关性；</li>
<li>可以在广告全集上end2end召回，在原先索引词挖掘和查询改写的两个模块上做统一的优化。</li>
</ul>
<p>向量召回的具体方法可以有很多。原理上，只要能将ad和query表示成向量，并可计算相似度，就可以应用于向量召回，如SVD、FM等基于矩阵分解的模型，Word2Vec等词向量模型，或者以DSSM为代表的双塔结构DNN等，都可以应用到向量召回。但是以下它们存在以下问题：</p>
<p><strong>这类建模方式只能应用在单一的训练数据上，信息表现不全</strong>。比如在搜索推荐系统中，相关性往往表现在许多层面，比如Query-Item的点击，多个Item的浏览/点击切换、同Session内的Query切换等等，但是上面这些经典模型面对如此多的数据类型和多种相关性定义则往往力不从心。</p>
<p><strong>图结构可以表征丰富的信息，上面提到的这些相关性都可以通过多种节点和边类型进行异构图的描述</strong>。而Graph Embedding的兴起，也为在异构图上挖掘丰富的相关性信息提供了方法论及模型支持。</p>
<blockquote>
<p>这个过程中，我们可以在向量建模方式、大规模训练工具、异构图构建、模型结构以及线上算法工程等方面进行探索和实践，并且争取较好的平衡召回率和相关性。</p>
</blockquote>
<h1 id="1.查询理解">查询理解</h1>

<p><strong>参考：《高质量博客总结》广告召回技术梳理</strong></p>
<h2 id="1.1.Query分析">Query分析</h2>

<p>分析什么？目标是什么？怎么做？</p>
<h2 id="1.2.意图识别">Query理解</h2>

<p>如何定义意图？</p>
<p>各种识别，多分类模型；</p>
<h1 id="2.查询改写">查询改写</h1>

<h2 id="2.9.改写词全局优化">改写词全局优化</h2>

<p>通常，Query线上改写主要依赖于混合simrank++、word2vec、graphSage等多种改写策略的<strong>同义词表</strong>，改写词之间的优先级需要人工指定（MatchLevel），缺乏统一的排序准则。并且，随着改写策略的不断迭代，一个query对应的改写词也越来越多。</p>
<blockquote>
<p>经统计，超过40% 的搜索请求，其改写词数量触发了截断策略。目前截断形式过于简单，直接取Top 20的改写词用于召回广告，在没有全局排序的情况下容易导致优质改写词被丢弃的现象。</p>
</blockquote>
<p>为此，我们使用<code>&lt;Query, 索引词&gt;</code>相关性模型(同<code>相关性匹配</code>章节中的树模型)，对线上的改写词进行统一排序， 以优化目前改写词截断策略，确保优质改写词优先召回广告。</p>
<ol>
<li><p>根据业务特点，比如对应LBS场景，优先<strong>本地</strong>有召回的改写词（city粒度或商圈粒度）；</p>
</li>
<li><p>构建<code>&lt;Query, 索引词&gt;</code>相关性模型，优先选择高相关性的改写词。</p>
</li>
</ol>
<p>用<code>&lt;Query, 索引词&gt;</code>相关性模型，对线上改写词统一排序，替换掉人工指定的优先级， 确保高相关的改写词优先召回广告。</p>
<p>需要展开训练数据生成与模型训练，评估数据与离线auc/acc等指标具体工作。</p>
<p>Query=”画室体验”，通过相关性排序，实验组将低相关改写词（“体验”）排在后面，以确保高相关改写词（“绘画培训”）优先召回广告。</p>
<ol>
<li>可以在改写词全局排序时，加入<strong>相关性控制</strong>，仅保留满足相关性要求的改写词，保证CTR。</li>
</ol>
<p>要求相关性控制模型，不仅支持文本相关性判定，还要具体语义相关的改写词识别能力。</p>
<blockquote>
<p>项目：《改写词全局排序优化》。可以放在Query改写策略线上落地后再推动该项目</p>
</blockquote>
<h1 id="3.索引挖掘">索引挖掘</h1>

<h2 id="3.1.知识图谱">知识图谱</h2>

<p>各种关系挖掘，建立实体、标签等关系，挖掘知识；</p>
<h3 id="3.1.1.知识图谱应用">知识图谱应用</h3>

<p><strong>生成个性化推荐理由</strong></p>
<p>在推荐结果中添加_个性化推荐理由_，可以增加搜索／推荐结果的解释性，提升用户体验，凸显Item亮点。可以用在创意模块。</p>
<p>推荐理由通常有两种做法：</p>
<ol>
<li>UGC推荐理由：基于用户评论的抽取式理由；</li>
<li>图谱推荐理由：是一种基于业务图谱数据的路径推理（如增加“快驴智能助理”）；</li>
</ol>
<blockquote>
<p>前端展示的内容都需要“文字模版”，如“亮马桥／三元桥 [职场人士]想打卡的火锅店”，对应文字模版为：<code>{user_worker_region} [${上班族}]想打卡的${modify}店</code></p>
</blockquote>
<p><strong>Query推荐：相似Query挖掘</strong></p>
<p>某些品类场景属于低频业务，针对比较稀疏的Query挖掘一批相似的Query，试图修正因稀疏Query导致特征计算不准确的问题（如统计特征），同时可以起到降低case的目的。</p>
<p>_Query向量产生_：</p>
<p>用用户点击的POI构图，然后从图中采样序列，使用skipgram训练得到POI向量，有了POI向量后，针对每个具体的Query，找到被点击的POI集合，采样avg pooling的方式映射得到Query向量；</p>
<p>_Query聚类和特征挖掘_</p>
<p>相似Quuery聚类，不一定要用kmeans聚类方法，可以考虑使用局部敏感哈希（LSH）等离线计算好，输出每个query对应的topK相似Query的集合；</p>
<p>_输出_：</p>
<ol>
<li>基于POI点击反馈的Query向量；</li>
<li>基于点击序列学习到的POI向量；</li>
</ol>
<h2 id="3.2.索引词优化">索引词优化</h2>

<p><strong>优化1：索引词相关性过滤</strong></p>
<p>背景：</p>
<p>已有的索引挖掘流程中，对于<strong>不同索引源产生的索引词</strong>缺乏统一的相关性计算和过滤方式，导致部分相关性较差的索引词被保留，而部分强相关的索引词被错误过滤。目前的相关性过滤方法存在以下不足：</p>
<ul>
<li>根据三级品类过滤时，对于提供<strong>跨品类服务的商家</strong>，其跨品类服务的相关索引可能被过滤；</li>
<li>基于点击统计进行过滤时，长尾数据过滤不准确，会有badcase引入。</li>
</ul>
<p>本次实验主要<strong>优化索引词的相关性计算及过滤方法</strong>，期望达到以下目标：</p>
<ul>
<li>给Doc下挂的索引词赋予<strong>统一且合理的相关性分数</strong>，比如对于“ 痘博士潍坊·全国连锁祛痘机构（胜利西街店）”商家，“祛痘”（0.99）具有较高的相关性分数，“美容美体”（0.90）次之，“修护”（0.32）分数较低，“水疗”（0.04）、“美容减肥”（0.02）等不相关的node分数最低；</li>
</ul>
<p>利用相关性分数进行索引词过滤，替代目前的过滤规则，整体上达到减少不相关索引词，扩充相关索引词的效果。</p>
<p>Doc-索引词相关性模型：</p>
<p>doc索引词可能包含多种类型，如品类词、品牌词、地址词、修饰词、商品词等。对于某个特定索引词，在判断其与doc的相关性时，对于doc各个特征的关注程度是不同的，比如商品词和修饰词更关注商品信息，品牌词更关注商家名称。</p>
<p>模型选用了类似阿里<strong>DIN（Deep Interest Network）的attention结构</strong>。DIN的主要思想为，对于推荐给用户的某一个商品，用户是否点击仅仅取决于历史行为数据中的一小部分，并且具有多峰分布的特性，即点击率预估模型更应该关注众多历史行为中的某一个或某几个。<strong>在doc-索引词相关性任务中，可以理解为，对于某一个索引词，相关性模型更应该关注doc众多特征中的某些部分</strong>。</p>
<blockquote>
<p>可以给出一个attention分布的示例，即某个索引词在doc各个特征上的attention scorer。</p>
</blockquote>
<p>Doc下挂索引词过滤：</p>
<p>使用相关性分数（设定阈值）和历史点击统计进行过滤</p>
<p>离线效果评估：</p>
<p>对每个一级类目下的doc采样K个，评估实验组相对于基线删除的索引词和新增的索引词的相关性。分别对两部分进行相关/不相关/无新增 进行打分，最后汇总。</p>
<h1 id="4.召回匹配">召回匹配</h1>

<p><strong>TODO 召回匹配不同的分类方法：可以画一个二维表格图：横向为显性隐性方法，纵向为召回方式。</strong></p>
<p>搜索（或广告）一般的召回触发方式有如下几种：</p>
<table>
<thead>
<tr>
<th>召回方式</th>
<th>做法</th>
<th>主要技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>检索召回</td>
<td>Query分析与变换，改写为检索关键词</td>
<td>查询改写，相关性校验</td>
</tr>
<tr>
<td>数据召回</td>
<td>通过数据挖掘或知识图谱得到优质的<code>&lt;query, bidword&gt;</code>数据，扩充查询改写</td>
<td>知识图谱、关联挖掘</td>
</tr>
<tr>
<td>模型召回</td>
<td>利用NMT模型和闭环数据，采用生成式方法进行<strong>关键词生成</strong>，关键是关键词对齐，难点是在线的生成式模型实现</td>
<td>NMT模型</td>
</tr>
<tr>
<td>向量召回</td>
<td>扩展召回边界，从全局item中直接检索</td>
<td>embedding学习（如双塔模型，类DSSM模型，Graph Embedding等）</td>
</tr>
</tbody>
</table>
<p> end2end架构与向量索引建设（如FAISS) |</p>
<p>线上召回架构：</p>
<ul>
<li>召回策略优先级通过matchLevel配置；</li>
</ul>
<p>召回匹配的目标是建立Query（或其改写词）与索引词之间的联系。按照精准度划分，可分为精准匹配和相关性匹配。</p>
<ul>
<li>精准匹配：可以保证召回的精准度，但是匹配率可能不高，同时对Query改写有了更高的要求（要求改写词精准命中索引词）;</li>
<li>相关性匹配：在Query（或其该写词）与索引词建立相关性模型（可以构造特征训练二分类模型），召回TopK个索引词。这样可以提升匹配率以及缓解Query改写的压力。</li>
</ul>
<p>以上两种都是为了根据Query得到索引词，都属于索引匹配。</p>
<h2 id="4.1.精确匹配">精确匹配</h2>

<p>使用query的分词／同义词／纠错词／改写词等集合与索引词集合的求交集，交集即为精确匹配的结果。</p>
<h2 id="4.2.相关性匹配">相关性匹配</h2>

<p>搜索系统一般的触发方式，主要是通过Query的改写词与索引词进行精确匹配从而召回广告。这种方式保证了改写词对索引词召回的准确率，但要求改写词和索引词完全精确匹配也给Matching侧改写带来巨大压力。</p>
<p>为此，我们<strong>引入Term粒度</strong>的索引词检索方式提升召回效率，并通过Query-索引词相关性模型来保证Query和召回索引词之间的相关性。有这样一些优势：</p>
<ul>
<li><p>解耦改写词和索引词的精确匹配，有效减轻Query改写压力，提升索引词的召回率。比如索引词是“上门美甲”，但Query是“美甲 上门”，那现在无需改写就能匹配出相应索引词。</p>
</li>
<li><p>把目前零散的改写相关性问题转化为<code>&lt;Query,索引词&gt;</code>排序问题，后续可以统一优化召回索引词的相关性。比如目前线上混合了simrank++、word2vec等多种来源<strong>同义词表</strong>，各自均需要调整相关性，但后续可以统一排序，以优化MatchUnit截断。</p>
<blockquote>
<p>Query的同义词表的目的是为了扩招回，是Query改写技术模块的输出。</p>
</blockquote>
</li>
<li><p>对RS检索逻辑没有变动，RS继续使用索引词粒度的索引，后续可以很方便的对倒排链按照某种规则（比如广告的ecpm)进行排序剪枝，保障检索系统的性能。</p>
</li>
</ul>
<p>我们把<code>&lt;Query, 索引词&gt;</code>相关性排序问题称之为相关性匹配。</p>
<p>算法流程见：<code>桌面/zy/搜索广告/召回算法/相关性匹配_索引词检索与召回算法流程</code></p>
<p><strong>Term权重的应用</strong></p>
<p>索引词的索引构建目前依赖BM25算法，但是传统的BM25没有考虑各term之间的重要性，容易将相关性较差的索引词排在前列。</p>
<p>为此，我们将term权重加入到BM25中进行优化， 调整后的公式为：</p>
<p>$$<br>score(D,Q)=\sum_{i=1}^{n} {\color{red}{tw(q_i)}} \cdot IDF(q_i)\cdot\frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{\left|D\right|}{avgdl})}<br>$$</p>
<p>$\color{red}{tw(q_i)}$为$q_i$的Term权重，一般$k1=2,b=0.75$。</p>
<p>此外，为了保障召回相关性，我们利用term权重识别关键词中的常见搭配，对多term组合的核心词此次不做索引词检索，避免语义漂移。</p>
<p>例如Query=“婚纱摄影”，如果通过“婚纱”或“摄影”两个term做索引词检索，容易导致跨类目问题。</p>
<blockquote>
<p>两个问题需要解决：BM25如何计算？Term Weight如何计算？</p>
</blockquote>
<p><strong>Node词相关性排序与截断</strong></p>
<p>检索出索引词之后，需要根据Query-索引词之间的相关性对索引词进行排序截断。</p>
<p>目前，线上使用树模型（xgboost）对<code>&lt;Query, 索引词&gt;</code>之间的相关性进行打分：</p>
<ul>
<li>建模： 二分类模型，使模型尽可能的拟合伯努利分布，以降低截断时阈值选择的影响。</li>
<li>样本构建：多粒度样本构造（到综业务，需要使用三级类目信息去构造负样本），以满足不同bu对相关性的不同需求。</li>
<li>特征建设：主要以统计特征（pv、ctr、点击类目分布等）、NLP特征为主，构建交叉特征（类目分布的交叉、成分属性的交叉等）。</li>
</ul>
<blockquote>
<p>这里需要提供稳定的模型服务，支持xgboost树模型！！！（对应openmi-serving项目）</p>
</blockquote>
<p>可能还存在以下问题待优化：</p>
<ol>
<li>没有稳定的test集用于评估模型效果，需要人工标注得到test集；</li>
<li>检索模块：为<strong>索引词的倒排链</strong>进行排序（如按照ecpm），方便截断和剪枝，提升检索性能；</li>
<li>相关性排序模块：样本构建部分，上位词可能会召回不相关doc，需要优化上位词的排序效果；</li>
</ol>
<blockquote>
<p>项目：《基于Term粒度的索引词检索与召回》</p>
<p>⚠️：</p>
<ol>
<li>在工程上需要对索引词建立倒排索引，那么召回整体上实现了两级索引架构：索引词索引（以改写词为key）和doc索引（以索引词为key）；</li>
<li>同时，考虑业务特点，可以对<code>索引词索引</code>按照city维度进行拆分，提升索引词集合的质量，降低倒排链长度，提升检索性能(全国索引词集合45w+，按city拆分后不足10w)。</li>
</ol>
</blockquote>
<h2 id="5.2.向量召回">向量召回</h2>

<p><strong>向量召回的特点</strong></p>
<ul>
<li>全局检索&amp;&amp;end2end学习</li>
<li>信息利用更全：Item特征，丰富的节点关系刻画（用户行为边，内容相似边）；</li>
</ul>
<p><strong>搜索广告向量召回难点</strong></p>
<ol>
<li>对召回ad的相关性要求较高，要求左塔的user网络结构能很好的提取关键词特征，保证召回相关性；</li>
<li>较强的LBS地域限制和品类差异，建模时需要考虑；</li>
<li>不能与所有的item计算相似度，这样复杂度太高，可以采用FAISS向量索引解决；</li>
</ol>
<p>扩召回方式之一，做u2u／i2i相似推荐，或直接i2u推荐；</p>
<p><strong>向量召回技术与落地</strong></p>
<blockquote>
<p>参考《深度学习》Chapter6 GraphEmbedding应用</p>
</blockquote>
<p>主要步骤如下：</p>
<ul>
<li>异构图构建；</li>
<li>样本生成（采样）；</li>
<li>图模型建模（encoder，decoder，loss）；</li>
<li>离线评估；</li>
<li>线上架构（向量索引，Query向量计算，相似度检索）；</li>
</ul>
<p>参考资料见：印象笔记－学习－召回链接</p>
<h1 id="6.相关性与粗排">相关性与粗排</h1>

<h2 id="6.1.相关性模型">相关性模型</h2>

<blockquote>
<p>需要关注以下问题：</p>
<ol>
<li>如何定义相关性？相关性的目标是什么？比如<code>&lt;日语，韩语&gt;, &lt;毛笔字、水彩画&gt;</code>在推荐场景下相关性较高，但是在搜索场景，虽然同属一个类目但是两个词是不相关的。<strong>可借助外部层次化知识学习相关性模型</strong>。</li>
<li>相关性评估指标和评估标准是什么？badcase@1, acc, 还是其他？</li>
</ol>
</blockquote>
<p>相关性的定义与业务更加密切，看业务关注哪方面的相关因素？</p>
<ol>
<li>在搜索（或搜索广告）场景，文本语义相关性是不可缺少的因素；</li>
<li>在O2O场景，因为涉及LBS限制，所以距离、品类甚至商圈都应该是相关性考虑的因素；</li>
<li>在配送调度场景，供需之间的关系（权重）同样可以引入相关性因子；</li>
</ol>
<p>相关性大致可以分为以下几类：文本（语义）相关性，业务相关性；</p>
<p>这里先梳理特征相关性特征体系与特征计算逻辑，后续介绍相关性模型。</p>
<h3 id="6.1.1.文本及语义相关性">文本及语义相关性</h3>

<p><strong>字符串相关性</strong></p>
<ol>
<li>字符串匹配：将query分词与item文本分词后，两个list进行叉乘，计算是否命中（0/1）以及命中率（命中个数／最短list）；</li>
<li>字符串相似度：query与item文本计算相似度（如BM25，不过短文本计算相似度偏差较大，无法计算<code>iphone, 苹果</code>之间的关系）；</li>
<li>字符串编辑距离：如计算Jaro距离、Levenshtein距离、Ngram距离等(是否有效，待验证)；</li>
</ol>
<p><strong>相似Query集合与POI文本计算相关性</strong></p>
<p>将《知识图谱应用》章节产出的相似Query集合与候选Item计算相关性，计算逻辑可以是：</p>
<ol>
<li>字符串相似度：&lt;相似Query集合, item&gt;</li>
<li>语义向量相似度：&lt;点击反馈学到的的Query向量，POI向量&gt;</li>
</ol>
<p><strong>文本语义相关性</strong></p>
<p>可以使用w2v学习词向量，但是最好是结合模型做；也可以使用类似DSSM、Bert模型学习词的embedding建立索引，用于特征相关性得分计算；</p>
<p><strong>Query意图相关</strong></p>
<p>将query标签词与POI品牌词、类目词匹配（是否命中）</p>
<ol>
<li>Query成分标签，如address, brand, cate, menu, movie, poi店名，旅游景点、目的地等；涉及到Query成分分析；</li>
<li>Query意图标签：如brand, cate, food, hotel, product, travel等；</li>
<li>Query分类标签：如item类别分类与权重；</li>
</ol>
<h3 id="6.1.2.业务相关性">业务相关性</h3>

<ol>
<li>比如LBS场景下，不同城市／不同品类下对距离的敏感度不同，可以将距离做的更佳细化作为相关性特征；</li>
<li>分析前端展示内容，也是最重要的用户决策信息。如POI店名、POI tag，商圈、销量、价格、折扣、距离等；</li>
<li>&lt;用户历史行为各个维度，POI各个维度算&gt;算diff，如item星级，价格差异，品类差异等；</li>
</ol>
<h3 id="6.1.3.相关性模型">相关性模型</h3>

<p>训练相关性模型最重要的前提之一就是训练样本的构造。是用用户反馈日志（click／order／时长）作为label，还是有人工标注的数据作为训练样本；</p>
<p>相关性模块可以在召回层＋精排层（预估层）同时做。主要的模型思路总结为：</p>
<p><strong>相关性特征＋XGBoost模型</strong></p>
<p>主要工作有两点：</p>
<ol>
<li>构建相关性特征体系（使用featurex完成特征配置和抽取）；</li>
<li>XGBoost离线与在线（离线使用公司平台，在线使用serving）；</li>
</ol>
<p><strong>DSSM模型及其变种</strong></p>
<p>DSSM模型（Deep Structured Semantic Models）是语义相似度计算方向一个比较重要且基础的模型。模型结构可以分为3层：输入层、表示层和匹配层。</p>
<ol>
<li>输入层: hash＋embedding。将句子映射到一个向量空间。英文（word hashing，比如采用letter-trigrams方法，压缩空间&amp;&amp;增强泛化能力）；中文（分词或单字，分词效果取决于分词模型，引入了不确定性，可以直接使用单字）；</li>
<li>表示层：pooling＋fc。 DSSM使用BOW方式（丢弃了字向量在句子中的位置信息，即语序信息和上下文信息）；</li>
<li>匹配层：两个语义向量计算相似度（如cosine距离）；输出：softmax得到正样本的后验概率；损失函数：极大似然估计，即$\mathcal{L} = -\log \prod_{(Q,D^+)} P(D^+|Q)$</li>
</ol>
<p>_DSSM实现时重要的细节：损失函数仅考虑正例，softmax计算时会考虑反例（这也是反例的意义）。_</p>
<blockquote>
<p>DSSM优点：</p>
<ol>
<li>可以用字向量作为输入，减少切词的依赖，同时提高模型泛化能力；</li>
<li>输入层embedding的学习不同于w2v这种无监督学习（会额外引入误差），DSSM统一采用有监督学习，有助于提高学习的精准度；</li>
</ol>
<p>DSSM缺点：</p>
<ol>
<li>采用词袋模型，因此丧失了语序信息和上下文信息；</li>
<li>DSSM采用弱监督、端到端的模型，预测结果不可控（？？？怎么理解）。</li>
</ol>
</blockquote>
<p><strong>CNN-DSSM模型</strong></p>
<p>又称CLSM（Convolutional Latent Semantic Model），主要为了解决DSSM的缺点，区别主要体现在 输入层和表示层。</p>
<blockquote>
<p>CNN-DSSM优点：通过卷积层提取了<strong>滑动窗口下的上下文信息</strong>，又通过池化层提取了<strong>全局的上下文信息</strong>，使得上下文信息得到有效的保留；</p>
<p>CNN-DSSM缺点：对于间隔较远的上下文信息，难以有效保留；</p>
</blockquote>
<p><strong>LSTM-DSSM模型</strong></p>
<p>为了解决CNN-DSSM模型的缺点，使用LSTM网络来捕获较远距离上下文信息。</p>
<p>DSSM及其变种对比</p>
<table>
<thead>
<tr>
<th>模型结构</th>
<th>DSSM</th>
<th>CNN-DSSM</th>
<th>LSTM-DSSM</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入层</td>
<td>1. 英文：letter-trigram</td>
</tr>
</tbody>
</table>
<ol>
<li>中文：单字 | 1. 英文：先word-trigram后letter-trigram</li>
<li>中文：单字 | 同CNN_DSSM |<br>| 表示层 | pooling + fc(tanh) | 卷积层＋Max pooling ＋ fc(tanh) | LSTM-引入了peep hole(f,i,o)计算时添加细胞态$C_{t-1},C_t$信息 |<br>| 匹配层 | cosine + sigmoid + 极大似然估计 | 同DSSM | 同DSSM |</li>
</ol>
<blockquote>
<p>DSSM模型应用场景：《基于层次化品类图谱的文本语义相关性模型（KG-DSSM）》</p>
<p>分析Query抽取品类词 -&gt; 对抽取的候选node词找到品类图谱节点，正样本：基于品类树扩展所有上位词为正例，负样本根据同位词随机采样，得到<code>&lt;Query, Positive_Node, Negative_Node&gt;</code> 然后交给CLSM模型学习；</p>
</blockquote>
<p><strong>Bert模型</strong></p>
<p>学习向量表示＋FC</p>
<p>TODO</p>
<ul>
<li>FeatureX需要对相关性模型的支持有：字符串匹配op，字符串相似度计算op；</li>
<li>DSSM模型实现；</li>
</ul>
<h2 id="6.2.粗排模块">粗排模块</h2>

<p>粗排作为召回截断的主要排序手段，直接影响到召回的质量以及召回内容的相关性，作为rerank的上游也对rerank整体的排序效果产生影响。因此，粗排的优化要考虑以下因素：</p>
<ul>
<li>性能：因为召回数量很大，因此不能向做精排模型的思路一样去做粗排模型，特征模型均要求简单有效；</li>
<li>目标：既然是排序，一定要考虑到平台收益和用户体验，可以以$score^{power} \times bid$为主作为排序公式；</li>
<li>业务：结合业务特点，需要对排序公式做一些修正，比如强化距离因素等。</li>
</ul>
<p>建模思路同精排模型，训练样本同样可以用点击曝光日志。</p>
<p>结合一个case聊粗排排序公式设计：粗排模型上线后，发现覆盖率下降，原因是机制距离门槛限制，此时可以在排序公式中添加_距离因子_，即：</p>
<p>$$<br>score = distScore^{distPower} \cdot ctr^{ctrPower} \cdot bid<br>$$</p>
<p>排序超参可以灵活配置（如ctrPower=1.2, distPower=1.1）</p>
<h1 id="9.工程建设">工程建设</h1>

<h2 id="9.1.索引系统">索引系统</h2>

<p>高性能索引系统（支持实时增删功能），相似度搜索，召回服务；</p>
<h3 id="9.2.工具积累">工具积累</h3>


<ul>
<li>可视化工具，如query画像；</li>
<li>case追踪工具；</li>
<li>指标监控工具：实时与天级别；</li>
</ul>
<p>召回算法</p>
<p>Query $\xrightarrow[Query-索引相关性]{Query分析与改写;改写优化;索引召回;}$ 索引词 $\xrightarrow[索引-Doc相关性]{Retrivel服务}$  Doc</p>
<p>召回工程</p>
<p>Query $\xrightarrow{Matching服务}$ 索引词 $\xrightarrow{Retrivel服务}$  Doc</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://www.52caml.com/deep_learning/dl-chapter9-recall-algorithm-dev/" data-title="计算广告与机器学习" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 

<div class="next">
<a href="/deep_learning/dl-chapter11-ad-ranking-dev/"  title="">
 <strong>下一篇：</strong><br/> 
 <span>(no title)
</span>
</a>
</div>

</nav>

	

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#基于关键词的召回算法与系统"><span class="toc-number">1.</span> <span class="toc-text">基于关键词的召回算法与系统</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0.召回算法系统综述"><span class="toc-number">2.</span> <span class="toc-text">召回算法系统综述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0.1.召回的数学模型"><span class="toc-number">2.1.</span> <span class="toc-text">召回的数学模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0.2.召回算法逻辑思路"><span class="toc-number">2.2.</span> <span class="toc-text">召回算法逻辑思路</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1.查询理解"><span class="toc-number">3.</span> <span class="toc-text">查询理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1.1.Query分析"><span class="toc-number">3.1.</span> <span class="toc-text">Query分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1.2.意图识别"><span class="toc-number">3.2.</span> <span class="toc-text">Query理解</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2.查询改写"><span class="toc-number">4.</span> <span class="toc-text">查询改写</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2.9.改写词全局优化"><span class="toc-number">4.1.</span> <span class="toc-text">改写词全局优化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3.索引挖掘"><span class="toc-number">5.</span> <span class="toc-text">索引挖掘</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3.1.知识图谱"><span class="toc-number">5.1.</span> <span class="toc-text">知识图谱</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3.1.1.知识图谱应用"><span class="toc-number">5.1.1.</span> <span class="toc-text">知识图谱应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3.2.索引词优化"><span class="toc-number">5.2.</span> <span class="toc-text">索引词优化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4.召回匹配"><span class="toc-number">6.</span> <span class="toc-text">召回匹配</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4.1.精确匹配"><span class="toc-number">6.1.</span> <span class="toc-text">精确匹配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4.2.相关性匹配"><span class="toc-number">6.2.</span> <span class="toc-text">相关性匹配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5.2.向量召回"><span class="toc-number">6.3.</span> <span class="toc-text">向量召回</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6.相关性与粗排"><span class="toc-number">7.</span> <span class="toc-text">相关性与粗排</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6.1.相关性模型"><span class="toc-number">7.1.</span> <span class="toc-text">相关性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6.1.1.文本及语义相关性"><span class="toc-number">7.1.1.</span> <span class="toc-text">文本及语义相关性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6.1.2.业务相关性"><span class="toc-number">7.1.2.</span> <span class="toc-text">业务相关性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6.1.3.相关性模型"><span class="toc-number">7.1.3.</span> <span class="toc-text">相关性模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6.2.粗排模块"><span class="toc-number">7.2.</span> <span class="toc-text">粗排模块</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9.工程建设"><span class="toc-number">8.</span> <span class="toc-text">工程建设</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9.1.索引系统"><span class="toc-number">8.1.</span> <span class="toc-text">索引系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9.2.工具积累"><span class="toc-number">8.1.1.</span> <span class="toc-text">工具积累</span></a></li></ol></li></ol></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/OpenMIT/" title="OpenMIT">OpenMIT<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/分布式机器学习/" title="分布式机器学习">分布式机器学习<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习/图网络/" title="图网络">图网络<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/强化学习与智能决策/" title="强化学习与智能决策">强化学习与智能决策<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/概率与统计/" title="概率与统计">概率与统计<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/深入浅出机器学习/" title="深入浅出机器学习">深入浅出机器学习<sup>9</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习/" title="深度学习">深度学习<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/编程语言与技术/" title="编程语言与技术">编程语言与技术<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Agent/" title="Agent">Agent<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/参数服务器/" title="参数服务器">参数服务器<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Attention/" title="Attention">Attention<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/DP/" title="DP">DP<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Evalation/" title="Policy Evalation">Policy Evalation<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Improvement/" title="Policy Improvement">Policy Improvement<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Iteration/" title="Policy Iteration">Policy Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Value-Iteration/" title="Value Iteration">Value Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Greedy-Policy/" title="Greedy Policy">Greedy Policy<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/连续随机变量/" title="连续随机变量">连续随机变量<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/MDP/" title="MDP">MDP<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Markov-Decision-Process/" title="Markov Decision Process">Markov Decision Process<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/RL/" title="RL">RL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Environments/" title="Environments">Environments<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/GD/" title="GD">GD<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/FTRL/" title="FTRL">FTRL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaGrad/" title="AdaGrad">AdaGrad<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaDelta/" title="AdaDelta">AdaDelta<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Adam/" title="Adam">Adam<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello, Welcome to CAML technology sharing platform.  <br/>
			I&#39;m Zhou Yong, engaged in algorithms work on computational advertising and machine learning.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1707438033" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/ComputationalAds" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:zhouyongsdzh@foxmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2020 
		
		<a href="/about" target="_blank" title="ZhouYong">ZhouYong</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>









<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
