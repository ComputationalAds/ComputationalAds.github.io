
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>计算广告与机器学习</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="ZhouYong">
    

    
    <meta name="description" content="机器学习预估系统-算法模型与算法工程以CTR预估为例

预估算法系统综述 
广告算法预估模块
预估系统架构


预估算法模型
大规模稀疏模型
Boosting模型
Embedding+NN模型
Wide&amp;amp;Deep
DeepFM
DIN


DNN+用户序列建模
DIEN
DSIN
超长序列建模


DNN+多任务学习
A Multitask Ranking System
ESMM 
MMO">
<meta property="og:type" content="article">
<meta property="og:title" content="计算广告与机器学习">
<meta property="og:url" content="http://www.52caml.com/deep_learning/dl-chapter10-ctrcvr-dev/index.html">
<meta property="og:site_name" content="计算广告与机器学习">
<meta property="og:description" content="机器学习预估系统-算法模型与算法工程以CTR预估为例

预估算法系统综述 
广告算法预估模块
预估系统架构


预估算法模型
大规模稀疏模型
Boosting模型
Embedding+NN模型
Wide&amp;amp;Deep
DeepFM
DIN


DNN+用户序列建模
DIEN
DSIN
超长序列建模


DNN+多任务学习
A Multitask Ranking System
ESMM 
MMO">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="计算广告与机器学习">
<meta name="twitter:description" content="机器学习预估系统-算法模型与算法工程以CTR预估为例

预估算法系统综述 
广告算法预估模块
预估系统架构


预估算法模型
大规模稀疏模型
Boosting模型
Embedding+NN模型
Wide&amp;amp;Deep
DeepFM
DIN


DNN+用户序列建模
DIEN
DSIN
超长序列建模


DNN+多任务学习
A Multitask Ranking System
ESMM 
MMO">

    
    <link rel="alternative" href="/atom.xml" title="计算广告与机器学习" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="计算广告与机器学习" title="计算广告与机器学习"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="计算广告与机器学习">计算广告与机器学习</a></h1>
				<h2 class="blog-motto">Computational Advertising and Machine Learning</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/home">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:www.52caml.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/deep_learning/dl-chapter10-ctrcvr-dev/" title="" itemprop="url"></a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2020-09-28T01:02:15.000Z" itemprop="datePublished"> 发表于 2020-09-28</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习预估系统-算法模型与算法工程"><span class="toc-number">1.</span> <span class="toc-text">机器学习预估系统-算法模型与算法工程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0.预估算法系统综述"><span class="toc-number">2.</span> <span class="toc-text">预估算法系统综述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#点击率预估"><span class="toc-number">2.1.</span> <span class="toc-text">点击率预估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#转化率预估"><span class="toc-number">2.2.</span> <span class="toc-text">转化率预估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#交易额预估"><span class="toc-number">2.3.</span> <span class="toc-text">交易额预估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1.预估算法模型"><span class="toc-number">3.</span> <span class="toc-text">预估算法模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1.3.Embedding+NN模型"><span class="toc-number">3.1.</span> <span class="toc-text">Embedding+NN模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1.3.1.WideAndDeep"><span class="toc-number">3.1.1.</span> <span class="toc-text">Wide&Deep</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.3.2.DeepFM"><span class="toc-number">3.1.2.</span> <span class="toc-text">DeepFM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.3.3.DIN"><span class="toc-number">3.1.3.</span> <span class="toc-text">DIN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1.4.DNN+行为序列建模"><span class="toc-number">3.2.</span> <span class="toc-text">DNN+行为序列建模</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1.4.1.DIEN"><span class="toc-number">3.2.1.</span> <span class="toc-text">DIEN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.4.2.DSIN"><span class="toc-number">3.2.2.</span> <span class="toc-text">DSIN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.4.3.超长序列建模"><span class="toc-number">3.2.3.</span> <span class="toc-text">超长序列建模</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1.5.DNN+多任务学习"><span class="toc-number">3.3.</span> <span class="toc-text">DNN+多任务学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1.5.1.A-Multitask-Ranking-System"><span class="toc-number">3.3.1.</span> <span class="toc-text">A Multitask Ranking System</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.5.2.ESMM"><span class="toc-number">3.3.2.</span> <span class="toc-text">ESMM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.5.2.MMOE"><span class="toc-number">3.3.3.</span> <span class="toc-text">MMOE</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1.预估算法工程"><span class="toc-number">4.</span> <span class="toc-text">预估算法工程</span></a></li></ol>
		
		</div>
		
		<h1 id="机器学习预估系统-算法模型与算法工程">机器学习预估系统-算法模型与算法工程</h1><p>以CTR预估为例</p>
<ul>
<li><a href="#0.预估算法系统综述">预估算法系统综述</a> <ul>
<li><a href="#0.1.广告算法预估模块">广告算法预估模块</a></li>
<li><a href="#0.2.预估系统架构">预估系统架构</a></li>
</ul>
</li>
<li><a href="#1.预估算法模型">预估算法模型</a><ul>
<li><a href="#1.1">大规模稀疏模型</a></li>
<li><a href="#1.2">Boosting模型</a></li>
<li><a href="#1.3.Embedding+NN模型">Embedding+NN模型</a><ul>
<li><a href="#1.3.1.WideAndDeep">Wide&amp;Deep</a></li>
<li><a href="#1.3.2.DeepFM">DeepFM</a></li>
<li><a href="#1.3.3.DIN">DIN</a></li>
</ul>
</li>
<li><a href="#1.4.DNN+用户序列建模">DNN+用户序列建模</a><ul>
<li><a href="#1.4.1.DIEN">DIEN</a></li>
<li><a href="#1.4.2.DSIN">DSIN</a></li>
<li><a href="#1.4.3.超长序列建模">超长序列建模</a></li>
</ul>
</li>
<li><a href="#1.5.DNN+多任务学习">DNN+多任务学习</a><ul>
<li><a href="#1.5.1.A-Multitask-Ranking-System">A Multitask Ranking System</a></li>
<li><a href="#1.5.1.ESMM">ESMM</a> </li>
<li><a href="#1.5.2.MMOE">MMOE</a> </li>
</ul>
</li>
</ul>
</li>
<li><a href="#2.预估算法工程">预估算法工程</a><ul>
<li><a href="#2.1.大数据处理框架">大数据处理框架-solar2</a></li>
<li><a href="#2.2.特征抽取框架">特征抽取框架-featurex</a> </li>
<li><a href="#2.3.离线训练框架">离线训练框架-modelx</a></li>
<li><a href="#2.4.在线预估服务">在线预估服务-aiserving</a></li>
</ul>
</li>
</ul>
<h1 id="0.预估算法系统综述">预估算法系统综述</h1>


<p>从业务角度看，预估问题主要包括点击率预估、转化率预估、交易额预估、ROI预估等。</p>
<h2 id="点击率预估">点击率预估</h2>

<h2 id="转化率预估">转化率预估</h2>


<p>CVR模型最大的问题是因延迟反馈导致的<strong>转化归因问题</strong>，影响了训练样本，进一步影响模型训练效果和偏差问题。</p>
<p>很早之前Google Adwords将归因模型分为以下几种：first click，last click，time delay等。</p>
<p>转化率预估问题的挑战</p>
<p>个人从优化CVR模型的实践经验来看，CVR预估问题最主要的问题是数据相关问题，具体包括：</p>
<ol>
<li>无法获取准确且全面的转化数据，主要针对非电商业务场景，如搜索引擎、社交网络、新闻资讯等；</li>
<li>转化数据正负样本如何归因。负样本如何定义？post-click如何定义以及系数？归因逻辑和权重系数的不同直接影响样本分布，进而对线上指标造成很大的“冲击”。</li>
</ol>
<h2 id="交易额预估">交易额预估</h2>

<p>因为label是成交的金额，是数值类型，因此这是一个回归问题。可以使用MSE或MAE作为loss function。</p>
<blockquote>
<p>loss function可以加log，或rmse等作为loss都可以尝试。</p>
</blockquote>
<p>交易额预估问题的挑战</p>
<ol>
<li>样本稀疏，与（强）转化正样本量级一致。此时，可以考虑其它业务的交易数据，如广告场景利用自然交易数据作为扩充解决样本稀疏问题（不过跨部门拿敏感数据，沟通成本可能很大）；</li>
<li>loss function值波动可能较大，一方面取决于label存在异常点（回归问题的通病，可能需要做截断），另一方面取决于loss的设计。经实践，mae比mse略稳定些。</li>
<li>交易额特征设计，不同于CTR场景，因为样本稀疏，应该多考虑<strong>泛化特征</strong>以及分析影响交易额的重要因素有哪些，比如就餐人数与餐费强相关等；</li>
</ol>
<p>关于多任务联合学习，几点结论：</p>
<ol>
<li>单交易额模型 效果差于<code>&lt;转化率，交易额&gt;</code>联合学习的效果 $$loss=tf.group(loss_{转化率ESMM}, mae_{交易额})$$</li>
</ol>
<p><strong>问题：交易额与ESMM联合训练，前者因为没有负样本的概念，其label如何定义？如果与esmm的label shape保持一致？</strong></p>
<h1 id="1.预估算法模型">预估算法模型</h1>


<h2 id="1.3.Embedding+NN模型">Embedding+NN模型</h2>

<p>TODO，介绍该类模型特点</p>
<h3 id="1.3.1.WideAndDeep">Wide&amp;Deep</h3>

<p>Wide&amp;Deep模型是Google于2016年在DLRS会议上发表的文章《Wide&amp;Deep Learning for Recommender System》。文中设计了一种<strong>融合浅层模型（wide）和深层模型（deep）进行联合训练的框架，综合浅层模型的记忆能力（wide）和DNN模型的泛化能力，从而提升模型整体性能</strong>。</p>
<blockquote>
<p>该模型成功应用在Google Play的app推荐业务中，并取得下载率3.9%的提升。线上性能通过batch size分包，多线程并行调用（处理）达到提升效率的目的，单次响应耗时由31ms降至14ms。 </p>
</blockquote>
<p>从用户体验的角度讲，搜索、推荐、广告都存在的一个挑战是如何通过获得返回结果的准确性和扩展性。更多时候，最好的状态是二者兼顾，不可偏废。</p>
<blockquote>
<ol>
<li>如果返回结果都是精准内容（与用户历史强相关），那么易造成用户兴趣收敛，无新鲜感，不利于用户长久留存，同时浪费了流量价值；</li>
<li>如果返回结果内容过于泛化，用户的精准兴趣无法得到满足，用户体验欠佳，流失风险较大。</li>
</ol>
</blockquote>
<p>因此，相对于准确性，扩展性倾向于改善返回结果的多样性。</p>
<p>另外，从机器学习模型能力的角度讲，模型能力的核心就是其表达能力，而表达能力可以从两个方面解读，即模型的记忆能力和泛化能力。更多时候，表达能力也需要在二者之间平衡。</p>
<blockquote>
<ol>
<li>过度追求模型的记忆能力，易造成模型过拟合。因为我们可以用很复杂的模型或设计复杂的特征来拟合训练样本，但是在测试数据上表现不一定好；</li>
<li>过度追求模型泛化能力，会出现模型表达能力不够，使得模型区分性差。在推荐场景下可能无法真正做法个性化推荐；在广告CTR预估场景，过于泛化的模型影响预估的精准度，继而影响计费等；</li>
</ol>
</blockquote>
<ul>
<li><strong>记忆能力（Memorization）</strong></li>
</ul>
<p>超大规模稀疏机器学习模型的特征体系通常离不开人工设计的组合特征（又叫交叉特征，cross-product）。通过设计二阶或高阶组合特征可以描述稀疏特征之间的关联性（这里特指共现性），基于此训练得到的模型可以很好的记忆历史数据中曾经共现过的组合特征信息。</p>
<p>传统超大规模稀疏模型如LR、FFM等，特征工程两个主要思路：</p>
<ol>
<li>离散特征：基于原始离散特征，设计2阶或高阶组合特征，如<code>user-item</code>, <code>request-item</code>, <code>user-request-item</code>等；</li>
<li>连续特征：做特征离散化，之后当作离散特征训练大规模稀疏模型。</li>
</ol>
<p>以上方法的优点是实现快速高效、特征重要性易分析、模型可解释性高等，在我们广告CTR预估系统证明非常有效（以FFM模型为例，亿级别特征，10GB+模型）</p>
<blockquote>
<p>模型效果比较：高维稀疏特征+LR模型 &amp;&amp; 低维连续特征+GBDT模型，在广告CTR核心广告位，auc／rps指标前者好于后者均在5%以上；</p>
</blockquote>
<p>人工设计特征这一点不能作为传统大规模稀疏模型的缺点，因为特征工程方面的工作本身就包涵了特征设计本身，这一点与具体模型关系不大。另外，个人经验，相对传统预估模型，DNN模型对特征工程的要求更高（因为特征种类更复杂，特征尺度要保持一致，如连续特征归一化等）。</p>
<p><strong>记忆能力的缺点：易出现过拟合</strong>。设计更加稀疏or更复杂的模型理论上可以无限拟合训练样本，但是也易出现模型的泛化能力较差在测试集表现不好。解决该问题的方法通常有：</p>
<ol>
<li>稀疏特征频次过滤（通过设定阈值），训练样本不包含小于阈值的稀疏特征；</li>
<li>模型训练时，优化目标加上L1惩罚项或使用FTRL优化算法等，得到模型的稀疏表示。</li>
</ol>
<ul>
<li><strong>泛化能力（Generialization）</strong></li>
</ul>
<p>在WDL文章中，对于泛化能力的理解是：为sparse特征学习低维的dense embedding用来表示特征相关性（Deep层的输入可以包含sparse特征和dense特征）。</p>
<p>泛化能力的优点是可以减少人工组合特征，对历史上没有出现的特征组合有一定的泛化性（类似FM）。但是物极必反，当<code>&lt;user,item&gt;</code>非常稀疏时，例如有和独特偏好的users以及很小众的items，DNN很难为users和items学习到有效的embedding。这种情况下，大部分<code>&lt;user,item&gt;</code>应该是没有关联的，但dense embedding 的方法还是可以得到对所有<code>&lt;user,item&gt;</code> pair 的非零预测，因此导致 过于泛化（over-generalize）并推荐无相关性的内容，准确性得不到保证。此时Memorization就展示了优势，它可以“记住”这些特殊的特征组合。</p>
<p>Memorization根据历史行为数据，返回的结果通常和用户已有行为的直接相关的item。而Generalization会学习新的特征模式，提高推荐item的多样性。 论文作者结合两者的优点，提出了一个新的学习算法——Wide&amp;Deep Learning，其中Wide、Deep分别对应Memorization、Generalization。</p>
<ul>
<li><strong>模型结构</strong></li>
</ul>
<p><strong>TODO 用DL工具画图</strong></p>
<p>模型表达式为：</p>
<p>$$<br>P(y=1|x) = \sigma\left(W_{wide}^T [x, \phi(x)] + W_{deep}^T \mathcal{a}^{(l_f)} + b \right) = \sigma\left(y_{\text{LR}} + y_{\text{DNN}} \right)<br>$$</p>
<p>其中，$\mathcal{a}^{(l_f)} = f(W^{(l-1)} \mathcal{a}^{(l-1)} + b^{(l-1)})$</p>
<ol>
<li>Wide部分：广义线性模型；</li>
<li>Deep部分：sparse特征输入学习低维embedding（embedding层），之后多层全连接，如果有dense特征，在全连接前与embedding输出concat在一起；</li>
<li>损失函数：交叉熵损失；</li>
<li>模型训练：论文中wide部分FTRL，deep部分AdaGrad算法；</li>
</ol>
<h3 id="1.3.2.DeepFM">DeepFM</h3>

<p>DeepFM是2017年华为发表在IJCAI会议上的一篇文章《DeepFM: A Factorization-Machine based Neural Network for CTR Prediction》提出的模型。</p>
<p>DeepFM模型的初衷同样是强化特征之间的交互关系，比如<code>&lt;饭点儿，外卖&gt;</code>反映了时间特征与item类别之间的关系。除此之外，还有其它的特征组合关系，有些很容易想到，有些因为不符合我们平时的认知不容易想出来。怎么办呢？当然是希望机器可以帮助我们自动学到很多特征之间的关系。</p>
<p>DeepFM模型结构融合了FM和DNN两部分，前者负责二阶组合特征的学习，后者侧重于高阶特征的学习。这两部分共享embedding层。因此DeepFM模型结构可以理解为：</p>
<p>$$<br>P(y=1|x) = \sigma \left(y_{\text{FM}} + y_{\text{DNN}} \right)<br>$$</p>
<blockquote>
<p>关于FM／FFM模型在<a href="http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/" target="_blank" rel="external">第09章：深入浅出ML之Factorization家族</a>中有详细的解读。</p>
</blockquote>
<p><strong>DeepFM vs FNN vs PNN</strong></p>
<ol>
<li>DeepFM: FM+DNN并行结构；</li>
<li>FNN: FN,DNN串行结构。FM模型先训练好embedding，然后作为DNN的输入；</li>
<li>PNN: 在embedding层与FC层添加了product操作（内积或外积），之后与原始的embedding层concat后作为FC的输入；</li>
</ol>
<p>总结DeepFM：FM和DNN共享embedding，同时学习模型二阶和高阶信息。</p>
<h3 id="1.3.3.DIN">DIN</h3>

<p>结合下述问题，先睹为快：</p>
<ol>
<li>DIN激活单元是如何工作的？是self-attention的实现吗？ 答：不是self-attention，<code>&lt;item, ad&gt;</code>向量作为输入，计算外积之后与两个原向量concat后进入一层FC，使用dice作为激活函数，输出1维值作为激活权重；</li>
<li>用户行为A,B,C分别点击了3次、1次、2次，那么广告D如何与用户行为作attention操作，是与ABC做attention，还是A,A,A,B,C,C做attention？文章没有提到，不过可以获取A、B、C次数作为权重，在item embedding后给予加权。</li>
<li>dice激活函数在tf中的实现？dice是对PRelu的改进。PRelu函数的校正点是固定的0，深度学习fc的每一层的输入分布发生变化时是不适用的。根据每层输入数据的分布自适应调整校正点的位置。（如何实现？）<br>dice函数的优势主要是校正点随输入分布变化而变化，引入mean和var两个变量来实现；</li>
<li>自适应正则在tf中的实现？</li>
<li>DIN模型新增的参数量是多少？ 严格意义，DIN增加的网络参数有：局部激活单元神经网络第一层单元数（N）和dice函数用于平滑校正点的均值和方差（2）。但是计算复杂度增加了 max_item_count * (item_feature_count个embedding + 乘积 + 一层dice激活函数的全连接计算attenion scorer + concat) 最后concat到主网络；</li>
</ol>
<p>DIN模型关键词：<strong>兴趣多样性（diversity interests），局部激活单元（local activation unit），mini-batch aware正则，自适应损失函数（Dice）</strong></p>
<ul>
<li><strong>局部激活单元（local activation unit）</strong></li>
</ul>
<p>用户兴趣是多样的，传统DNN模型用户兴趣特征表示为一个定长的向量，与具体的候选ad无关。但实际上，不同的ad与用户兴趣中某些部分是相关的，与另外一些兴趣可能不相关，“只有与ad相关的那部分兴趣”才是有意义的。因此文章主要针对不同的ad，动态地表示用户兴趣。做法是对不同的ad计算用户兴趣中的item的权重，从而实现用户兴趣特征的差异性表达。用户兴趣表达式如下：<br>$$<br>V_U(A) = f(V_A, e_1, e_2, \cdots, e_H) = \sum_{j=1}^H a(e_j, V_A) e_j = \sum_{j=1}^H w_j e_j<br>$$</p>
<p>公式中$a(e_j, V_A)$是一个前馈网络，输出对应的激活权重，其计算过程如下：</p>
<ol>
<li>Out Product：$<e_j, v_a="">$计算外积，得到等长的向量$V_p$。外积操作有利于relevance modeling；</e_j,></li>
<li>Concat：tf.concat([$e_j, V_A, V_P$], axis=1, keepdims=false)</li>
<li>PRelu或Dice：激活函数，这里有一层FC参数（W,b）；</li>
<li>Linear：转化为1维作为activation weight；</li>
</ol>
<p>这里的局部激活单元计算方法有别于传统attention，它放宽了权重累加和等于1（$\sum w_j = 1$）的限制条件，$w_j$有利于表达用户兴趣的差异化强度。因此无需对$a(e_j, V_A)$使用softmax归一化。</p>
<ul>
<li><strong>自适应激活函数（Dice）</strong></li>
</ul>
<p>PRelu函数的rectified point（校正点）是固定的0，这在每一层的输入分布发生变化时是不适用的，所以文章对该激活函数进行了改进，提出Dice激活函数，平滑了rectified point附近曲线的同时，<strong>Dice会根据每层输入数据的分布来自适应调整rectified point的位置</strong>。</p>
<p>PRelu激活函数表达式：</p>
<p>$$<br>f(x) =<br>\begin{cases}<br>\displaystyle<br>x &amp; \text{if x &gt; 0}, \\\<br>\alpha x &amp; \text{if x } \leq 0<br>\end{cases}<br>\longrightarrow<br>f(x)  = p(x) \cdot x + (1 - p(x)) \cdot \alpha x \quad<br>p(x) = I(x)   \quad // p(x) 是指示函数<br>$$<br>Dice函数的$p(x)$表达式不是指示函数，而是：<br>$$<br>p(x) = \frac{1} { 1 + \exp{ \left(- \frac{x - E(x)} {\sqrt{Var(x) + \epsilon} } \right) } }<br>$$</p>
<p>计算输入x的均值和方差，动态调整，$\epsilon=1e^{-8}$。</p>
<blockquote>
<p>Dice函数对模型效果的影响。列表：auc+0.1%；搜索：无提升</p>
</blockquote>
<ul>
<li><strong>mini-batch aware正则方法</strong></li>
</ul>
<p>// TODO</p>
<p><strong>DIN模型总结</strong></p>
<p>Base模型相比，DIN建模思想反映在模型结构上，差别主要体现在<strong>如何聚合多个用户行为Embedding向量</strong>，DIN建模考虑了用户兴趣的多样性，以及针对不同目标item，用户历史行为发挥的作用（weight）也是有差异的（局部激活）。</p>
<ol>
<li>Base模型中直接对多个Embedding向量进行等权的sum-pooling，这种方法肯定会带来信息的丢失，而且相对重要的Embedding向量也无法完全突出自己所包含的信息。DIN采取了一个比较直观的方式，就是weighted-sum pooling，而且Attention的本质也可以认为是weighted-sum，让模型更加关注有用的信息（有用信息的权重会更大一些）；</li>
<li>DIN不足：DIN仍然是直接把“用户行为”直接作为“兴趣”特征feed给模型，没有考虑<strong>兴趣具有动态性及演化漂移</strong>的特点，所以<strong>DIN抽取的兴趣之间是独立无关联的，不能捕获到兴趣的动态演进性</strong>。</li>
</ol>
<p>基于DIN存在的不足，DIN团队又提出了升级版模型－DIEN模型。</p>
<p>DIN实践总结</p>
<ol>
<li>Dice和BN可以提升模型效果；</li>
<li>Dice可以起到加速模型收敛的效果（loss收敛曲线）；</li>
<li>DIN attention对历史行为特征提升微弱（列表场景，行为稀疏？）</li>
</ol>
<p>// TODO 需要进一步实验，验证各个模块带来的效果；</p>
<h2 id="1.4.DNN+行为序列建模">DNN+行为序列建模</h2>

<h3 id="1.4.1.DIEN">DIEN</h3>

<ol>
<li>如何表示用户行为用法哪些item信息表示？ID和属性类结合？</li>
<li>兴趣抽取层计算辅助loss时，如何做负采样并得到向量？答：将下一个行为的embedding作为正例，然后除了user clicks之外的item集合中采样作为负例（可以是多个item）；</li>
<li>辅助loss的权重$\alpha$设置多少？辅助loss与全局loss之间的关系？</li>
<li>AIGRU, AGRU, AUGRU的实现区别是什么？</li>
</ol>
<p>DIEN关键词：兴趣动态性；兴趣漂移；兴趣抽取层（Interest Extractor）；兴趣演化层（Interest Evolution）；</p>
<p><strong>背景</strong></p>
<p>深度兴趣演进网络（Deep Interest Evolution Network，简称DIEN）的提出主要是为了解决非搜索场景下，如果通过设计模型来更好的捕获用户的动态变化的兴趣。</p>
<p><strong>DIEN模型特点</strong></p>
<ol>
<li>提出一个新的网络结构对<strong>用户兴趣演化过程</strong>进行建模，提升兴趣表达能力。</li>
<li><strong>兴趣抽取层</strong>。与直接把行为序列作为兴趣特征喂给模型不同，DIEN设计了特征抽取层，针对GRU的兴趣状态在表达用户兴趣时缺少指导目标，作者提出了<strong>辅助Loss</strong>的方法，辅助Loss利用连续的行为在每一步去监督学习隐含兴趣状态，从而让<strong>隐含兴趣状态更好的表达隐含兴趣</strong>。</li>
<li><strong>兴趣演进层</strong>。将GRU升级为AUGRU（Attention Update GRU）加强了兴趣与目标之间的相关性，目的是<strong>克服由兴趣漂移带来的预估不准的问题</strong>。</li>
</ol>
<p><strong>DIEN模型结构</strong></p>
<p>DIN与DIEN底层都是Embedding层，基础特征的处理方式是一致的。不同的是，DIEN将用户行为组织成了序列数据的形式，并把简单的使用out product完成的activation unit变成了attention-based GRU网路。</p>
<p>_兴趣抽取层（Interest Extractor Layer）_</p>
<p>兴趣抽取层的目标是<strong>从embedding数据中提取出每一个兴趣的状态信息</strong>。但是一个用户在某一时间的兴趣不仅与当前的behavior有关，也与之前的behavior相关，所以作者使用GRU单元提取一系列兴趣状态。为了学习的更加准确，引入了辅助loss用监督学习的方式。</p>
<blockquote>
<p>GRU克服了RNN梯度消失问题，同时快于LSTM，综合考量选择GRU。</p>
</blockquote>
<p>GRU表达式和解读参考系列总结<a href="http://www.52caml.com/deep_learning/dl-chapter4-rnn/#4.5.Gated_Recurrent_Units" target="_blank" rel="external">《深度学习》第4章：循环神经网络#GRU</a></p>
<p>如何评估GRU提取出的用户兴趣是否合理，是否准确？文中引入了辅助loss，通过监督学习来提升兴趣表达的准确性。</p>
<p>这里，作者设计了一个二分类模型来计算兴趣抽取的准确性，将用户下一时刻真实的行为$e_{t+1}$作为正例，负采样得到的行为作为负例，用$e_{t+1}^{’}$表示，分别与抽取出来的兴趣$h_t$结合输入到辅助网络中，得到预测结果，并通过logloss计算一个辅助损失。公式如下：</p>
<p>$$<br>L_{aux} = -\frac{1}{N} \left(\sum_{i=1}^N \sum_t \log \sigma(h_t, e_b^i[t+1]) + log(1 - \sigma(h_t, \hat{e}_b^i[t+1])) \right)<br>$$</p>
<p>全局损失函数为：$L = L_{target} + \alpha L_{aux}$，超参数是用来平衡兴趣表示和CTR预估的关系。</p>
<blockquote>
<p><strong>问题</strong>：特征抽取层引入GRU的目的是认为兴趣与当前行为有关，与之前行为也有关；引入辅助loss是为了监督学习兴趣的状态表达。那么既然这里的GRU和辅助loss都是为了更准确的学习兴趣表示，并且建模过程中已经用了当前行为的前后信息，为啥还要有下文_兴趣进化层_的必要性呢？</p>
</blockquote>
<p>TODO</p>
<p>_兴趣进化层（Interest Evolution Layer）_</p>
<p>兴趣进化层的目的是与target ad一起刻画用户兴趣的进化过程，该层结构是AUGRU。建模用户兴趣进化过程有两方面好处：</p>
<ol>
<li>追踪用户的interest可以使我们学习final interest的表达时包含更多的历史信息；</li>
<li>可以根据interest的变化趋势更好的进行CTR预测。（怎么证明？？）</li>
</ol>
<p>兴趣变化过程中有两点规律： </p>
<ol>
<li>兴趣漂移（interest shift）：用户在某一段时间的interest有一定的集中性，兴趣之间有一定的跳跃；</li>
<li>兴趣独立（interest individual）：不同种类的interest之间很少相互影响；</li>
</ol>
<p><strong>DIEN实践总结</strong></p>
<ol>
<li>数据特征方面<ul>
<li>模型效果与行为序列长度正相关，序列越长效果越佳（10,20,30,50）；</li>
<li>Item维度，属性信息增加（POI，3级品类，商圈基础上扩充2级品类、城市信息等），效果不明显；</li>
<li>提升训练天数&amp;&amp;降低采样率，模型效果有提升；</li>
</ul>
</li>
<li>模型结构方面<ul>
<li>单层GRU不如双层GRU效果好，后者提升0.15%左右；</li>
<li>辅助loss的权重对效果不敏感，$\alpha=0.5,2$离线模型效果差异不大；</li>
<li>learning rate对效果很敏感，可作为重点调参对象；</li>
<li>embedding size对效果不敏感（8,12,16均有尝试），耗时增加明显；</li>
</ul>
</li>
</ol>
<h3 id="1.4.2.DSIN">DSIN</h3>

<p>深度会话兴趣网络（Deep Session Interest Network）</p>
<h3 id="1.4.3.超长序列建模">超长序列建模</h3>

<p>关键词：MIMN（2019），SIM（2020）等</p>
<p>超长序列建模存在计算与存储问题，通常有两种解决方案：用户兴趣向量表示（<a href="https://zhuanlan.zhihu.com/p/163511597" target="_blank" rel="external">UIC/MIMN</a>），抽取子序列（SIM）。</p>
<p>在MIMN文章中，线上引入了UIC模块（User Interest Center）来将用户兴趣建模部分从整个模型中解耦出来。那么，用户兴趣建模存在存储与计算性能2个关键问题，提出了<strong>多通道的用户兴趣记忆网络</strong>，即MIMN（是对Neural Turing Machine的改进）。MIMN借助了memory network的思想，将用户的多样兴趣映射到一个固定大小的memory矩阵，这个矩阵会在有新的用户行为时更新，通过这种方式，用户兴趣的计算从CTR预估中进行了解耦。</p>
<p>然而，建模任意长度的序列数据对于memory network仍有很大挑战。实际上，当用户行为长度进一步增加，比如增加10倍或者更多，MIMN就不能准确地得到给定特定候选商品的用户兴趣，这是因为将所有的用户历史行为编码到固定大小的memory matrix会给memory unit带来很多噪音。</p>
<p>因此，阿里妈妈定向广告算法团队今年发出了一个文章《Search-based User Interest Modeling》（简称SIM）。它的做法比较好理解，根据不同的候选商品从用户超长序列中搜索与候选商品相关的序列，选择TopK，这样就得到了用户子序列，然后拿子序列与候选商品进行attention建模。文章中给了搜索方法：hard-search和soft-search。hardsearch就是对用户行为按照商品类目划分，候选商品的类目与用户行为做匹配；softsearch方法对行为item表示为一个向量，使用类似LSH方法选择topk。<br>最后实验发现，这两种搜索方法得到的子序列相似度很高，尽管soft-search model在离线实验中比hard-search model稍微好些，权衡收益和资源消耗，最终选择在我们系统中用hard-search model部署SIM。</p>
<p>在到店搜索广告场景，SIM借鉴意义不大，首先我们一直与候选ad类目相同的用户子序列建模，对应到sim已经使用了hardsearch方式。在用户美食行为序列仍然会很长（超过1000），尤其是点评用户（两侧数据融合）。</p>
<p>阿里在2020KDD发表了一篇文章讲述利用从用户点击序列中学习用户多兴趣，即使用多个兴趣向量表示一个用户，论文名称：《Controllable Multi-Interest Framework for Recommendation》。</p>
<h2 id="1.5.DNN+多任务学习">DNN+多任务学习</h2>

<h3 id="1.5.1.A-Multitask-Ranking-System">A Multitask Ranking System</h3>

<p>TODO</p>
<p>参考YouTube论文：《Recommending What Video to Watch Next: A Multitask Ranking System》</p>
<h3 id="1.5.2.ESMM">ESMM</h3>

<p>全空间多任务模型（Entire Space Multi-Task Model, 简称ESMM）是阿里妈妈在2018年提出的一篇关于CVR建模场景的模型。</p>
<table>
<thead>
<tr>
<th>CVR建模对比</th>
<th>数据</th>
<th>模型</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>传统CVR建模</td>
<td>点击－转化样本</td>
<td>二分类模型</td>
<td>1. 样本选择偏差：训练基于clk样本空间，而预测基于imp空间，样本有偏，违背了数据IID前提；<br>2. 数据稀疏：点击样本占整体样本空间比例低，因稀疏导致训练不充分；</td>
</tr>
<tr>
<td>ESMM</td>
<td>曝光－点击－转化样本</td>
<td>多任务模型</td>
<td>1. 完整样本空间学习pCTR,pCTCVR两个任务</td>
</tr>
</tbody>
</table>
<p>ESMM建模思想</p>
<p>预估条件概率，用y表示点击label，z表示转化label，那么有：</p>
<p>$$<br>\underbrace{p(y=1,z=1 | x)}_{\text{pCTCVR}}<br>= \underbrace{p(y=1|x)}_{\text{pCTR}} \cdot<br>\underbrace{p(z=1|y=1,x)}_{\text{pCVR}}<br>$$</p>
<p>ESMM借鉴了多任务学习（迁移学习）的思想，引入了两个辅助任务，分别拟合pCTR和pCTCVR，把pCVR作为一个中间变量，从而减弱传统CVR模型建模的两个弊端。该模型的几个特点总结如下：</p>
<ul>
<li>两个子网络，分别对pCVR和pCTR建模；</li>
<li>共享embedding：两个子网络的输入特征（CVR-task features和CTR-task features）共享embedding；</li>
<li>两个子任务的loss function均是交叉熵损失，对应的label数据有两组，分别是$label_{ctr}和$label_{cvr}$；</li>
</ul>
<p>注意：<strong>共享embedding的好处是CVR子网络能够从只曝光没点击的样本中学习特征表示，有利于缓解CVR训练数据稀疏的问题(迁移学习思想)。</strong></p>
<blockquote>
<p>问题：从工程实现上看，两个网络的输入特征是有差异的，特征工程上如何区分一条样本哪些属于cvr特征，哪些属于ctr特征？ 还是不予区分？</p>
</blockquote>
<h3 id="1.5.2.MMOE">MMOE</h3>

<p>上面的ESMM对CVR建模是通过学习pCTCVR和pCTR两个辅助任务，以及它们之间的关系得到主任务pCVR的结果。ESMM多任务学习是为了解决单一目标（CVR预估）学习问题而建模的。</p>
<p>那么，针对多个优化目标任务时，同样可以使用多任务建模。但是，在实践中多任务学习模型并不总比单任务模型效果更突出，这是因为<strong>不同任务之间的相关性低（如数据的分布不同等）导致的。</strong></p>
<p>2018年由谷歌内容推荐团队发表的MMOE模型，针对多任务学习场景充分考虑了任务之间的相关性而提出了MMOE模型（Multi-gate Multi-task of ）</p>
<p>具体介绍MMOE模型之前，我们先看一下。</p>
<p><strong>任务相关性</strong></p>
<p>一般的多任务学习模型框架是<strong>Shared-bottom结构</strong>。针对不同的任务，底层参数和网络结构是共享的，然后上层经过不同的神经网络得到对应任务的输出。假设shared-bottom输出是$f(x)$，那么第$k$个任务的输出$y_k = h^k(f(x))$，$h^k$为第k个任务的网络结构；</p>
<p>任务相关性和多任务学习效果之间的关系是怎样的呢？论文中通过模拟给出了结论：利用皮尔逊相关系数计算label之间的相关性 vs 任务权重cosine相似度（任务相关性），发现二者呈现正相关关系。</p>
<p>因此MMOE使用参数的cosine距离来近似表示任务之间的相关性。</p>
<p><strong>MoE模型</strong></p>
<p>MoE（One-gate Mixture-of-Experts）模型的特点是：_多个任务共享gating network_，结构如下：</p>
<ol>
<li>Shared-bottom层：由n个Export Network组成，得到n个Export输出（向量），ensemble model思想，有助于提升模型效果；</li>
<li>一个Gate network：使不同的数据可以多样化的使用共享层。</li>
<li>多目标层：与shared-bottom结构一致；</li>
</ol>
<p>共享层输出，相当于<strong>weighted-sum of all experts</strong>：</p>
<p>$$<br>y = \sum_{i=1}^n g(x)_i f_i(x)<br>$$</p>
<p>其中$f_i$表示第i个expert的输出，$\sum_{i=1}^n g(x)_i = 1, g_i$表示第i个expert对应的权重（标量），是基于输入数据得到的，计算公式为$g(x)=softmax(W_g x)$。</p>
<p><strong>问题：如何设计gate network？n个expert网络的输入是一致的？如何区分n个expert的参数？？</strong></p>
<p>答：gate network输出unit size＝n，网络结构可以是fc；</p>
<p><strong>MMoE模型</strong></p>
<p>相对于MoE模型，Multi-gate Mixture-of-Experts模型<strong>为每个task设置一个gate</strong>，使不同的任务和不同的数据可以多样化的使用共享层。</p>
<blockquote>
<p>MMoE模型：任务数与gating network数量一致；</p>
</blockquote>
<p>此时每个任务的共享层输出不同，第k个任务的共享层输出计算公司如下：</p>
<p>$$<br>f^k(x) = \sum_{i=1}^k g^k(x)_i f_i(x) \\\<br>g^k(x) = \text{softmax}(W_{gk} x)<br>$$</p>
<p>经过多层FC得到每个任务的输出：</p>
<p>$$<br>y_k = h^k(f^k(x))<br>$$</p>
<p>直观上考虑，如果两个任务并不十分相关，那么经过gate后，二者得到的权重系数会差别比较大，从而可以利用部分expert网络输出的信息，近似于多个单任务学习模型。如果两个任务紧密相关，那么经过gate得到的权重分布应该相差不多，类似于一般的多任务学习框架。</p>
<p>总结MMOE：</p>
<ul>
<li>共享层</li>
</ul>
<h1 id="1.预估算法工程">预估算法工程</h1>

<p>在个人总结：《AiTools-算法能力沉淀》有记录，可以先总结，后迁；</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://www.52caml.com/deep_learning/dl-chapter10-ctrcvr-dev/" data-title="计算广告与机器学习" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 

<div class="next">
<a href="/deep_learning/dl-chapter9-recall-algorithm-dev/"  title="">
 <strong>下一篇：</strong><br/> 
 <span>(no title)
</span>
</a>
</div>

</nav>

	

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习预估系统-算法模型与算法工程"><span class="toc-number">1.</span> <span class="toc-text">机器学习预估系统-算法模型与算法工程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0.预估算法系统综述"><span class="toc-number">2.</span> <span class="toc-text">预估算法系统综述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#点击率预估"><span class="toc-number">2.1.</span> <span class="toc-text">点击率预估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#转化率预估"><span class="toc-number">2.2.</span> <span class="toc-text">转化率预估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#交易额预估"><span class="toc-number">2.3.</span> <span class="toc-text">交易额预估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1.预估算法模型"><span class="toc-number">3.</span> <span class="toc-text">预估算法模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1.3.Embedding+NN模型"><span class="toc-number">3.1.</span> <span class="toc-text">Embedding+NN模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1.3.1.WideAndDeep"><span class="toc-number">3.1.1.</span> <span class="toc-text">Wide&Deep</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.3.2.DeepFM"><span class="toc-number">3.1.2.</span> <span class="toc-text">DeepFM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.3.3.DIN"><span class="toc-number">3.1.3.</span> <span class="toc-text">DIN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1.4.DNN+行为序列建模"><span class="toc-number">3.2.</span> <span class="toc-text">DNN+行为序列建模</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1.4.1.DIEN"><span class="toc-number">3.2.1.</span> <span class="toc-text">DIEN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.4.2.DSIN"><span class="toc-number">3.2.2.</span> <span class="toc-text">DSIN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.4.3.超长序列建模"><span class="toc-number">3.2.3.</span> <span class="toc-text">超长序列建模</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1.5.DNN+多任务学习"><span class="toc-number">3.3.</span> <span class="toc-text">DNN+多任务学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1.5.1.A-Multitask-Ranking-System"><span class="toc-number">3.3.1.</span> <span class="toc-text">A Multitask Ranking System</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.5.2.ESMM"><span class="toc-number">3.3.2.</span> <span class="toc-text">ESMM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1.5.2.MMOE"><span class="toc-number">3.3.3.</span> <span class="toc-text">MMOE</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1.预估算法工程"><span class="toc-number">4.</span> <span class="toc-text">预估算法工程</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/OpenMIT/" title="OpenMIT">OpenMIT<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/分布式机器学习/" title="分布式机器学习">分布式机器学习<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习/图网络/" title="图网络">图网络<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/强化学习与智能决策/" title="强化学习与智能决策">强化学习与智能决策<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/概率与统计/" title="概率与统计">概率与统计<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/深入浅出机器学习/" title="深入浅出机器学习">深入浅出机器学习<sup>9</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习/" title="深度学习">深度学习<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/编程语言与技术/" title="编程语言与技术">编程语言与技术<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Agent/" title="Agent">Agent<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/参数服务器/" title="参数服务器">参数服务器<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Attention/" title="Attention">Attention<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/DP/" title="DP">DP<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Evalation/" title="Policy Evalation">Policy Evalation<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Improvement/" title="Policy Improvement">Policy Improvement<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Iteration/" title="Policy Iteration">Policy Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Value-Iteration/" title="Value Iteration">Value Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Greedy-Policy/" title="Greedy Policy">Greedy Policy<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/连续随机变量/" title="连续随机变量">连续随机变量<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/MDP/" title="MDP">MDP<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Markov-Decision-Process/" title="Markov Decision Process">Markov Decision Process<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/RL/" title="RL">RL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Environments/" title="Environments">Environments<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/GD/" title="GD">GD<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/FTRL/" title="FTRL">FTRL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaGrad/" title="AdaGrad">AdaGrad<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaDelta/" title="AdaDelta">AdaDelta<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Adam/" title="Adam">Adam<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello, Welcome to CAML technology sharing platform.  <br/>
			I&#39;m Zhou Yong, engaged in algorithms work on computational advertising and machine learning.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1707438033" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/ComputationalAds" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:zhouyongsdzh@foxmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2020 
		
		<a href="/about" target="_blank" title="ZhouYong">ZhouYong</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>









<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
