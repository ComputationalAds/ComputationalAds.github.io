
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>计算广告与机器学习</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="ZhouYong">
    

    
    <meta name="description" content="广告是流量变现的主要手段！鄙站主要梳理在线广告的主要产品形态，并就不同产品形态中的涉及的核心技术，尤其是大规模机器学习、投放推荐策略、基础数据挖掘以及博弈论（机制、均衡、拍卖）等方面，分享一些解决方案、经验教训和走过的坑儿！">
<meta property="og:type" content="article">
<meta property="og:title" content="计算广告与机器学习">
<meta property="og:url" content="http://www.52caml.com/page/2/index.html">
<meta property="og:site_name" content="计算广告与机器学习">
<meta property="og:description" content="广告是流量变现的主要手段！鄙站主要梳理在线广告的主要产品形态，并就不同产品形态中的涉及的核心技术，尤其是大规模机器学习、投放推荐策略、基础数据挖掘以及博弈论（机制、均衡、拍卖）等方面，分享一些解决方案、经验教训和走过的坑儿！">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="计算广告与机器学习">
<meta name="twitter:description" content="广告是流量变现的主要手段！鄙站主要梳理在线广告的主要产品形态，并就不同产品形态中的涉及的核心技术，尤其是大规模机器学习、投放推荐策略、基础数据挖掘以及博弈论（机制、均衡、拍卖）等方面，分享一些解决方案、经验教训和走过的坑儿！">

    
    <link rel="alternative" href="/atom.xml" title="计算广告与机器学习" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="计算广告与机器学习" title="计算广告与机器学习"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="计算广告与机器学习">计算广告与机器学习</a></h1>
				<h2 class="blog-motto">Computational Advertising and Machine Learning</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/home">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:www.52caml.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/hello-world/" title="测试Hexo功能" itemprop="url">测试Hexo功能</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2016-02-13T03:21:55.000Z" itemprop="datePublished"> 发表于 2016-02-13</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="公式">公式</h2><p>$$J_\alpha(x)=\sum_{m=0}^\infty \frac{(-1)^ m}{m! \, \Gamma (m + \alpha + 1)}{\left({\frac{x}{2}}\right)}^{2 m + \alpha }$$</p>
<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h4 id="1-2-1_线性回归模型">1.2.1 线性回归模型</h4><ul>
<li><p>模型表达</p>
<p>  $$<br>  y(x, w) = w_0 + w_1 x_1 + \cdots + w_n x_n \quad (n \ge 1) \qquad (ml.1.1.1)<br>  $$ </p>
<p>  其中，\(x_1,x_2,\cdots,x_n\)表示自变量（集合），\(y\)是因变量，\(w\)为参数向量，\(w_i\)表示对应自变量（特征）的权重，\(w_0\)是偏倚项（又称为截距）。</p>
<blockquote>
<p> 关于参数\(w\)： <br></p>
<ol>
<li>在物理上可以这样解释：<strong>在自变量（特征）之间相互独立的前提下</strong>，\(w_i\)反映自变量\(x_i\)对因变量影响程度，\(w_i\)越大，说明\(x_i\)对结果\(y\)的影响越大。<br></li>
<li>通过每个字变量（特征）前面的参数，可以很直观的看出哪些特征分量对结果的影响比较大。<br></li>
<li>在统计中，\(w_1,w_2,\cdots,w_n\)称为偏回归系数，\(w_0\)称为截距。</li>
</ol>
</blockquote>
<p>  如果令\(x_0=1, y(x,w)=h_{w}(x)\), 可以将公式\((ml.1.1.1)\)写成向量形式，即：</p>
<p>  $$<br>  h_{w}(x) = \sum_{i=0}^{n} w_i x_i = w^T x \qquad(ml.1.1.2)<br>  $$</p>
<p>  其中，\(w=(w_0, w_1, \cdots, w_n)\)，\(x=(1, x_1, x_2, \cdots, x_n)\) 均为向量，\(w^T\)为\(w\)的转置。</p>
</li>
</ul>
<p>   上式是对一条样本进行建模的数据表达。对于多条样本，假设每条样本生成过程独立，在整个样本空间中（\(m\)个样本）的概率分布为：</p>
<p>   $$P(Y|X; w) = \prod_{i=1}^{m} \left( (h_{w}(x^{(i)}))^{y^{(i)}} \cdot (1 - h_{w}(x^{(i)}))^{1-y^{(i)}} \right) \qquad(ml.1.8)$$</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter9-factorization-family/" title="第09章：深入浅出ML之Factorization家族" itemprop="url">第09章：深入浅出ML之Factorization家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2016-01-12T14:43:47.000Z" itemprop="datePublished"> 发表于 2016-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-12-18</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>Factorization Machine</li>
<li>Field-aware Factorization Machine</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<ul>
<li>FM</li>
<li>FFM</li>
</ul>
<p><br></p>
<h3 id="因子分解机">因子分解机</h3><hr>
<p>因子分解机（Factorization Machine，简称FM），又称分解机器。是由<a href="http://www.uni-konstanz.de/" target="_blank" rel="external">Konstanz大学（德国康斯坦茨大学）</a>Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？</p>
<p>用户在网站上的行为数据会被Server端以日志的形式记录下来，这些数据通常会存放在多台存储机器的硬盘上。</p>
<blockquote>
<p>以<a href="http://www.sina.com.cn/" target="_blank" rel="external">我浪</a>为例，各产品线纪录的用户行为日志会通过flume等日志收集工具交给数据中心托管，它们负责把数据定时上传至HDFS上，或者由数据中心生成Hive表。</p>
</blockquote>
<p>我们会发现日志中大多数出现的特征是categorical类型的，这种特征类型的取值仅仅是一个标识，本身并没有实际意义，更不能用其取值比较大小。比如日志中记录了用户访问的频道（channel）信息，如”news”, “auto”, “finance”等。</p>
<blockquote>
<p>假设channel特征有10个取值，分别为\(\{\text{“auto”}, \text{“finance”}, \text{“ent”}, \text{“news”}, \text{“sports”}, \text{“mil”}, \text{“weather”}, \text{“house”}, \text{“edu”}, \text{“games”} \}\)。部分训练数据如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">user</th>
<th style="text-align:center">channel</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">user1</td>
<td style="text-align:center">sports</td>
</tr>
<tr>
<td style="text-align:center">user2</td>
<td style="text-align:center">news</td>
</tr>
<tr>
<td style="text-align:center">user3</td>
<td style="text-align:center">finance</td>
</tr>
<tr>
<td style="text-align:center">user4</td>
<td style="text-align:center">house</td>
</tr>
<tr>
<td style="text-align:center">user5</td>
<td style="text-align:center">edu</td>
</tr>
<tr>
<td style="text-align:center">user6</td>
<td style="text-align:center">news</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</blockquote>
<p><strong>特征ETL</strong>过程中，需要对categorical型特征进行one-hot编码（独热编码），即将categorical型特征转化为数值型特征。channel特征转化后的结果如下：</p>
<blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">user</th>
<th style="text-align:center">chn-auto</th>
<th style="text-align:center">chn-finance</th>
<th style="text-align:center">chn-ent</th>
<th style="text-align:center">chn-news</th>
<th style="text-align:center">chn-sports</th>
<th style="text-align:center">chn-mil</th>
<th style="text-align:center">chn-weather</th>
<th style="text-align:center">chn-house</th>
<th style="text-align:center">chn-edu</th>
<th style="text-align:center">chn-games</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">user1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user6</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</blockquote>
<p>可以发现，<strong>由one-hot编码带来的数据稀疏性会导致特征空间变大</strong>。上面的例子中，一维categorical特征在经过one-hot编码后变成了10维数值型特征。真实应用场景中，未编码前特征总维度可能仅有数十维或者到数百维的categorical型特征，经过one-hot编码后，达到数千万、数亿甚至更高维度的数值特征在业内都是常有的。</p>
<blockquote>
<p>我组广告和推荐业务的点击预估系统，编码前是特征不到100维，编码后（包括feature hashing）的维度达百万维量级。</p>
</blockquote>
<p>此外也能发现，<strong>特征空间增长的维度取决于categorical型特征的取值个数</strong>。在数据稀疏性的现实情况下，我们如何去利用这些特征来提升learning performance？</p>
<p>或许在学习过程中考虑特征之间的关联信息。针对特征关联，我们需要讨论两个问题：1. 为什么要考虑特征之间的关联信息？2. 如何表达特征之间的关联？</p>
<ol>
<li><p>为什么要考虑特征之间的关联信息？</p>
<p> 大量的研究和实际数据分析结果表明：某些特征之间的关联信息（相关度）对事件结果的的发生会产生很大的影响。从实际业务线的广告点击数据分析来看，也正式了这样的结论。</p>
</li>
<li><p>如何表达特征之间的关联？</p>
<p> 表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征：</p>
<ol>
<li>人工特征工程（数据分析＋人工构造）；</li>
<li>通过模型做组合特征的学习（深度学习方法、FM/FFM方法）</li>
</ol>
</li>
</ol>
<p>本章主要讨论FM和FFM用来学习特征之间的关联。我们在<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>看到的多项式回归模型，其中的交叉因子项\(x_i x_j\)就是组合特征最直观的例子。</p>
<blockquote>
<p>\(x_i x_j\)表示特征\(x_i\)和\(x_j\)的组合，当\(x_i\)和\(x_j\)都非零时，组合特征\(x_i x_j\)才有意义。</p>
</blockquote>
<p>这里我们以二阶多项式模型（degree=2时）为例，来分析和探讨FM原理和参数学习过程。</p>
<p><br></p>
<h4 id="FM模型表达">FM模型表达</h4><hr>
<p>为了更好的介绍FM模型，我们先从多项式回归、交叉组合特征说起，然后自然地过度到FM模型。</p>
<p><strong>二阶多项式回归模型</strong></p>
<p>我们先看二阶多项式模型的表达式：</p>
<blockquote>
<p>$$<br>\hat{y}(x) := \underbrace {w_0 + \sum_{i=1}^{n} w_i x_i }_{\text{线性回归}} + \underbrace {\sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j}_{\text{交叉项（组合特征）}} \qquad \text{(n.ml.1.9.1)}<br>$$</p>
</blockquote>
<p>其中，\(n\)表示样本特征维度，截距\(w_0 \in R,\; w ＝ \{w_1, w_2, \cdots, w_n\}\in R^n, w_{ij} \in R^{n \times n}\)为模型参数。</p>
<p>从公式\(\text{(n.ml.1.9.1)}\)可知，交叉项中的组合特征参数总共有\(\frac{n(n-1)}{2}\)个。在这里，<strong>任意两个交叉项参数\(w_{ij}\)都是独立的</strong>。然而，在数据非常稀疏的实际应用场景中，交叉项参数的学习是很困难的。why？</p>
<p>因为我们知道，<strong>回归模型的参数\(w\)的学习结果就是从训练样本中计算充分统计量（凡是符合<a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank" rel="external">指数族分布</a>的模型都具有此性质）</strong>，而在这里交叉项的每一个参数\(w_{ij}\)的学习过程需要大量的\(x_i、x_j\)同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“\(x_i\)和\(x_j\)都非零”的样本数就会更少。训练样本不充分，学到的参数\(w_{ij}\)就不是充分统计量结果，导致参数\(w_{ij}\)不准确，而这会严重影响模型预测的效果（performance）和稳定性。How to do it ?</p>
<p>那么，如何在降低数据稀疏问题给模型性能带来的重大影响的同时，有效地解决二阶交叉项参数的学习问题呢？矩阵分解方法已经给出了解决思路。这里借用CMU讨论课中提到的<a href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf" target="_blank" rel="external">FM课件</a>和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">美团－深入FFM原理与实践</a>中提到的矩阵分解例子（美团技术团队的分享很赞👍）。</p>
<blockquote>
<p>在基于Model-Based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。如下图所示。</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_1_fm_mf_example.png" width="700" height="400" alt="cf_mf"></p>
<p>上图把每一个user表示成了一个二维向量，同时也把item表示成一个二维向量，两个向量的内积就是矩阵中user对item的打分。</p>
</blockquote>
<p>根据矩阵分解的启发，如果把多项式模型中二阶交叉项参数\(w_{ij}\)组成一个对称矩阵\(W\)（对角元素设为正实数），那么这个矩阵就可以分解为\(W = V V^T\)，\(V \in R^{n \times k}\)称为系数矩阵，其中第\(i\)行对应着第\(i\)维特征的隐向量 (这部分在FM公式解读中详细介绍)。</p>
<p><strong>将每个交叉项参数\(w_{ij}\)用隐向量的内积\(\langle \mathbf{v}_i, \mathbf{v}_j\rangle\)表示，是FM模型的核心思想</strong>。下面对FM模型表达式和参数求解过程，给出详细解读。</p>
<p><strong>FM模型表达</strong></p>
<p>这里我们只讨论二阶FM模型（degree＝2），其表达式为：</p>
<p>$$<br>\hat{y}(\mathbf{x}) := w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j \qquad \text{(ml.1.9.1)}<br>$$  </p>
<p>其中，\(\mathbf{v}_i\)表示第\(i\)特征的隐向量，\(\langle \cdot, \cdot\rangle\)表示两个长度为\(k\)的向量的内积，计算公式为：</p>
<p>$$<br>\langle \mathbf{v}_i, \mathbf{v}_j \rangle := \sum_{f=1}^{k} v_{i,f} \cdot v_{j,f} \qquad \text{(ml.1.9.2)}<br>$$</p>
<blockquote>
<p>公式解读：</p>
<ul>
<li><p><strong>线性模型＋交叉项</strong></p>
<p> 直观地看FM模型表达式，前两项是<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/#线性回归模型" target="_blank" rel="external">线性回归模型</a>的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将<strong>两个互异的特征分量</strong>之间的关联信息考虑进来。<strong>用交叉项表示组合特征，从而建立特征与结果之间的非线性关系</strong>。</p>
</li>
<li><p><strong>交叉项系数 \(\to\) 隐向量内积</strong></p>
<p> 由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数\(w_{ij}\)（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积\(\langle \mathbf{v}_i, \mathbf{v}_j\rangle\)表示\(w_{ij}\)。</p>
<p>具体的，FM求解过程中的做法是：对每一个特征分量\(x_i\)引入**隐向量\(\mathbf{v}_i ＝ (v_{i,1}, v_{i,2}, \cdots, v_{i,k})\)**，利用\(v_i v_j^T\)内积结果对交叉项的系数\(w_{ij}\)进行估计，公式表示：\(\hat{w}_{ij} := v_i v_j^T\).</p>
</li>
</ul>
</blockquote>
<p>隐向量的长度\(k\)称为超参数\((k \in N^+, k \ll n)\)，\(\mathbf{v}_i = (v_{i,1}, v_{i,2}, \cdots, v_{i,k})\)的含义是用\(k\)个描述特征的因子来表示第\(i\)维特征。根据公式\(\text{(ml.1.9.1)}\)，二阶交叉项的参数由\(n \cdot n\)个减少到\(n \cdot k\)个，远少于二阶多项式模型中的参数数量。</p>
<p>此外，<strong>参数因子化表示后，使得\(x_h x_i\)的参数与\(x_i x_j\)的参数不再相互独立</strong>。这样我们就可以在样本稀疏情况下相对合理的估计FM模型交叉项的参数。具体地：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\langle \mathbf{v}_h, \mathbf{v}_i \rangle &amp;:= \sum_{f=1}^{k} v_{h,f} \cdot v_{i,f}  \quad(1)\\<br>\langle \mathbf{v}_i, \mathbf{v}_j \rangle &amp;:= \sum_{f=1}^{k} v_{i,f} \cdot v_{j,f} \;\quad(2)<br>\end{align} \qquad (n.ml.1.9.2)<br>$$</p>
</blockquote>
<p>\(x_h x_i\)与\(x_i x_j\)的系数分别为\(\langle \mathbf{v}_h, \mathbf{v}_i \rangle\)和\(\langle \mathbf{v}_i, \mathbf{v}_j \rangle\)，他们之间有共同项\(\mathbf{v}_i\)。也就是说，所有包含\(x_i\)的非零组合特征（存在某个\(j \neq i\)，使得\(x_ix_j \neq 0\)）的样本都可以用来学习隐向量\(\mathbf{v}_i\)，这在很大程度上避免了数据稀疏行造成参数估计不准确的影响。</p>
<blockquote>
<p>在二阶多项式模型中，参数\(w_{hi}\)和\(w_{ij}\)的学习过程是相互独立的。</p>
</blockquote>
<p>论文中还提到FM模型的应用场景，并且说公式\(\text{(ml.1.9.1)}\)作为一个通用的拟合模型（Generic Model），可以采用不同的损失函数来解决具体问题。比如：</p>
<table>
<thead>
<tr>
<th style="text-align:center">FM应用场景</th>
<th style="text-align:center">损失函数</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">回归</td>
<td style="text-align:center">均方误差（MSE）损失</td>
<td style="text-align:center">Mean Square Error，与平方误差类似</td>
</tr>
<tr>
<td style="text-align:center">二类分类</td>
<td style="text-align:center">Hinge/Cross-Entopy损失</td>
<td style="text-align:center">分类时，结果需要做sigmoid变换</td>
</tr>
<tr>
<td style="text-align:center">排序</td>
<td style="text-align:center">.</td>
<td style="text-align:center">. </td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="FM参数学习">FM参数学习</h4><hr>
<p><strong>等式变换</strong></p>
<p>公式\(\text{(ml.1.9.1)}\)中直观地看，FM模型的复杂度为\(O(kn^2)\)，但是通过下面的等价转换，可以将FM的二次项化简，其复杂度可优化到\(O(kn)\)。即：</p>
<p>$$<br>\sum_{i=1}^{n} \sum_{j=i+1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j = \frac{1}{2} \sum_{f=1}^{k} {\left \lgroup \left(\sum_{i=1}^{n} v_{i,f} x_i \right)^2 - \sum_{i=1}^{n} v_{i,f}^2 x_i^2\right \rgroup} \qquad(ml.1.9.3)<br>$$</p>
<p>下面给出详细推导：</p>
<blockquote>
<p>$$<br>\begin{align}<br>&amp; \sum_{i=1}^{n} \sum_{j=i+1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j \qquad\qquad\qquad\qquad\qquad\qquad(1)\\<br>= &amp; \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j - \frac{1}{2} \sum_{i=1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_i \rangle} x_i x_i \qquad\qquad\;\;(2)\\<br>= &amp; \frac{1}{2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} v_{i,f} v_{j,f} x_i x_j - \sum_{i=1}^{n} \sum_{f=1}^{k} v_{i,f} v_{i,f} x_i x_i \right) \qquad\,(3) \\<br>= &amp; \frac{1}{2} \sum_{f=1}^{k} {\left \lgroup \left(\sum_{i=1}^{n} v_{i,f} x_i \right) \cdot \left(\sum_{j=1}^{n} v_{j,f} x_j \right) - \sum_{i=1}^{n} v_{i,f}^2 x_i^2 \right \rgroup} \quad\;\;\,(4) \\<br>= &amp; \frac{1}{2} \sum_{f=1}^{k}  {\left \lgroup \left(\sum_{i=1}^{n} v_{i,f} x_i \right)^2 - \sum_{i=1}^{n} v_{i,f}^2 x_i^2\right \rgroup} \qquad\qquad\qquad\;\;(5)<br>\end{align} \qquad(n.ml.1.9.3)<br>$$</p>
<p>解读第（1）步到第（2）步，这里用\(A\)表示系数矩阵\(V\)的上三角元素，\(B\)表示对角线上的交叉项系数。由于系数矩阵\(V\)是一个对称阵，所以下三角与上三角相等，有下式成立：</p>
<p>$$<br>A = \frac{1}{2} (2A+B) - \frac{1}{2} B.  \quad \underline{ A=\sum_{i=1}^{n} \sum_{j=i+1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j } ; \quad \underline{ B = \frac{1}{2} \sum_{i=1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_i \rangle} x_i x_i } \quad (n.ml.1.9.4)<br>$$</p>
</blockquote>
<p>如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：</p>
<p>$$<br>\frac{\partial}{\partial \theta} y(\mathbf{x}) =<br>\left \{<br>\begin{array}{ll}<br>1,         &amp; \text{if}\; \theta\; \text{is}\; w_0 \qquad \text{(常数项)} \\<br>x_i     &amp; \text{if}\; \theta\; \text{is}\; w_i \;\qquad \text{(线性项)} \\<br>x_i \sum_{j=1}^{n} v_{j,f} x_j - v_{i,f} x_i^2, &amp; \text{if}\; \theta\; \text{is}\; v_{i,f} \qquad \text{(交叉项)}<br>\end{array}<br>\right. \qquad\quad(ml.1.9.4)<br>$$</p>
<p>其中， \(v_{j,f}\)是隐向量\(\mathbf{v}_j\)的第\(f\)个元素。</p>
<p><strong>梯度法训练FM</strong></p>
<p>给出伪代码</p>
<p><strong>FM训练复杂度</strong></p>
<p>由于\(\sum_{j=1}^{n} v_{j,f} x_j\)只与\(f\)有关，在参数迭代过程中，只需要计算第一次所有\(f\)的\(\sum_{j=1}^{n} v_{j,f} x_j\)，就能够方便地得到所有\(v_{i,f}\)的梯度。显然，计算所有\(f\)的\(\sum_{j=1}^{n} v_{j,f} x_j\)的复杂度是\(O(kn)\)；已知\(\sum_{j=1}^{n} v_{j,f} x_j\)时，计算每个参数梯度的复杂度是\(O(n)\)；得到梯度后，更新每个参数的复杂度是 \(O(1)\)；模型参数一共有\(nk + n + 1\)个。因此，FM参数训练的时间复杂度为\(O(kn)\)。</p>
<p>综上可知，FM算法可以在线性时间内完成模型训练，以及对新样本做出预测，所以说FM是一个非常高效的模型。</p>
<p><br></p>
<h4 id="FM总结">FM总结</h4><hr>
<p>上面我们主要是从FM模型引入（多项式开始）、模型表达和参数学习的角度介绍的FM模型，这里我把我认为FM最核心的精髓和价值总结出来，与大家讨论。FM模型的核心作用可以概括为以下3个：</p>
<p><strong>1. <font color="#EE0000">FM降低了交叉项参数学习不充分的影响</font></strong></p>
<p>one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用\(k\)维的隐向量表示，交叉项的参数\(w_{ij}\)用对应特征隐向量的内积表示，即\(\langle \mathbf{v}_i, \mathbf{v}_j \rangle\)（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数\(w_{ij}\)的过程，转变为学习\(n\)个单特征对应\(k\)维隐向量的过程。</p>
<p>很明显，单特征参数（\(k\)维隐向量\(\mathbf{v}_i\)）的学习要比交叉项参数\(w_{ij}\)学习得更充分。示例说明：</p>
<blockquote>
<p>假如有10w条训练样本，其中出现<code>女性</code>特征的样本数为3w，出现<code>男性</code>特征的样本数为7w，出现<code>汽车</code>特征的样本数为2000，出现<code>化妆品</code>的样本数为1000。特征共现的样本数如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">共现交叉特征</th>
<th style="text-align:center">样本数</th>
<th>注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>&lt;女性，汽车&gt;</code></td>
<td style="text-align:center">500</td>
<td>同时出现<code>&lt;女性，汽车&gt;</code>的样本数</td>
</tr>
<tr>
<td style="text-align:center"><code>&lt;女性，化妆品&gt;</code></td>
<td style="text-align:center">1000</td>
<td>同时出现<code>&lt;女性，化妆品&gt;</code>的样本数</td>
</tr>
<tr>
<td style="text-align:center"><code>&lt;男性，汽车&gt;</code></td>
<td style="text-align:center">1500</td>
<td>同时出现<code>&lt;男性，汽车&gt;</code>的样本数</td>
</tr>
<tr>
<td style="text-align:center"><code>&lt;男性，化妆品&gt;</code></td>
<td style="text-align:center">0</td>
<td>样本中无此特征组合项</td>
</tr>
</tbody>
</table>
</blockquote>
<p><code>&lt;女性，汽车&gt;</code>的含义是<font color="#EE0000">女性看汽车广告</font>。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。 </p>
<p>因此，可以说<strong>FM降低了因数据稀疏，导致交叉项参数学习不充分的影响</strong>。</p>
<p><strong>2. <font color="#EE0000">FM提升了模型预估能力</font></strong>    </p>
<p>依然看上面的示例，样本中没有<code>&lt;男性，化妆品&gt;</code>交叉特征，即没有<font color="#EE0000">男性看化妆品广告</font>的数据。如果用多项式模型来建模，对应的交叉项参数<font color="#EE0000">\(w_{\text{男性,化妆品}}\)</font>是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的<font color="#EE0000">男性看化妆品广告</font>场景给出准确地预估。</p>
<p>FM模型是否能得到交叉项参数\(w_{\text{男性,化妆品}}\)呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为\(w_{\text{男性,化妆品}} = \langle \mathbf{v}_{男性}, \mathbf{v}_{化妆品}\rangle\)。</p>
<blockquote>
<p>用<code>男性</code>特征隐向量\(\mathbf{v}_{男性}\)和<code>化妆品</code>特征隐向量\(\mathbf{v}_{化妆品}\)的内积表示交叉项参数\(w_{\text{男性,化妆品}}\)。</p>
</blockquote>
<p>由于FM学习的参数就是单特征的隐向量，那么<font color="#EE0000">男性看化妆品广告</font>的预估结果可以用\(\langle \mathbf{v}_{男性}, \mathbf{v}_{化妆品}\rangle\)得到。这样，即便训练集中没有出现<font color="#EE0000">男性看化妆品广告</font>的样本，FM模型仍然可以用来预估，提升了预估能力。</p>
<p><strong>3. <font color="#EE0000">FM提升了参数学习效率</font></strong></p>
<p>这个显而易见，参数个数由\((n^2 + n + 1)\)变为\((nk + n + 1)\)个，模型训练复杂度也由\(O(m n^2)\)变为\(O(m n k)\)。\(m\)为训练样本数。对于训练样本和特征数而言，都是线性复杂度。</p>
<p>此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人<strong>把FM模型称为多项式的广义线性模型</strong>，也是恰如其分的。</p>
<p><strong>从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式</strong>，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑<font color="#EE0000"><code>User-Ad-Context</code></font>三个维度特征之间的关系，在FM模型中对应的degree为3。</p>
<p>最后一句话总结，FM最大特点和优势：</p>
<table>
<thead>
<tr>
<th><strong><font color="#EE0000">FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力。</font></strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><br></p>
<h3 id="场感知分解机">场感知分解机</h3><hr>
<p>场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自于Yu-Chin Juan与其比赛队员，它们借鉴了辣子Michael Jahrer的论文中field概念，提出了FM的升级版模型。</p>
<p>通过引入field的概念，FFM吧相同性质的特征归于同一个field。在FM开头one-hot编码中提到用于访问的channel，编码生成了10个数值型特征，这10个特征都是用于说明用户PV时对应的channel类别，因此可以将其放在同一个field中。那么，我们可以把同一个categorical特征经过one-hot编码生成的数值型特征都可以放在同一个field中。</p>
<blockquote>
<p>同一个categorical特征可以包括用户属性信息（年龄、性别、职业、收入、地域等），用户行为信息（兴趣、偏好、时间等），上下文信息（位置、内容等）以及其它信息（天气、交通等）。</p>
</blockquote>
<p>在FFM中，每一维特征\(x_i\)，针对其它特征的每一种”field” \(f_j\)，都会学习一个隐向量\(\mathbf{v}_{i,f_j}\)。因此，隐向量不仅与特征相关，也与field相关。</p>
<p>假设每条样本的\(n\)个特征属于\(f\)个field，那么FFM的二次项有\(nf\)个隐向量。而在FM模型中，每一维特征的隐向量只有一个。因此可以吧FM看作是FFM的特例，即把所有的特征都归属到一个field是的FFM模型。根据FFM的field敏感特性，可以导出其模型表达式：</p>
<p>$$<br>\hat{y}(\mathbf{x}) := w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_{i,\,f_j}, \mathbf{v}_{j,\,f_i} \rangle x_i x_j \qquad(ml.1.9.5)<br>$$</p>
<p>其中，\(f_j\)是第\(j\)个特征所属的field。如果隐向量的长度为\(k\)，那么FFM的二交叉项参数就有\(nfk\)个，远多于FM模型的\(nk\)个。此外，由于隐向量与field相关，FFM的交叉项并不能够像FM那样做化简，其预测复杂度为\(O(kn^2)\)。</p>
<p>这里以<a href="http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf" target="_blank" rel="external">NTU_FFM.pdf</a>和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">美团－深入FFM原理与实践</a>都提到的例子，给出FFM－Fields特征组合的工作过程。</p>
<blockquote>
<p>给出一下输入数据：</p>
<table>
<thead>
<tr>
<th style="text-align:center">User</th>
<th style="text-align:center">Movie</th>
<th style="text-align:center">Genre</th>
<th style="text-align:center">Price</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">YuChin</td>
<td style="text-align:center">3Idiots</td>
<td style="text-align:center">Comedy, Drama</td>
<td style="text-align:center">$9.99</td>
</tr>
</tbody>
</table>
<p>Price是数值型特征，实际应用中通常会把价格划分为若干个区间（即连续特征离散化），然后再one-hot编码，这里假设$9.99对应的离散化区间tag为”2”。当然不是所有的连续型特征都要做离散化，比如某广告位、某类广告／商品、抑或某类人群统计的历史CTR（pseudo－CTR）通常无需做离散化。</p>
<p>该条记录可以编码为5个数值特征，即<code>User^YuChin</code>, <code>Movie^3Idiots</code>, <code>Genre^Comedy</code>, <code>Genre^Drama</code>, <code>Price^2</code>。其中<code>Genre^Comedy</code>, <code>Genre^Drama</code>属于同一个field。为了说明FFM的样本格式，我们把所有的特征和对应的field映射成整数编号。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Field Name</th>
<th style="text-align:center">Field Index</th>
<th style="text-align:center">Feature Name</th>
<th style="text-align:center">Feature Index</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">User</td>
<td style="text-align:center"><font color="#EE0000">1</font></td>
<td style="text-align:center"><code>User^YuChin</code></td>
<td style="text-align:center"><font color="#0000FF">1</font></td>
</tr>
<tr>
<td style="text-align:center">Movie</td>
<td style="text-align:center"><font color="#EE0000">2</font></td>
<td style="text-align:center"><code>Movie^3Idiots</code></td>
<td style="text-align:center"><font color="#0000FF">2</font></td>
</tr>
<tr>
<td style="text-align:center">Genre</td>
<td style="text-align:center"><font color="#EE0000">3</font></td>
<td style="text-align:center"><code>Genre^Comedy</code></td>
<td style="text-align:center"><font color="#0000FF">3</font></td>
</tr>
<tr>
<td style="text-align:center">－</td>
<td style="text-align:center">－</td>
<td style="text-align:center"><code>Genre^Drama</code></td>
<td style="text-align:center"><font color="#0000FF">4</font></td>
</tr>
<tr>
<td style="text-align:center">Price</td>
<td style="text-align:center"><font color="#EE0000">4</font></td>
<td style="text-align:center"><code>Price^2</code></td>
<td style="text-align:center"><font color="#0000FF">5</font></td>
</tr>
</tbody>
</table>
<p>那么，FFM所有的（二阶）组合特征共有10项（\(\mathbf{C}_{5}^{2} = \frac{5 \times 4}{ 2!}= 10\)），即为：</p>
<p></p><p style="text-align:left"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_2_ffm_samples.png" width="700" height="200" alt="ffm_samples"></p>
<p>其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有\(\mathbf{C}_{n}^{2} = \frac{n(n-1)}{2}\)个。</p>
</blockquote>
<h4 id="FFM应用场景">FFM应用场景</h4><p>在我们的广告业务系统、商业推荐以及自媒体－推荐系统中，<strong>FFM模型作为点击预估系统中的核心算法之一，用于预估广告、商品、文章的点击率（CTR）和转化率（CVR）</strong>。</p>
<p>在鄙司广告算法团队，点击预估系统已成为基础设施，支持并服务于不同的业务线和应用场景。预估模型都是离线训练，然后定时更新到线上实时计算，因此预估问题最大的差异就体现在数据场景和特征工程。以广告的点击率为例，特征主要分为如下几类：</p>
<ul>
<li>用户属性与行为特征：</li>
<li>广告特征：</li>
<li>上下文环境特征：</li>
</ul>
<p>为了使用开源的FFM模型，所以的特征必须转化为<code>field_id:feat_id:value</code>格式，其中<code>field_id</code>表示特征所属field的编号，<code>feat_id</code>表示特征编号，value为特征取值。数值型的特征如果无需离散化，只需分配单独的field编号即可，如历史pseudo-ctr。categorical特征需要经过one-hot编码转化为数值型，编码产生的所有特征同属于一个field，特征value只能是0/1, 如用户年龄区间、性别、兴趣、人群等。</p>
<p><strong>开源工具FFM使用时，注意事项（参考新浪广告算法组的实战经验和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">美团－深入FFM原理与实践</a>）:</strong></p>
<ul>
<li>样本归一化：</li>
<li>特征归一化：</li>
<li>省略0值特征：</li>
</ul>
<p>回归、分类、排序等。推荐算法，预估模型（如CTR预估等）</p>
<h4 id="参考资料">参考资料</h4><ul>
<li>Sina广告点击预估系统实践</li>
<li>FM、FFM相关Paper、技术博客</li>
<li><a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html</a></li>
</ul>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/FFM/">FFM</a><a href="/tags/FM/">FM</a><a href="/tags/Factorization-Machine/">Factorization Machine</a><a href="/tags/Field-aware-FM/">Field-aware FM</a><a href="/tags/分解机器/">分解机器</a><a href="/tags/因子分解机/">因子分解机</a><a href="/tags/场感知分解机器/">场感知分解机器</a><a href="/tags/特征交叉/">特征交叉</a><a href="/tags/组合特征/">组合特征</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter6-boosting-family/" title="第06章：深入浅出ML之Boosting家族" itemprop="url">第06章：深入浅出ML之Boosting家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-12-12T14:42:16.000Z" itemprop="datePublished"> 发表于 2015-12-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-11-12</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>Boosting<ul>
<li>Boosting介绍</li>
<li>前向分步加法模型</li>
<li>Boosting四大家族</li>
</ul>
</li>
<li>AdaBoost<ul>
<li>算法学习过程 </li>
<li>算法实例</li>
<li>训练误差分析</li>
<li>前向分步加法模型与AdaBoost</li>
</ul>
</li>
<li>Boosted Decision Tree</li>
<li>Gradient Boosting</li>
</ul>
<p><br></p>
<h3 id="写在前面"><strong>写在前面</strong></h3><hr>
<p>提升（boosting）方法是一类应用广泛且非常有效的统计学习方法。</p>
<p>在2006年，Caruana和Niculescu-Mizil等人完成了一项实验，比较当今世界上现成的分类器（off-the-shelf classifiers）中哪个最好？实现结果表明Boosted Decision Tree（提升决策树）不管是在misclassification error还是produce well-calibrated probabilities方面都是最好的分离器，以ROC曲线作为衡量指标。（效果第二好的方法是随机森林）</p>
<blockquote>
<p>参见paper：《An Empirical Comparison of Supervised Learning Algorithms》ICML2006.</p>
</blockquote>
<p> 下图给出的是Adaboost算法（Decision Stump as Weak Learner）在处理二类分类问题时，随着弱分类器的个数增加，训练误差与测试误差的曲线图。</p>
<center><br><table><tr><br><td><p style="text-align:left"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_0_0_adboost_binary_classification_1.png" width="420" height="340" alt="损失函数示意图"><br></p></td><br><td><p style="text-align:right"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_0_0_adboost_binary_classification_2.png" width="400" height="320" alt="损失函数示意图"><br></p></td><br></tr></table><br></center>

<p>从图中可以看出，Adaboost算法随着模型复杂度的增加，测试误差（红色点线）基本保持稳定，并没有出现过拟合的现象。</p>
<p>其实不仅是Adaboost算法有这种表现，Boosting方法的学习思想和模型结构上可以保证其不容易产生过拟合（除非Weak Learner本身出现过拟合）。</p>
<p>下面我们主要是从损失函数的差异，来介绍Boosting的家族成员；然后我们针对每个具体的家族成员，详细介绍其学习过程和核心公式；最后从算法应用场景和工具方法给出简单的介绍。</p>
<p><br></p>
<h3 id="Boosting"><strong>Boosting</strong></h3><hr>
<p><br></p>
<h4 id="Boosting介绍"><strong>Boosting介绍</strong></h4><hr>
<ul>
<li><p>基本思想</p>
<p>  Boosting方法基于这样一种思想：</p>
<blockquote>
<p>对于一个复杂任务来说，将多个专家的判定进行<strong>适当的综合</strong>得出的判断，要比其中任何一个专家单独的判断好。很容易理解，就是”三个臭皮匠顶个诸葛亮”的意思…😄😄😄。</p>
</blockquote>
</li>
<li><p>历史由来</p>
<p>  历史上，Kearns和Valiant首先提出了”强可学习（strongly learnable）”和“弱可学习（weakly learnable）”的概念。他们指出：</p>
<blockquote>
<p><strong>在概率近似正确（probably approximately correct，PAC）学习框架中：</strong><br><br>①. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，<strong>并且正确率很高</strong>，那么就称这个概念是强可学习的；<br><br>②. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，<strong>学习的正确率仅比随机猜测略好</strong>，那么就称这个概念是弱可学习的。</p>
<p>Schapire后来证明了: <strong>强可学习和弱可学习是等价的。</strong> 也就是说，<strong>在PAC学习的框架下，一个概念是强可学习的 充分必要条件 是这个概念是弱可学习的。</strong> 表示如下：</p>
<p>  $$<br>  强可学习 \Leftrightarrow 弱可学习<br>  $$</p>
</blockquote>
<p>  如此一来，问题便成为：在学习中，如果已经发现了”弱学习算法”，那么能否将它提升为”强学习算法”？ 通常的，发现弱学习算法通常要比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，最具代表性的当数AdaBoost算法（是1995年由Freund和Schapire提出的）。</p>
</li>
<li><p>Boosting学习思路</p>
<p>  对于一个学习问题来说（以分类问题为例），给定训练数据集，求一个弱学习算法要比求一个强学习算法要容易的多。Boosting方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合弱分类器，得到一个强分类器。Boosting方法在学习过程中通过改变训练数据的权值分布，针对不同的数据分布调用弱学习算法得到一系列弱分类器。</p>
<p>  这里面有两个问题需要回答：</p>
<ol>
<li>在每一轮学习之前，如何改变训练数据的权值分布？</li>
<li><p>如何将一组弱分类器组合成一个强分类器？</p>
<blockquote>
<p>具体不同的boosting实现，主要区别在弱学习算法本身和上面两个问题的回答上。</p>
</blockquote>
<p>针对第一个问题，Adaboost算法的做法是：    </p>
<p><strong>提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。</strong></p>
<blockquote>
<p>如此，那些没有得到正确分类的样本，由于其权值加大而受到后一轮的弱分类器的更大关注。</p>
</blockquote>
<p>第二个问题，弱分类器的组合，AdaBoost采取<strong>加权多数表决</strong>的方法。具体地：</p>
<p><strong>加大 分类误差率小 的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</strong></p>
<p>AdaBoost算法的巧妙之处就在于它将这些学习思路自然并且有效地在一个算法里面实现。</p>
</li>
</ol>
</li>
</ul>
<p><br></p>
<h4 id="前向分步加法模型"><strong>前向分步加法模型</strong></h4><hr>
<p>英文名称：Forward Stagewise Additive Modeling</p>
<ul>
<li><p>加法模型（addtive model）</p>
<p>  $$<br>  f(x) = \sum_{k=1}^{K} \beta_k \cdot b(x; \gamma_k) \qquad(ml.1.6.1)<br>  $$ </p>
<p>  其中，\(b(x; \gamma_k)\) 为基函数，\(\gamma_k\)为基函数的参数，\(\beta_k\)为基函数的系数。</p>
</li>
<li><p>前向分步算法</p>
<p>  在给定训练数据及损失函数\(L(y,f(x))\)的条件下，学习加法模型\(f(x)\)成为经验风险极小化即损失函数极小化的问题：</p>
<p>  $$<br>  \min_{\beta_k, \gamma_k} \quad \sum_{i=1}^{M} L \left[y^{(i)}, \sum_{k=1}^{K} \beta_k b(x^{(i)}; \gamma_k)\right] \qquad(ml.1.6.2)<br>  $$</p>
<p>  通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：<strong>因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式\((ml.1.6.1)\)，那么就可以简化优化的复杂度</strong>。具体地，每步只需优化如下损失函数：</p>
<blockquote>
<p>$$<br>  \min_{\beta, \gamma} \quad \sum_{i=1}^{M} L(y^{(i)}, \beta b(x^{(i)}; \gamma)) \qquad(n.ml.1.6.1)<br>  $$</p>
</blockquote>
<p>  给定训练数据集\(D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y} = \)\(\{-1, +1\}\)。损失函数\(L(y, f(x))\)和基函数的集合\(\{b(x; \gamma)\}\)，学习加法模型\(f(x)\)的前向分步算法如下：</p>
<blockquote>
<p>\(<br>  \{ \\<br>  \quad 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}; 损失函数L(y, f(x));\\<br>  \qquad\quad  基函数集\{b(x, \gamma)\}； \\<br>  \quad 输出：加法模型f(x)。 \\<br>  \quad 计算过程：\\<br>  \qquad (1). 初始化f_0(x) = 0 \\<br>  \qquad (2). 对于k=1,2,\cdots,K \\<br>  \qquad\qquad (a). 极小化损失函数 \\<br>  \qquad\qquad\qquad (\beta_k, \gamma_k) = \arg \min_{\beta, \gamma} \sum_{i=1}^{M} L(y^{(i)}, f_{k-1}(x^{(i)}) + \beta b(x; \gamma)) \quad(n.ml.1.6.2)\\<br>  \qquad\qquad 得到参数\beta_k, \gamma_k. \\<br>  \qquad\qquad (b). 更新 \\<br>  \qquad\qquad\qquad f_k(x) = f_{k-1}(x) + \beta_k b(x; \gamma_k) \qquad(n.ml.1.6.3) \\<br>  \qquad (3). 得到加法模型 \\<br>  \qquad\qquad\qquad f(x) = f_K(x) = \sum_{k=1}^{K} \beta_k b(x; \gamma_k) \qquad(n.ml.1.6.4) \\<br>  \}<br>  \)</p>
</blockquote>
<p>  这样前向分步算法将<strong>同时求解</strong>从\(k=1\)到\(K\)的所有参数\(\beta_k, \gamma_k\)的优化问题简化为<strong>逐次求解</strong>各个\(\beta_k, \gamma_k\)的优化问题。</p>
</li>
</ul>
<p><br></p>
<h4 id="Boosting四大家族"><strong>Boosting四大家族</strong></h4><hr>
<p>Boosting并非是一个方法，而是一类方法。这里按照损失函数的不同，将其细分为若干类算法，下表给出了4种不同损失函数对应的Boosting方法：</p>
<table>
<thead>
<tr>
<th style="text-align:center">名称(Name)</th>
<th style="text-align:center">损失函数(Loss)</th>
<th style="text-align:center">导数(Derivative)</th>
<th style="text-align:center">目标函数\(f^*\)</th>
<th style="text-align:center">算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">平方损失<br>(Squared Error)</td>
<td style="text-align:center">\(\frac{1}{2} (y^{(i)} - f(x^{(i)}))^2\)</td>
<td style="text-align:center">\(y^{(i)} - f(x^{(i)})\)</td>
<td style="text-align:center">\(E[y \vert x^{(i)}]\)</td>
<td style="text-align:center">L2Boosting</td>
</tr>
<tr>
<td style="text-align:center">绝对损失<br>(Absolute Error)</td>
<td style="text-align:center">\(\vert y^{(i)} - f(x^{(i)}) \vert\)</td>
<td style="text-align:center">\(sign(y^{(i)} - f(x^{(i)})\)</td>
<td style="text-align:center">\(median(y \vert x^{(i)})\)</td>
<td style="text-align:center">Gradient Boosting</td>
</tr>
<tr>
<td style="text-align:center">指数损失<br>(Exponentail Loss)</td>
<td style="text-align:center">\(\exp(- \tilde {y^{(i)}} f(x^{(i)}))\)</td>
<td style="text-align:center">\(- \tilde {y^{(i)}} exp(-\tilde {y^{(i)}} f(x^{(i)}))\)</td>
<td style="text-align:center">\(\frac{1}{2} \log \frac{\pi_i}{1 - \pi_i}\)</td>
<td style="text-align:center">AdaBoost</td>
</tr>
<tr>
<td style="text-align:center">对数损失<br>(LogLoss)</td>
<td style="text-align:center">\(\log (1+e^{- \tilde{y^{(i)}} f_i})\)</td>
<td style="text-align:center">\(y^{(i)} - \pi_i\)</td>
<td style="text-align:center">\(\frac{1}{2} \log \frac{\pi_i}{1 - \pi_i}\)</td>
<td style="text-align:center">LogitBoost</td>
</tr>
</tbody>
</table>
<blockquote>
<p>说明：</p>
<p>该表来自于<a href="">Machine Learning: A Probabilistic Perspective</a>P587页</p>
<p><strong>L2Boosting全称：Least Squares Boosting；该算法由Buhlmann和Yu在2003年提出。</strong></p>
</blockquote>
<p>二分类问题时损失函数示意图：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_1_1_loss_function_graph.png" width="550" height="440" alt="损失函数示意图"></p>
<p>下面主要以AdaBoost算法作为示例，给出以下3个问题的解释：</p>
<ul>
<li>AdaBoost为什么能够提升学习精度？</li>
<li>如何解释AdaBoost算法？</li>
<li>Boosting方法更具体的实例－Boosting Tree。</li>
</ul>
<p>下面首先介绍Adaboost算法。</p>
<p><br>    </p>
<h3 id="Adaboost"><strong>Adaboost</strong></h3><hr>
<p><br></p>
<h4 id="算法学习过程"><strong>算法学习过程</strong></h4><hr>
<p>Adaboost算法在分类问题中的主要特点：<strong>通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。</strong> AdaBoost－算法描述（伪代码）如下：</p>
<p>\(<br>    \begin{align}<br>    &amp;\{ \\<br>    &amp;\quad 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)},y^{(M)})\}，x^{(i)} \in \mathcal{X} \subseteq R^N，\\<br>    &amp;\qquad\qquad y^{(i)} \in \mathcal{Y} = \{-1, +1\}, 弱分类器; \\<br>    &amp;\quad 输出：最终分类器G(x). \\<br>    &amp;\quad 过程：\\<br>    &amp;\qquad (1). 初始化训练数据的权值分布 \\<br>    &amp;\qquad\qquad\qquad D_1=(w_{11}, w_{12}, \cdots, w_{1M}), \quad w_{1i}=\frac{1}{M}, \; i=1,2,\cdots,M \\<br>    &amp;\qquad (2). 训练K个弱分类器 k=1,2,\cdots,K \\<br>    &amp;\qquad\qquad (a). 使用具有权值分布D_k的训练数据集学习，得到基本分类器 \\<br>    &amp;\qquad\qquad\qquad\qquad G_k(x): \mathcal{X} \rightarrow \{-1, +1\} \qquad\qquad(ml.1.6.3)\\<br>    &amp;\qquad\qquad (b). 计算G_k(x)在训练数据集上的分类误差率 \\<br>    &amp;\qquad\qquad\qquad\qquad e_k = P(G_k(x^{(i)}) \not= y^{(i)}) = \sum_{i=1}^{M} w_{ki} I(G_k(x^{(i)}) \not= y^{(i)}) \qquad(ml.1.6.4)\\<br>    &amp;\qquad\qquad (c). 计算G_k(x)的系数 \\<br>    &amp;\qquad\qquad\qquad\qquad \alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k} \quad(e是自然对数) \qquad(ml.1.6.5)\\<br>    &amp;\qquad\qquad (d). 更新训练数据集的权值分布 \\<br>    &amp;\qquad\qquad\qquad\qquad D_{k+1} = (w_{k+1,1}, w_{k+1,2}, \cdots, w_{k+1,M})\\<br>    &amp;\qquad\qquad\qquad\qquad w_{k+1,i} = \frac{w_{k,i}}{Z_k} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})), \quad i=1,2,\cdots,M \quad(ml.1.6.6)\\<br>    &amp;\qquad\qquad\qquad Z_k是规范化因子 \\<br>    &amp;\qquad\qquad\qquad\qquad Z_k = \sum_{i=1}^{M} w_{k,i} \cdot \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \qquad(ml.1.6.7)\\<br>    &amp;\qquad\qquad\qquad 使D_{k+1}成为一个概率分布。\\<br>    &amp;\qquad (3). 构建基本分类器的线性组合 \\<br>    &amp;\qquad\qquad\qquad f(x) = \sum_{k=1}^{K} \alpha_k G_k(x) \qquad(ml.1.6.8)\\<br>    &amp;\qquad 得到最终的分类器 \\<br>    &amp;\qquad\qquad G(x) = sign (f(x)) = sign \left(\sum_{k=1}^{K} \alpha_k G_k(x) \right) \qquad(ml.1.6.9)\\<br>    &amp;\}<br>    \end{align}<br>    \)</p>
<ul>
<li><p><strong>AdaBoost算法描述说明</strong></p>
<ul>
<li><p>步骤（1）假设训练数据集具有均匀（相同）的权值分布，即每个训练样本在基本分类器的学习中作用相同。</p>
<blockquote>
<p>这一假设保证，第一步能在原始数据上学习基本分类器\(G_1(x)\)。</p>
</blockquote>
</li>
<li><p>步骤（2）AdaBoost反复学习基本分类器，在每一轮\(k=1,2,\cdots,K\)顺序地执行下列操作：</p>
<ul>
<li>（a）学习基本分类器：使用当前分布\(D_k\)加权的训练数据集，学习基本分类器\(G_k(x)\)；</li>
<li><p>（b）误差率：计算基本分类器\(G_k(x)\)在加权训练数据集上的分类误差率</p>
<blockquote>
<p>$$<br>  e_k = P(G_k(x^{(i)}) \not= y^{(i)}) = \sum_{G_k(x^{(i)}) \not= y^{(i)}} w_{ki} \qquad(n.ml.1.6.5)<br>  $$</p>
</blockquote>
<p>  这里，\(w_{ki}\)表示第\(k\)轮中第\(i\)个样本的权值，\(\sum_{i=1}^{M} w_{ki} = 1\)。</p>
<blockquote>
<p>这表明，\(G_k(x)\)在加权的训练数据集上的分类误差率 是 被\(G_k(x)\)误分类样本的权值之和。由此可以看出数据权值分布\(D_k\)与基本分类器\(G_k(x)\)的分类误差率的关系。</p>
</blockquote>
</li>
<li><p>（c）分类器权重：计算基本分类器\(G_k(x)\)的系数\(\alpha_k\)，\(\alpha_k\)表示\(G_k(x)\)在最终分类器中的重要性</p>
<blockquote>
<p>根据(ml.1.6.3)中公式可知，当\(e_k \le 0.5\)时，\(\alpha_k \ge 0\)，并且\(\alpha_k\)随着\(e_k\)的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。</p>
</blockquote>
</li>
<li><p>（d）更新训练数据的权值分布，为下一轮做准备，公式\((ml.1.6.6)\)可以写成：</p>
<blockquote>
<p>$$<br>  w_{k+1, i} =<br>  \begin{cases}<br>  \frac{w_{ki}} {Z_k} e^{-\alpha_k}, &amp;\quad G_k(x^{(i)}) = y^{(i)} \\<br>  \frac{w_{ki}} {Z_k} e^{\alpha_k}, &amp;\quad G_k(x^{(i)}) \not= y^{(i)}<br>  \end{cases} \qquad(n.ml.1.6.6)<br>  $$</p>
<p>由此可知，被基本分类器\(G_k(x)\)误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。相比较来说，误分类样本的权值被放大\(e^{2\alpha_k} = \frac{e_k}{1-e_k}\)倍。因此，误分类样本在下一轮学习中起更大的作用。</p>
</blockquote>
<p><strong>不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器中起不同的作用，这也是AdaBoost的一个特点。</strong></p>
</li>
</ul>
</li>
<li><p>步骤（3）线性组合\(f(x)\)实现\(K\)个基本分类器的加权表决。系数\(\alpha_k\)表示了极本分类器\(G_k(x)\)的重要性。</p>
<blockquote>
<p> 注意：在这里所有\(\alpha_k\)之和并不为1。\(f(x)\)的符号决定实例\(x\)的类别，\(f(x)\)的绝对值表示分类的精确度。</p>
</blockquote>
<p>  <strong>利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</strong></p>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="示例：AdaBoost算法"><strong>示例：AdaBoost算法</strong></h4><hr>
<p>此示例参考李航老师的《统计学习方法》.</p>
<blockquote>
<p>给定下表所示训练数据。</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
</tr>
<tr>
<td>y</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>-1</td>
</tr>
</tbody>
</table>
<p>假设弱分类器由\(x \le v 或 x&gt;v\)产生，其阈值\(v\)使该分类器在训练数据集上分类误差率最低。试用AdaBoost算法学习一个强分类器。</p>
<p>解： 首先初始化数据权值分布（均匀分布）：</p>
<p>$$<br>D_1 = (w_{1,1}, w_{1,2}, \cdots, w_{1,10}), \quad w_{1,i}=0.1, \;  i=1,2,\cdots,10<br>$$</p>
<p>对\(k=1\)，</p>
<p>(a). 在权值分布为\(D_1\)的训练数据上，阈值\(v\)取2.5时，分类误差率最低，故基本分类器为：</p>
<p>$$<br>G_1(x) =<br>\begin{cases}<br>1,&amp; \quad x &lt; 2.5 \\<br>-1, &amp; \quad x &gt; 2.5<br>\end{cases}<br>$$</p>
<p>(b). \(G_1(x)\)在训练数据集上的误差率\(e_1=P(G_1(x^{(i)}) \not= y^{(i)})=0.3\);</p>
<p>(c). 计算\(G_1(x)\)的系数：\(\alpha_1 = \frac{1}{2} \log \frac{1-e_1}{e_1} = 0.4236\)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$<br>    \begin{align}<br>    &amp; D_2 = (w_{2,1}, w_{2,2}, \cdots, w_{2,10}) \\<br>    &amp; w_{2,i} = \frac{w_{1,i}} {Z_1} \exp(-\alpha_1 y^{(i)} G_1(x^{(i)})), \quad i=1,2,\cdots,10 \\<br>    &amp; D_2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, \\<br>    &amp;\qquad\;\; 0.1666, 0.1666, 0.1666, 0.0715) \\<br>    &amp; f_1(x) = 0.4236 G_1(x)<br>    \end{align}<br>$$</p>
<p>分类器\(sign[f_1(x)]\)在训练数据上有3个误分类点。</p>
<p>对\(k=2\)，</p>
<p>(a). 在权值分布为\(D_2\)的训练数据上，阈值\(v\)取8.5时，分类误差率最低，基本分类器为：</p>
<p>$$<br>    G_2(x) =<br>    \begin{cases}<br>    1,&amp; \quad x &lt; 8.5 \\<br>    -1, &amp; \quad x &gt; 8.5<br>    \end{cases}<br>    $$</p>
<p>(b). \(G_2(x)\)在训练数据集上的误差率\(e_2=0.2143\);</p>
<p>(c). 计算\(G_2(x)\)的系数：\(\alpha_2 = 0.6496\)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$<br>    \begin{align}<br>    &amp; D_3=(0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.1667, \\<br>    &amp;\qquad\;\; 0.1060, 0.1060, 0.1060, 0.0455) \\<br>    &amp; f_2(x) = 0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x)<br>    \end{align}<br>$$</p>
<p>分类器\(sign[f_2(x)]\)在训练数据上有3个误分类点。</p>
<p>对\(k=3\)，</p>
<p>(a). 在权值分布为\(D_3\)的训练数据上，阈值\(v\)取5.5时，分类误差率最低，基本分类器为：</p>
<p>$$<br>G_3(x) =<br>\begin{cases}<br>1,&amp; \quad x &lt; 5.5 \\<br>-1, &amp; \quad x &gt; 5.5<br>\end{cases}<br>$$</p>
<p>(b). \(G_3(x)\)在训练数据集上的误差率\(e_3=0.1820\);</p>
<p>(c). 计算\(G_3(x)\)的系数：\(\alpha_2 = 0.7514\)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$<br>    \begin{align}<br>    &amp; D_4=(0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065, 0.065, 0.065, 0.125)<br>    \end{align}<br>$$</p>
<p>于是得到模型线性组合</p>
<p>$$<br>    f_3(x) = 0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7514 \cdot G_3(x)<br>    $$</p>
<p>分类器\(sign[f_3(x)]\)在训练数据上误分类点个数为0。</p>
<p>于是最终分类器为：</p>
<p>$$<br>    \begin{align}<br>    G(x) &amp;= sign[f_3(x)] \\<br>    &amp;= sign[0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7514 \cdot G_3(x)]<br>    \end{align}<br>$$</p>
</blockquote>
<p><br></p>
<h4 id="训练误差分析"><strong>训练误差分析</strong></h4><hr>
<p><strong>AdaBoost算法最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。</strong> 对于这个问题，有个定理可以保证分类误差率在减少－AdaBoost的训练误差界。</p>
<ul>
<li><strong>定理：AdaBoost训练误差界</strong></li>
</ul>
<table>
<thead>
<tr>
<th>［定理］AdaBoost训练误差界</th>
</tr>
</thead>
<tbody>
<tr>
<td>$$ \frac{1}{M} \sum_{i=1}^{M} I(G(x^{(i)}) \not= y^{(i)}) \le \frac{1}{M} \sum_{i} \exp(-y^{(i)} f(x^{(i)})) = \prod_{k=1}^{K} Z_k \qquad(ml.1.6.10) $$</td>
</tr>
<tr>
<td>其中，\(G(x), f(x)\)和\(Z_k\)分别由公式\((ml.1.6.9), (ml.1.6.8), (ml.1.6.7)\)给出</td>
</tr>
</tbody>
</table>
<blockquote>
<p>证明如下：</p>
<p>当\(G(x^{(i)}) \not= y^{(i)}\)时，\(y^{(i)} f(x^{(i)}) &lt; 0\)，因而\(\exp(-y^{(i)} f(x^{(i)})) \ge 1\)。由此，可以直接推导出前半部分。</p>
<p>后半部分的推导要用到\(Z_k\)的定义式\((ml.1.6.7)\)和\((ml.1.6.6)\)的变形:</p>
<p>$$<br>w_{k,i} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) = Z_k w_{k+1,i} \qquad(n.ml.1.6.7)<br>$$</p>
<p>推导如下：</p>
<p>$$<br>    \begin{align}<br>    \frac{1}{M} \sum_{i=1}^{M} \exp(-y^{(i)} f(x^{(i)})) &amp;= \underline{ \frac{1}{M} \sum_{i=1}^{M} } \exp \left(-\sum_{k=1}^{K} \alpha_k y^{(i)} G_k(x^{(i)}) \right) \\<br>    &amp;= \underline{ \sum_{i=1}^{M} w_{1,i} } \prod_{k=1}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\<br>    &amp;= \sum_{i=1}^{M} w_{1,i} \underline{ \exp(-\alpha_1 y^{(i)}) G_k(x^{(i)}) \prod_{k=2}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) } \\<br>    &amp;= Z_1 \sum_{i=1}^{M} w_{2,i} \prod_{k=2}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\<br>    &amp;= Z_1 Z_2 \sum_{i=1}^{M} w_{3,i} \prod_{k=3}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\<br>    &amp;= \cdots \cdots \\<br>    &amp;= Z_1 Z_2 \cdots Z_{K-1} \sum_{i=1}^{M} w_{K,i} \exp(-\alpha_K y^{(i)} G_K(x^{(i)})) \\<br>    &amp;= \prod_{k=1}^{K} Z_k<br>    \end{align} \quad(n.ml.1.6.8)<br>$$</p>
<p>注意：\(w_{1,i} = \frac{1}{M}\)</p>
</blockquote>
<p>这一定理说明：<strong>可以在每一轮选取适当的\(G_k\)使得\(Z_k\)最小，从而使训练误差下降最快。</strong> 对于二类分类问题，有如下定理。</p>
<ul>
<li>定理：二类分类问题AdaBoost训练误差界</li>
</ul>
<table>
<thead>
<tr>
<th>［定理］二类分类问题AdaBoost训练误差界</th>
</tr>
</thead>
<tbody>
<tr>
<td>$$\prod_{k=1}^{K} Z_k = \prod_{k=1}^{K} \left[2 \sqrt{e_k(1-e_k)} \;\right] = \prod_{k=1}^{K} \sqrt{(1-4\gamma_k^2)} \le \exp \left(-2 \sum_{k=1}^{K} \gamma_k^2 \right) \qquad(ml.1.6.9)$$</td>
</tr>
<tr>
<td>这里，\(\gamma_k = 0.5 - e_k \)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>证明：由公式\((ml.1.6.7)\)和\((n.ml.1.6.5)\)可得：</p>
<p>$$<br>\begin{align}<br>Z_k &amp;= \sum_{i=1}^{M} w_{k,i} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\<br>&amp;= \sum_{y^{(i)} = G_k(x^{(i)})} w_{k,i} \cdot e^{-\alpha_k} + \sum_{y^{(i)} \not= G_k(x^{(i)})} w_{k,i} \cdot e^{\alpha_k} \\<br>&amp;= (1-e_k) \cdot e^{-\alpha_k} + e_k \cdot e^{\alpha_k} \\<br>&amp;= 2 \sqrt{e_k (1-e_k)} = \sqrt{1-4\gamma_m^2}<br>\end{align} \qquad(n.ml.1.6.9)<br>$$</p>
<p>注：\(\alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k}, e^{\alpha_k} = \sqrt{\frac{1-e_k}{e_k}}\)</p>
<p>对于不等式部分</p>
<p>$$<br>\prod_{k=1}^{K} \sqrt{1-4\gamma_m^2} \le \exp \left(-2 \sum_{k=1}^{K} \gamma_k^2 \right) \qquad(n.ml.1.6.10)<br>$$</p>
<p>则可根据\(e^x\)和\(\sqrt{1-x}\)在点\(x=0\)的泰勒展开式推出不等式\(\sqrt{1-4\gamma_m^2} \le \exp(-2 \gamma_m^2)\)。</p>
</blockquote>
<table>
<thead>
<tr>
<th>[推论] AdaBoost训练误差指数速率下降</th>
</tr>
</thead>
<tbody>
<tr>
<td>如果存在\(\gamma &gt; 0\)，对所有的\(m\)有\(\gamma_k \ge \gamma\)，则有</td>
</tr>
</tbody>
</table>
<p>$$\frac{1}{M} \sum_{i=1}^{M} I(G(x^{(i)}) \not= y^{(i)}) \le \exp(-2K\gamma^2) \qquad(ml.1.6.12)$$|</p>
<p>推论表明，在此条件下，<strong>AdaBoost的训练误差是以指数速率下降的</strong>。这一性质对于AdaBoost计算（迭代）效率是利好消息。</p>
<blockquote>
<p>注意：AdaBoost算法不需要知道下界\(\gamma\)，这正是Freund和Schapire设计AdaBoost时所考虑的。与一些早期的提升方法不同，AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率。这也是其算法名称的由来（适应的提升）。Ada是Adaptive的简写。</p>
</blockquote>
<p><br></p>
<h4 id="前向分步加法模型与Adaboost"><strong>前向分步加法模型与Adaboost</strong></h4><hr>
<p>AdaBoost算法还有另一个解释，即可以认为<strong>AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法</strong>时的学习方法。</p>
<p>根据前向分步算法可以推导出AdaBoost，用一句话叙述这一关系.</p>
<table>
<thead>
<tr>
<th>AdaBoost算法是前向分步加法算法的特例</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p> 此时，模型是由基本分类器组成的加法模型，损失函数是指数函数。</p>
<blockquote>
<p>证明：<strong>前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器：</strong></p>
<p>$$<br>f(x) = \sum_{k=1}^{K} \alpha_k G_k(x) \qquad (n.ml.1.6.11)<br>$$</p>
<p>由基本分类器\(G_k(x)\)及其系数\(\alpha_k\)组成，\(k=1,2,\cdots,K\)。前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。</p>
<p>下面证明：</p>
<p><strong>前向分步算法的损失函数是指数损失函数（Exponential）\(L(y, f(x)) = \exp [-y f(x)]\) 时，其学习的具体操作等价于AdaBoost算法学习的具体操作。</strong> </p>
<p>假设经过\(k-1\)轮迭代，前向分步算法已经得到\(f_{k-1}(x)\):</p>
<p>$$<br>\begin{align}<br>f_{k-1} (x) &amp;= f_{k-2}(x) + \alpha_{k-1} G_{k-1}(x) \\<br>&amp;= \alpha_1 G_1(x) + \cdots + \alpha_{m-1} G_{m-1}(x)<br>\end{align}  \qquad(n.ml.1.6.12)<br>$$</p>
<p>在第\(k\)轮迭代得到\(\alpha_k, G_k(x)\)和\(f_k(x)\)。</p>
<p>$$<br>f_{k} (x) = f_{k-1}(x) + \alpha_{k} G_{k}(x) \qquad(n.ml.1.6.13)<br>$$</p>
<p>目标是使前向分步算法得到的\(\alpha_k\)和\(G_k(x)\)使\(f_k(x)\)在训练数据集\(D\)上的指数损失最小，即</p>
<p>$$<br>(\alpha_k, G_k(x)) = \arg \min_{\alpha, G} \underbrace{ \sum_{i=1}^{M} \exp [-y^{(i)} (f_{k-1}(x) + \alpha G(x^{(i)}))] }_{指数损失表达式} \quad(n.ml.1.6.14)<br>$$</p>
<p>进一步可表示为：</p>
<p>$$<br>(\alpha_k, G_k(x)) = \arg \min_{\alpha, G} \sum_{i=1}^{M} \overline{w}_{k,i}  \cdot \exp [-y^{(i)} \alpha G(x^{(i)})] \quad(n.ml.1.6.15)<br>$$</p>
<p>其中，\(\overline{w}_{k,i} = \exp [-y^{(i)} f_{k-1} (x^{(i)})]\)表示第\(i\)样本在之前模型上的指数损失。因为\(\overline{w}_{k,i}\)既不依赖\(\alpha\)也不依赖\(G\)，所以与最小化无关。但\(\overline{w}_{k,i}\)依赖于\(f_{k-1}(x)\)，随着每一轮迭代而发生变化。</p>
<p>现在使公式\((n.ml.1.6.15)\)达到最小的\(\alpha_k^{\ast}\)和\(G_k^{\ast}\)就是AdaBoost算法所得到的\(\alpha_k\)和\(G_k(x)\)。求解公式\((n.ml.1.6.15)\)可分为两步：</p>
<p>第一步：求\(G_k^{\ast}\). 对于任意\(\alpha &gt; 0\)，使公式\((n.ml.1.6.15)\)最小的\(G(x)\)由下式得到：</p>
<p>$$<br>G_k^{\ast}(x) = \arg \min_{G} \sum_{i=1}^{M} \overline{w}_{k,i} \cdot I(y^{(i)} \not= G(x^{(i)})) \qquad(n.ml.1.6.16)<br>$$</p>
<p> 此分类器\(G_k^{\ast}(x)\)即为AdaBoost算法的基本分类器\(G_k(x)\)，因为它是使第\(k\)轮加权训练数据分类误差率最小的基本分类器。</p>
<p>之后，求\(\alpha_k^{\ast}\)。参考公式\((n.ml.1.6.5)\)，公式\((n.ml.1.6.15)\)中的：</p>
<p>$$<br>\begin{align}<br>\sum_{i=1}^{M} \overline{w}_{k,i}  \cdot \exp [-y^{(i)} \alpha G(x^{(i)})] &amp;= \sum_{y^{(i)} = G_k(x^{(i)})} \overline{w}_{k,i} \cdot e^{-\alpha} + \sum_{y^{(i)} \neq G_k(x^{(i)})} \overline{w}_{k,i} \cdot e^{\alpha} \\\<br>&amp;= (e^{\alpha} - e^{-\alpha}) \sum_{i=1}^{M} \overline{w}_{k,i} I(y^{(i)} \neq G(x^{(i)})) + e^{-\alpha} \sum_{i=1}^{M} \overline{w}_{k,i}<br>\end{align} \quad(n.ml.1.6.17)<br>$$</p>
<p>把已得到的\(G_k^{\ast}\)带入公式\((n.ml.1.6.17)\)，并对\(\alpha\)求导（导数为0），即得到使公式\((n.ml.1.6.15)\)最小的\(\alpha\)。</p>
<p>$$<br>\alpha_{k}^{\ast} = \frac{1}{2} \log \frac{1-e_k}{e_k}<br>$$</p>
<p>\(e_k\)为分类误差率</p>
<p>$$<br>e_k = \frac{ \sum_{i=1}^{M} \overline{w}_{k,i} I(y^{(i)} \neq G_k(x^{(i)})) } {\sum_{i=1}^{M} \overline{w}_{k,i} } = \sum_{i=1}^{M} w_{k,i} I(y^{(i)} \neq G_k(x^{(i)}))<br>$$</p>
<p>可以看出，这里求得的\(\alpha_k^{\ast}\)与AdaBoost算法\((2)-(c)\)步的\(\alpha_k\)完全一致。</p>
<p>最后看一下每一轮样本权值的更新。由：\(f_k(x) = f_{k-1}(x) + \alpha_k G_k(x)\)以及\(\overline{w}_{k,i} = \exp[-y^{(i)} f_{k-1}(x^{(i)})]\)，可得：</p>
<p>$$<br>\overline{w}_{k+1,i} = \overline{w}_{k,i} \exp[-y^{(i)} \alpha_k G_k(x)]<br>$$</p>
<p>这与Adaboost算法的第\((2)-(d)\)步的样本权值的更新，可看出二者是等价的（只相差规范化因子）。</p>
</blockquote>
<ul>
<li><p>AdaBoost算法缺点</p>
<ul>
<li><p>对异常点敏感</p>
<p>  指数损失存在的一个问题是不断增加误分类样本的权重（指数上升）。如果数据样本是异常点（outlier），会极大的干扰后面基本分类器学习效果；</p>
</li>
<li><p>模型无法用于概率估计</p>
<blockquote>
<p>MLAPP中的原话：”\(e^{-\tilde{y}f}\) is not the logarithm of any pmf for binary variables \(\tilde{y} \in \{-1, +1\}\); consequently we cannot recover probability estimate from \(f(x)\).” </p>
<p>意思就是说对于取值为\(\tilde{y} \in \{-1, +1\}\)的随机变量来说，\(e^{-\tilde{y}f}\)不是任何概率密度函数的对数形式，模型\(f(x)\)的结果无法用概率解释。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h3 id="Boosted_Decision_Tree"><strong>Boosted Decision Tree</strong></h3><hr>
<p>提升决策树是指以<strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#分类与回归树" target="_blank" rel="external">分类与回归树（CART）</a></strong>为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一。</p>
<blockquote>
<p>提升决策树简称提升树，Boosting Tree.</p>
</blockquote>
<p><br></p>
<h4 id="提升树模型"><strong>提升树模型</strong></h4><hr>
<p>提升树模型实际采用加法模型（即基函数的线性组合）与前向分步算法，以决策树为基函数的提升方法称为提升树（Boosting Tree）。</p>
<blockquote>
<p>对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。在6.1.3节AdaBoost例子中，基本分类器是\(x<v\\)或\\(x>v\)，可以看作是由一个跟结点直接连接两个叶结点的简单决策树，即所谓的<a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#写在前面" target="_blank" rel="external">决策树桩（Decision Stump）</a>。</v\\)或\\(x></p>
</blockquote>
<p>提升树模型可以表示为CART决策树的加法模型：</p>
<p>$$<br>f_K(x) = \sum_{k=1}^{K} T(x; \Theta_k) \qquad(ml.1.6.13)<br>$$</p>
<p>其中，\(T(x; \Theta_k)\)表示二叉决策树，\(\Theta_k\)为决策树的参数，\(K\)为树的个数。</p>
<blockquote>
<p>基本学习器－CART决策树，请参考<a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/" target="_blank" rel="external">第03章：深入浅出ML之Tree-Based家族</a></p>
</blockquote>
<p><br></p>
<h4 id="提升树算法"><strong>提升树算法</strong></h4><hr>
<p>提升树算法采用前向分步算法。首先确定初始提升树\(f_0(x) = 0\)，第\(k\)步的模型为：</p>
<p>$$<br>f_k(x) = f_{k-1}(x) + T(x; \Theta_k) \qquad(ml.1.6.14)<br>$$</p>
<p>其中，\(f_{k-1}(x)\)为当前模型，通过<strong>经验风险极小化</strong>确定下一颗决策树的参数\(\Theta_k\)，</p>
<p>$$<br>\hat{\Theta}_k = \arg \min_{\Theta_k} \sum_{i=1}^{M} L(y^{(i)}, \; f_{k-1}(x) + T(x^{(i)}; \Theta_k))  \qquad(ml.1.6.15)<br>$$</p>
<blockquote>
<p>由于树的线性组合可以很好的拟合训练数据，即使数据中的输入和输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。 </p>
</blockquote>
<p><strong>提升树家族</strong></p>
<p>不同问题的提升树学习算法，其主要区别在于<strong>损失函数</strong>不同。平方损失函数常用于回归问题，用指数损失函数用于分类问题，以及绝对损失函数用于决策问题。</p>
<ul>
<li><p><strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#二叉分类树" target="_blank" rel="external">二叉分类树</a></strong></p>
<p>  对于二类分类问题，提升树算法只需要将AdaBoost算法例子中的基本分类器限制为<strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#二叉分类树" target="_blank" rel="external">二叉分类树</a></strong>即可，可以说此时的决策树算法时AdaBoost算法的特殊情况。</p>
<blockquote>
<p>损失函数仍为指数损失，提升树模型仍为前向加法模型。</p>
</blockquote>
</li>
<li><p><strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#二叉回归树" target="_blank" rel="external">二叉回归树</a></strong></p>
<p>  已知训练数据集\(D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y} \) \( \subseteq R, \mathcal{Y}\)为输出空间。如果将输入空间\(\mathcal{X}\)划分为\(J\)个互不相交的区域\(R_1, R_2, \cdots, R_J\)，并且在每个区域上确定输出的常量\(c_j\)，那么树可以表示为：</p>
<p>  $$<br>  T(x; \Theta) = \sum_{j=1}^{J} c_j I(x \in R_j)  \qquad(ml.1.6.16)<br>  $$</p>
<p>  其中，参数\(\Theta=\{(R_1, c_1), (R_2, c_2), \cdots, (R_J, c_J)\}\)表示树的区域划分和各区域上的常数。\(J\)是回归树的复杂度即叶结点的个数。</p>
</li>
<li><p>回归问题提升树－前向分步算法</p>
<blockquote>
<p>回归问题提升树使用以下前向分步算法：</p>
<p>  $$<br>  \begin{align}<br>  f_0(x) &amp;= 0 \\<br>  f_k(x) &amp;= f_{k-1}(x) + T(x; \Theta_k) \quad k=1,2,\cdots,K \\<br>  f_K(x) &amp;= \sum_{k=1}^{K} T(x; \Theta_k)<br>  \end{align} \qquad(n.ml1.6.17)<br>  $$</p>
<p>在前向分布算法的第\(k\)步，给定当前模型\(f_{k-1}(x)\)，需求解：</p>
<p>  $$<br>  \hat{\Theta}_k = \arg \min_{\Theta_k} \sum_{i=1}^{M} \underbrace{ L(y^{(i)}, \; f_{k-1}(x) + T(x^{(i)}; \Theta_k)) }_{损失函数} \qquad(n.ml.1.6.18)<br>  $$</p>
<p>得到\(\hat{\Theta}_k\)，即第\(k\)颗树的参数。</p>
<p>当采用<strong>平方误差损失函数</strong>时，</p>
<p>  $$<br>  L(y, f(x)) = (y - f(x))^2 \qquad (n.ml.1.6.19)<br>  $$</p>
<p>将平方误差损失函数展开为：</p>
<p>  $$<br>  \begin{align}<br>  &amp;L(y, f_{k-1}(x) + T(x; \Theta_k)) \\<br>  &amp;\qquad = [y - f_{k-1}(x) - T(x; \Theta_k)]^2 \\<br>  &amp;\qquad = [r - T(x; \Theta_k)]^2<br>  \end{align} \qquad(n.ml.1.6.20)<br>  $$</p>
<p>这里\( r = y - f_{k-1}(x) \)，表示当前模型的拟合数据的残差（residual）。所以，对回归问题的提升树算法来说，只需要简单地拟合当前模型的残差。</p>
<blockquote>
<p>由于损失函数是平方损失，因此该方法属于<strong>L2Boosting</strong>的一种实现。</p>
</blockquote>
</blockquote>
</li>
<li><p><strong>回归问题提升树－算法描述</strong></p>
<p>  \(<br>  \{ \\<br>  \quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\<br>  \quad 输出：提升树f_K(x). \\<br>  \quad 过程: \\<br>  \qquad (1). 初始化模型f_0(x) = 0； \\<br>  \qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\<br>  \qquad\qquad (a). 计算残差：r_{ki} = y^{(i)} - f_{k-1}(x^{(i)}), \quad i=1,2,\cdots,M \\<br>  \qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到T(x;\Theta_k) \\<br>  \qquad\qquad (c). 更新f_k(x) = f_{k-1}(x) + T(x; \Theta_k) \\<br>  \qquad\; (3). 得到回归提升树 \\<br>  \qquad\qquad f_K(x) = \sum_{k=1}^{K} T(x; \Theta_k) \\<br>   \}<br>  \)</p>
</li>
</ul>
<p><br></p>
<h3 id="Gradient_Boosting"><strong>Gradient Boosting</strong></h3><hr>
<p>提升树方法是利用加法模型与前向分布算法实现整个优化学习过程。Adaboost的指数损失和回归提升树的平方损失，在前向分布中的每一步都比较简单。但对于一般损失函数而言（比如绝对损失），每一个优化并不容易。</p>
<p>针对这一问题。Freidman提出了梯度提升（gradient boosting）算法。该算法思想：</p>
<p><strong>利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，拟合一个回归树。</strong></p>
<p>损失函数的负梯度为：</p>
<p>$$<br>-\left[ \frac{\partial L(y^{(i)}, f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \approx r_{m,i}<br>$$</p>
<ul>
<li><p>Gradient Boosting－算法描述</p>
<p>  \(<br>  \{ \\<br>  \quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\<br>  \qquad\quad\; 损失函数L(y, f(x)); \\<br>  \quad 输出：提升树\hat{f}(x). \\<br>  \quad 过程: \\<br>  \qquad (1). 初始化模型 \\<br>  \qquad\qquad\qquad f_0(x) = \arg \min_c \sum_{i=1}^{M} L(y^{(i)}, c)； \\<br>  \qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\<br>  \qquad\qquad (a). 计算残差：对于i=1,2,\cdots,M \\<br>  \qquad\qquad\qquad\qquad r_{ki} = -\left[ \frac{\partial L(y^{(i)}, \; f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \\<br>  \qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到第k颗树的叶结点区域R_{kj}，\quad j=1,2,\cdots,J \\<br>  \qquad\qquad (c). 对j=1,2,\cdots,J, 计算：\\<br>  \qquad\qquad\qquad\qquad c_{kj} = \arg \min_c \sum_{x^{(i)} \in R_{kj}} L(y^{(i)}, \; f_{k-1}(x^{(i)}) + c)\\<br>  \qquad\qquad (d). 更新模型：\\<br>  \qquad\qquad\qquad\qquad    f_k(x) = f_{k-1}(x) + \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\<br>  \qquad\; (3). 得到回归提升树 \\<br>  \qquad\qquad\qquad \hat{f}(x) = f_K(x) = \sum_{k=1}^{K} \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\<br>   \}<br>  \)</p>
<blockquote>
<p>算法解释：</p>
<ol>
<li>第（1）步初始化，估计使损失函数极小化的常数值（是一个只有根结点的树）；</li>
<li>第(2)(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。(对于平方损失函数，他就是残差；对于一般损失函数，它就是残差的近似值)</li>
<li>第(2)(b)步估计回归树的结点区域，以拟合残差的近似值；</li>
<li>第(2)(c)步利用线性搜索估计叶结点区域的值，使损失函数极小化；</li>
<li>第(2)(d)步更新回归树。</li>
</ol>
</blockquote>
</li>
</ul>
<p><br></p>
<h3 id="Boosting利器"><strong>Boosting利器</strong></h3><hr>
<p>Boosting类方法在不仅在二分类、多分类上有着出色的表现，在预估问题上依然出类拔萃。</p>
<blockquote>
<p>2012年KDD cup竞赛和Kaggle上的许多数据挖掘竞赛，Boosting类方法帮助参赛者取得好成绩提供了强有力的支持。</p>
</blockquote>
<p>“工欲善其事，必先利其器”。Github上和机器学习工具包（如sklearn）中有很多优秀的开源boosting实现。在这里重点介绍两个Boosting开源工具。</p>
<ul>
<li><p><a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">XGBoost</a> </p>
<p>  说到Boosting开源工具，首推<a href="http://www.weibo.com/u/2397265244?is_all=1" target="_blank" rel="external">@陈天奇怪</a>同学的<a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">XGBoost</a> (eXtreme Gradient Boosting)。上面说的各种竞赛很多优秀的战果都是用@陈天奇同学的神器。</p>
<p>  从名称可以看出，该版本侧重于Gradient Boosting方面，提供了Graident Boosting算法的框架，给出了GBDT，GBRT，GBM具体实现。提供了多语言接口（C++, Python, Java, R等），供大家方便使用。</p>
<p>  更令人振奋的一件事情是，最新版本的xgboost是基于分布式通信协议rabit开发的，可部署在分布式资源调度系统上（如yarn，s3等）。我们完全可以利用最新版的xgboost在分布式环境下解决分类、预估等场景问题。</p>
<blockquote>
<p>注：</p>
<ol>
<li>XGBoost是<a href="https://github.com/dmlc" target="_blank" rel="external">DMLC</a>（即分布式机器学习社区）下面的一个子项目，由@陈天奇怪，@李沐等机器学习大神发起。</li>
<li><a href="https://github.com/dmlc/rabit" target="_blank" rel="external">Rabit</a>是一个为分布式机器学习提供Allreduce和Broadcast编程范式和容错功能的开源库（也是@陈天奇同学的又一神器）。它主要是解决MPI系统机器之间无容错功能的问题，并且主要针对Allreduce和Broadcast接口提供可容错功能。</li>
</ol>
<p>题外话：</p>
<p>2014年第一次用XGBoost时，用于Kaggle移动CTR预估竞赛。印象比较深刻的是，同样的训练数据（特征工程后），分别用XGBoost中的GBDT和MLLib中的LR模型（LBFGS优化），在验证集上的表现前者比后者好很多（logloss和auc都是如此）。线上提交结果时，名次直接杀进前100名，当时给我留下了非常好的印象。后来，因为项目原因，没有过多的使用xgboost，但一直关注着。</p>
<p>目前，我个人更加关注的是：<strong>基于Rabit开发/封装一些在工业界真正能发挥重要价值的（分布式）机器学习工具</strong>，用于解决超大规模任务的学习问题。这里面会涉及到分布式环境下的编程范式，可以高效地在分布式环境下工作的优化算法（<code>admm</code>等）和模型(<code>loss + regularization term</code>)等。</p>
<p>关于大数据下的机器学习发展，个人更看好将<strong>计算引擎模块</strong>与<strong>资源调度模块</strong>独立开来，专注做各自的事情。计算引擎可以在任意的分布式资源调度系统上工作，实现真正的可插拔，是一个不错的方向。</p>
<p>与之观念对应的是，spark上集成的graphx和mllib中的许多计算模块，虽然使用起来很简便（几十行核心代码就能搭建一个学习任务的pipeline）。但可以想象的是，随着spark的进一步发展，该分布式计算平台会变的非常重，功能也会越来越多。离专注、专一和极致的解决某类问题越来越远，对每一类问题给出的解决方案并不会特别好。</p>
</blockquote>
</li>
<li><p><a href="http://www.multiboost.org/" target="_blank" rel="external">MultiBoost</a></p>
<p>  <a href="http://www.multiboost.org/" target="_blank" rel="external">MultiBoost</a>工具的侧重点不同于<a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">XGBoost</a>，是Adaboost算法的多分类版本实现，更偏向于解决<strong>multi-class / multi-label / multi-task的分类问题</strong>。</p>
<p>  我们曾经基于该工具训练了用于<strong>用户兴趣画像</strong>的多标签（multi-label）分类模型，其分类效果（Precision / Recall作为指标）要比Naive Bayes好。</p>
<p>  <a href="http://www.multiboost.org/" target="_blank" rel="external">MultiBoost</a>是用C++实现的。值得一提的是，由我们组的算法大神和男神<a href="http://weibo.com/baigang111?is_all=1" target="_blank" rel="external">@BaiGang</a>实现了MulitBoost的spark版本（Scala语言），详见<a href="https://github.com/BaiGang/spark_multiboost" target="_blank" rel="external">Github: Spark_MultiBoost</a></p>
</li>
</ul>
<hr>
<ul>
<li>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/AdaBoost/">AdaBoost</a><a href="/tags/BGDT/">BGDT</a><a href="/tags/Boosting/">Boosting</a><a href="/tags/Gradient-Boosting/">Gradient Boosting</a><a href="/tags/XGBoost/">XGBoost</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter5-bayes-based-family/" title="第05章：深入浅出ML之Bayes-Based家族" itemprop="url">第05章：深入浅出ML之Bayes-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-11-27T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-11-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-11-15</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>判别模型与生成模型</li>
<li>高斯判别分析</li>
<li>朴素贝叶斯算法</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<p>本章虽然是谈贝叶斯，但是先不从贝叶斯开始说起。先介绍有监督学习中两大类经典的模型：判别模型与生成模型。</p>
<p><br></p>
<h3 id="判别模型与生成模型">判别模型与生成模型</h3><hr>
<p>实际上，机器学习的有监督学习的所有模型大体上可以划分为两大类：一类是判别模型，另一类是生成模型。</p>
<ul>
<li><p>判别模型</p>
<p>  判别模型主要是根据特征值求结果的概率，形式化表示为\(P(y|x;w)\)（比如LR模型）。在参数\(w\)确定的情况下，求解<strong>条件概率\(P(y|x)\)</strong>。</p>
<blockquote>
<p>举例：</p>
<p>比如要确定一只羊是山羊还是绵羊，用判别模型的方法是：先从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率和绵羊的概率（即条件概率\(P(y|x)\)）。 </p>
</blockquote>
</li>
</ul>
<table>
<thead>
<tr>
<th>常见的判别模型：</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性回归、对数回归、线性判别分析、SVM、Boosting、CRF、神经网络等</td>
</tr>
</tbody>
</table>
<ul>
<li><p>生成模型</p>
<p>  延续判别模型中的例子，换一种思路考虑：</p>
<blockquote>
<p>我们可以根据山羊的特征，首先学习出一个山羊模型；然后根据绵羊的特征学习出一个绵羊模型。然后从某一只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是哪个。</p>
</blockquote>
<p>  上面描述形式化表示为\(P(x|y)\)（包括\(P(y)\)），\(y\)是模型结果，\(x\)是特征（向量）。利用贝叶斯公式可以发现两个模型的关联性：</p>
<p>  $$<br>  P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x|y) \cdot P(y)}{P(x)} \qquad(ml.1.5.1)<br>  $$</p>
<blockquote>
<p>在生成模型里面，由于我们关系的是\(y\)的离散值结果中哪个概率大（比如山羊概率和绵羊概率哪个大），而不是关心具体的概率，因此上式可改写为：</p>
<p>$$<br>  \begin{align}<br>  \arg \max_{y} P(y|x) &amp; = \arg \max_{y} \frac{P(x|y) P(y)}{P(x)} \quad(1) \\<br>  &amp; = \arg \max_{y} P(x|y) P(y)    \;\quad(2)<br>  \end{align} \qquad(n.ml.1.5.1)<br>  $$</p>
<p>其中，\(P(y)\)称为先验概率，\(P(y|x)\)是后验概率，\(P(x|y)\)为似然函数。公式\((1)\)等价于\((2)\)是因为对于离散值\(y\)来说，\(P(x)\)都是一样的。</p>
</blockquote>
</li>
</ul>
<table>
<thead>
<tr>
<th>常见的生成模型：</th>
</tr>
</thead>
<tbody>
<tr>
<td>隐马尔可夫模型、朴素贝叶斯模型、高斯混合模型、主题模型（LDA等）、受限波尔斯曼机等</td>
</tr>
</tbody>
</table>
<ul>
<li>联系与区别<ul>
<li>由于\(P(x,y) = P(x|y) \cdot P(y)\)，判别模型求的是条件概率，生成模型求的是联合概率。</li>
</ul>
</li>
</ul>
<ol>
<li>概率论的乘法法则</li>
<li>流派之争：频率学派与贝叶斯学派</li>
<li>先验概率与后验概率</li>
</ol>
<p><br></p>
<h3 id="高斯判别分析">高斯判别分析</h3><hr>
<p>这里在介绍朴素贝叶斯算法之前，先介绍一下高斯判别分析模型 (Gaussian Discriminant Analysis)。</p>
<ul>
<li><p>多变量正态分布</p>
<p>  多变量正态分布（又称多值正态分布）描述的是\(n\)维随机变量的分布情况。单变量正态分布中的期望\(\mu\)在这里变成了向量，\(\sigma\)也变成了矩阵\(\Sigma\)，记作\(N(\mu,\Sigma)\)。</p>
<p>  假设有\(n\)个随机变量\(X_1, X_2, \cdots, X_n\)。\(\mu\)的第\(i\)个分量是\(E(X_i)\)，其中\(\Sigma_{ii}=Var(X_i)\)，\(\Sigma_{ij} = Cov(X_i, X_j)\)。多变量概率分布对应的概率密度函数如下：</p>
<p>  $$<br>  P(x; \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot |\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) \qquad(ml.1.5.2)<br>  $$ </p>
<p>  其中\(|\Sigma|\)是\(\Sigma\)的行列式，\(\Sigma\)是协方差矩阵，并且是对称半正定的。</p>
<p>  （二维空间示意图）</p>
<blockquote>
<p>注：\(\mu\)决定中心位置，\(\Sigma\)决定投影椭圆的朝向和大小。</p>
</blockquote>
</li>
<li><p>高斯判别分析模型</p>
<p>  如果输入特征\(x\)是连续性随机变量，那么可以使用高斯判别分析模型来确定\(P(x|y)\)。模型如下：</p>
<p>  $$<br>  \begin{align}<br>  y &amp; \thicksim Bernoulli(\phi) \\<br>  x|y=0 &amp; \thicksim \mathcal{N}(\mu_0, \Sigma) \\<br>  x|y=1 &amp; \thicksim \mathcal{N}(\mu_1, \Sigma)<br>  \end{align}  \qquad\qquad(ml.1.5.3)<br>  $$</p>
<p>  其中，输出结果\(y\)服从伯努利分布（\(\phi\)为参数）；在给定的模型下，特征（向量）符合多变量高斯分布。</p>
<blockquote>
<p>回到上面山羊与绵羊的例子，在山羊模型下，它的体征如胡须长度、角大小、毛长度等连续型变量符合高斯分布，他们组成的特征向量符合多变量高斯分布。</p>
</blockquote>
<p>  下面给出该模型中涉及到的概率密度函数：</p>
<p>  $$<br>  \begin{align}<br>  P(y) &amp; = \phi^{y} (1-\phi)^{1-y} \\<br>  P(x|y=0) &amp; = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot |\Sigma|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (x-\mu_0)^T \Sigma^{-1} (x-\mu_0) \right) \\<br>  P(x|y=1) &amp; = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot |\Sigma|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (x-\mu_1)^T \Sigma^{-1} (x-\mu_1) \right) \\<br>  \end{align}  \quad(ml.1.5.4)<br>  $$</p>
<p>  最大似然估计（对数似然）公式：</p>
<p>  $$<br>  \begin{align}<br>  \ell(\phi, \mu_0, \mu_1, \Sigma) &amp; = \log \prod_{i=1}^{m} P(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) \\<br>  &amp; = \log \prod_{i=1}^{m} P(x^{(i)}|y^{(i)};\mu_0, \mu_1, \Sigma) \cdot P(y^{(i)}; \phi)<br>  \end{align} \qquad(ml.1.5.5)<br>  $$</p>
<blockquote>
<p>\(m\)表示样本数，这里有两个\(\mu\)，表示在不同的结果模型下，特征均值不同。但我们假设协方差相同。反映在图形上表现为：不同模型中心位置不同，但形状相同。这样就可以用直线来进行分隔判别。</p>
</blockquote>
<p>  将\((ml.1.5.4)\)带入\((ml.1.5.5)\)，求导后，得到参数估计公式：</p>
<p>  $$<br>  \begin{align}<br>  \phi &amp; = \frac{1}{m} \sum_{i=1}^{m} 1\{y^{(i)}=1\} \qquad\qquad(1)\\<br>  \mu_0 &amp; = \frac{\sum_{i=1}^{m} 1\{y^{(i)}=0\}x^{(i)}} {\sum_{i=1}^{m} 1\{y^{(i)}=0\}} \quad\qquad(2) \\<br>  \mu_1 &amp; = \frac{\sum_{i=1}^{m} 1\{y^{(i)}=1\}x^{(i)}} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}} \quad\qquad(3) \\<br>  \Sigma &amp; = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - \mu_{y^{(i)}}) (x^{(i)} - \mu_{y^{(i)}})^T \quad(4)<br>  \end{align}  \qquad(ml.1.5.6)<br>  $$</p>
<p>  其中，\(\phi\)是训练样本中结果\(y=1\)的样本占比；\(\mu_0\)和\(\mu_1\)分别表示\(y=0\)和\(y=1\)的样本中特征均值；\(\Sigma\)是样本特征方差均值。</p>
<p>  （高斯判别分析模型 示意图）</p>
</li>
<li><p>高斯判别分析（GDA）与LR的关系</p>
<blockquote>
<p>如果将GDA用条件概率方式表示的话，可表示为：</p>
<p>  $$<br>  P(y=1|x; \phi, \mu_0, \mu_1, \Sigma) \qquad\quad(n.ml.1.5.2)<br>  $$</p>
<p>\(y\)是\(x\)的函数，其中\(\phi, \mu_0, \mu_1, \Sigma\)都是参数。我们进一步推导，可得：</p>
<p>  $$<br>  P(y=1|x; \phi, \mu_0, \mu_1, \Sigma) = \frac{1}{1+\exp(-w^Tx)} \qquad(n.ml.1.5.3)<br>  $$</p>
<p>这里\(w\)是\(\phi, \mu_0, \mu_1, \Sigma\)的函数。恰好公式\((n.ml.1.5.3)\)是LR模型的表达式。</p>
</blockquote>
<p>  总结如下：<strong>如果P(x|y)符合多变量高斯分布，那么P(y|x)符合Logisti回归模型</strong>。反之不成立，因为GDA有着更强的假设条件和约束。</p>
<blockquote>
<p>如果我们认定训练数据满足多变量高斯分布，那么GDA能够在训练集上是最好的模型。然而，我们往往事先并不知道训练数据满足什么样的分布，不能做很强的假设。<strong>Logistic回归的条件假设要弱于GDA，因此更多时候采用LR方法。</strong></p>
<p>例如，训练数据满足泊松分布，\(x|y=0 \thicksim Poisson(\lambda_0); \; x|y=1 \thicksim Poisson(\lambda_1)\)。那么可以得到\(P(y|x)\)也是Logistic回归模型。（此时如果采用GDA，效果就会比较差，因此训练样本的特征分布是泊松分布，非多变量高斯分布。）</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h3 id="朴素贝叶斯">朴素贝叶斯</h3><hr>
<p><br></p>
<h4 id="朴素贝叶斯模型">朴素贝叶斯模型</h4><hr>
<p>在高斯判别分析中，我们要求特征向量\(x\)是连续型的实数向量。如果\(x\)是离散型向量的话，可以考虑采用朴素贝叶斯的分类方法。</p>
<ul>
<li><p>多项式分布（Multinomial Distribution）</p>
<blockquote>
<p>这里以经典的邮件分类（文本分类的一种应用）为例，假如邮件要分为两类：垃圾邮件和正常邮件。采用最简单的特征描述方法，首先要找一个词库，然后将每封邮件中的内容表示成一个向量（根据词库进行过滤后），向量中每一维都是词库中的一个词的0/1值（1表示该词在词库中出现，0表示未出现）。</p>
<p>比如一封邮件中出现了“的”和“打折”，没有出现“通知”，“提交”，“预约”等，那么形式化表示为：</p>
<p>  $$<br>  \phi(x) =<br>  \begin{equation}<br>  \left[</p>
<pre><code><span class="string">\begin{array}{cc}</span>
    <span class="number">1</span><span class="string">\\</span>
    <span class="number">0</span> <span class="string">\\</span>
    <span class="string">\vdots</span> <span class="string">\\</span>
    <span class="number">1</span> <span class="string">\\</span>
    <span class="string">\vdots</span> <span class="string">\\</span>
    <span class="number">0</span> <span class="string">\\</span>
<span class="string">\end{array}</span>
</code></pre><p>  \right] \; \leftarrow \;<br>  \left[</p>
<pre><code><span class="string">\begin{array}{cc}</span>
    的<span class="string">\\</span>
    提交 <span class="string">\\</span>
    <span class="string">\vdots</span> <span class="string">\\</span>
    打折 <span class="string">\\</span>
    <span class="string">\vdots</span> <span class="string">\\</span>
    通知 <span class="string">\\</span>
<span class="string">\end{array}</span>
</code></pre><p>  \right]<br>  \end{equation} \qquad(exp.ml.1.5.1)<br>  $$</p>
<p>假设词库中有10000个词，那么\(x\)是10000维的。</p>
</blockquote>
<p>  此时如果要建立多项式分布模型，对该问题来说，就是把每封邮件看作一次随机试验，那么结果的可能性就有\(x^{10000}\)种。(意味着参数\(p_i\)有\(x^{10000}\)个，参数太多，不可能直接用该思路建模。)</p>
<blockquote>
<p>多项式分布：</p>
<p>假设某随机试验有\(k\)个可能结果\(X_1, X_2, \cdots, X_k\)，它们的概率分布分别是\(p_1, p_2, \cdots, p_k\)。在\(N\)次采样的总结果中，\(X_1, X_2, \cdots, X_k\)出现的次数分别为\(n_1, n_2, \cdots, n_k\)（\(N = \sum_{i=1}^{k} n_i\)）。那么这次事件出现的概率\(P\)可表示为（公式中\(x_i\)代表出现\(n_i\)次）：</p>
<p>  $$<br>  P(X_1 = n_1, \cdots, X_k = n_k) =<br>  \begin{cases}<br>  \frac{N!}{n_1! n_2! \cdots n_k!} p_1^{n_1} p_2^{n_2} \cdots p_k^{n_k} &amp;     \; 当N = \sum_{i=1}^{k} n_i 时，\\<br>  \quad\qquad 0 &amp; \; 其它<br>  \end{cases}    \;(n.ml.1.5.4)<br>  $$</p>
</blockquote>
</li>
<li><p>朴素贝叶斯假设</p>
<p>  我们先换一种思路，我们要求的是\(P(y|x)\)，根据生成模型定义，可以求\(P(x|y)\)和\(P(y)\)。假设\(x\)中的特征都是独立的，这个作为朴素贝叶斯假设。</p>
<blockquote>
<p>特征之间相互条件独立是朴素贝叶斯模型的前提条件。如果一封邮件是垃圾邮件\(y=1\)，且这封邮件出现词”buy”与这封邮件是否出现”price”无关，那么”buy”和”price”之间是<strong>条件独立</strong>的。</p>
<p>在给定\(Z\)的情况下，\(X\)和\(Y\)条件独立，形式化表示为：</p>
<p>  $$<br>  P(X|Z) = P(X|Y,Z)    \qquad (n.ml.1.5.5)<br>  $$</p>
<p>也可以表示为：</p>
<p>  $$<br>  P(X,Y|Z) = P(X|Z) \cdot P(Y|Z)    \qquad(n.ml.1.5.6)<br>  $$</p>
</blockquote>
<p>  那么对于一条样本来说，有下式成立：</p>
<p>  $$<br>  \begin{align}<br>  P(x_1, \cdots, x_{10000}) = &amp; P(x_1|y) P(x_2|y, x_1) P(y_3|y, x_1, x_2) \cdots P(x_{10000}|y, x_1, x_2, \cdots, x_{9999}) \\<br>  = &amp; P(x_1|y) P(x_2|y) \cdots P(x_{10000}|y) \\<br>  = &amp; \sum_{i=1}^{n} P(x_i|y)    \qquad\qquad\qquad(ml.1.5.7)<br>  \end{align}<br>  $$</p>
<p>  公式\((ml.1.5.7)\)与NLP中的n-gram语言模型非常类似，这里相当于1-gram。同时也可以看出，朴素贝叶斯假设是约束性很强的假设。通常来说，”buy”与”price”是有关系的，而在NB这里，假设条件独立。</p>
<blockquote>
<p>注：条件独立与独立是两个不同的概念。</p>
</blockquote>
</li>
<li><p><strong>朴素贝叶斯模型（Naive Bayes Model）</strong></p>
<p>  建立朴素贝叶斯的形式化模型：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \phi_{j|y=1} = P(x_j=1 | y=1) \quad(1)\\<br>  &amp; \phi_{j|y=0} = P(x_j=1 | y=0) \quad(2)\\<br>  &amp; \phi_y = P(y=1) \qquad\qquad\;\;(3)<br>  \end{align}    \qquad (ml.1.5.8)<br>  $$</p>
<p>  而我们想要的是模型在所有训练样本上的概率累积最大，即最大似然估计：</p>
<p>  $$<br>  \mathcal{L}(\phi_y, \phi_{i|y=0}, \phi_{i|y=1}) = \prod_{i=1}^{m} P(x^{(i)}, y^{(i)})     \qquad(ml.1.5.9)<br>  $$</p>
<blockquote>
<p>注意：这里是联合概率分布积最大（非LR中的条件概率积），说明朴素贝叶斯是省城模型。</p>
</blockquote>
<p>  公式\((ml.1.5.9)\)求解，可得：</p>
<p>  $$<br>  \begin{align}<br>  \phi_{j|y=1} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=1 \}} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}} \quad(1)\\<br>  \phi_{j|y=0} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=0 \}} {\sum_{i=1}^{m} 1\{y^{(i)}=0\}} \quad(2)\\<br>  \phi_y &amp; = \frac{\sum_{i=1}^{m} 1\{y^{(i)}=0\}} {m} \;\;\,\qquad\qquad(3)<br>  \end{align}    \qquad(ml.1.5.10)<br>  $$</p>
<blockquote>
<p>(3)式表示y=1的样本数在全部样本中的占比。前两个分别表示在\(y=1\)和\(y=0\)的样本中，特征\(x_j=1\)的比例。</p>
</blockquote>
<p>  然而我们最终要求的是：</p>
<p>  $$<br>  \begin{align}<br>  P(y=1|x) &amp; = \frac{P(x|y=1) P(y=1)}{P(x)} \\<br>  &amp; = \frac{\left(\prod_{j=1}^{n}P(x_j|y=1)\right) P(y=1)} {\left(\prod_{j=1}^{n}P(x_j|y=1)\right) P(y=1) + \left(\prod_{j=1}^{n}P(x_j|y=0)\right) P(y=0)}<br>  \end{align} \qquad(ml.1.5.11)<br>  $$</p>
</li>
</ul>
<pre><code>&gt; 实际上，只需要求分子即可，分母对于<span class="command">\\</span>(y=1<span class="command">\\</span>)和<span class="command">\\</span>(y=0<span class="command">\\</span>)都一样。
&gt;
&gt; 朴素贝叶斯模型可以扩展到<span class="command">\\</span>(x<span class="command">\\</span>)和<span class="command">\\</span>(y<span class="command">\\</span>)都有多个值的情况。对于特征是连续的情况，可采用分段的方法将连续值转化为离散值。这涉及到了特征离散化的一些技术，比如等分、等差、信息增益、基尼指数等。（该部分在第11章：特征工程方法中会详细介绍。）
</code></pre><p><br></p>
<h4 id="拉普拉斯平滑">拉普拉斯平滑</h4><hr>
<p>朴素贝叶斯方法有一个致命的缺点就是对数据的稀疏问题过于敏感。</p>
<blockquote>
<p>比如，5.2.1节中介绍的邮件分类，加入现在又来了一封邮件，邮件中出现”NIPS call for papers”。其中NIPS在现有的词典库中不存在（10000个词）。现在我们使用更大的词典库来分类（词的数目由10000变为50000），假设NIPS这个词在词典中的位置是50000。而这个词在训练数据中未曾出现过，那么此时算条件概率的结果是：</p>
<p>$$<br>\begin{align}<br>\phi_{50000|y=1} = \frac {\sum_{i=1}^{m} 1\{x_{50000}^{(i)}=1 \wedge y^{(i)}=1\}} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}} = 0 \\<br>\phi_{50000|y=0} = \frac {\sum_{i=1}^{m} 1\{x_{50000}^{(i)}=1 \wedge y^{(i)}=0\}} {\sum_{i=1}^{m} 1\{y^{(i)}=0\}} = 0<br>\end{align}  \qquad(exp.ml.1.5.2)<br>$$</p>
<p>由于NIPS在以前的所有邮件中，故而其对应的条件概率为0。带入公式\((ml.1.5.11)\)，得到最终的条件概率也为0。</p>
</blockquote>
<p>上述导致条件概率为0的主要原因是朴素贝叶斯的假设－特征之间条件独立，而模型又实用相乘的方式得到结果。为解决此问题，也为训练样本中未出现的特征值，赋予一个较小的值（非0）。具体方法如下：</p>
<blockquote>
<p>假设离散随机变量\(x\)有\(\{1,2,\cdots, k\}\)个值，这里用\(\phi_i=P(x=i)\)来表示每个值的概率。在\(m\)个训练样本中，\(x\)的观察值表示为\(\{x^{(1)}, x^{(2)}, \cdots, x^{(k)} \}\)。其中每个观察值对应k个值中的一个，那么根据之前的方法可以得到：</p>
<p>$$<br>\phi_j = \frac{\sum_{i=1}^{m} 1\{x^{(i)}=j\}}{m}  \qquad(exp.ml.1.5.3)<br>$$</p>
<p>即\(x=j\)出现的比例。</p>
</blockquote>
<p>拉普拉斯平滑法(Laplace Smooth)将每个\(k\)值出现次数事先都加1，通俗地讲就是假设他们都出现过一次，那么修改后的表达式为：</p>
<p>$$<br>\phi_j = \frac{\sum_{i=1}^{m} 1\{x^{(i)}=j\}+1} {m+k}  \qquad(ml.1.5.12)<br>$$</p>
<blockquote>
<p>每个\(x=j\)的分子都加1，分母加k。有\(\sum_{j=1}^{k}=1\)成立。那么在邮件分类问题中，修改后的公式为：</p>
<p>$$<br>\begin{align}<br>\phi_{j|y=1} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=1 \} + 1} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}+2} \quad(1)\\<br>\phi_{j|y=0} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=0 \} + 1} {\sum_{i=1}^{m} 1\{y^{(i)}=0\} + 2} \quad(2)<br>\end{align} \qquad(n.ml.1.5.7)<br>$$</p>
</blockquote>
<p><br></p>
<h4 id="NB示例：文本分类">NB示例：文本分类</h4><hr>
<p>上面介绍的邮件分类其实就是一个典型的文本分类的例子，用于该文本分类的朴素贝叶斯模型又称作多变量伯努利事件模型。</p>
<blockquote>
<p>邮件生成过程描述：</p>
<p>（第1步）首先随机选定了邮件的类型（垃圾邮件与正常邮件，即\(P(y)\)），（第2步）然后观察词库中的每一个词，随机决定是否要出现在邮件中，出现则标记为1，否则标记为0。然后将出现的词组成一封邮件。决定一个词是否出现时根据概率\(P(x_i|y)\)，那么一封邮件的概率可表示为：</p>
<p>$$<br>一封邮件的生成概率=P(x,y) = P(y) \cdot \prod_{i=1}^{n} P(x_i|y) \qquad(exp.ml.1.5.3)<br>$$</p>
<p>注意：这里的\(n\)是词典中词的数目；\(x_i\)表示一个词是否出现，只有两个值0和1，概率之和为1；\(x\)向量是长度为\(n\)的0/1值。</p>
</blockquote>
<p>现在我么换一种思路，不先从词典入手，而是选择从邮件入手。让((i\)表示邮件中的第\(i\)个词，\(x_i\)表示这个词在词典中的位置，那么\(x_i\)的取值范围是\(\{1,2,\cdots,|V|\}\)（\(|V|\)是字典中词的数目）。这样，一封邮件可以表成\(n\)维向量\((x_1, x_2, \cdots, x_n)\)（\(n\)为邮件中词的个数，可以变化，因为每封邮件长度可不同）。</p>
<blockquote>
<p>这就相当于重复投掷\(|V|\)面的骰子，将观察值纪录下来就形成了一封邮件。当然每个面的概率服从\(P(x_i|y)\)，而且每次实验条件独立，这样我们得到的邮件概率为\(P(y) \prod_{i=1}^{n} P(x_i|y)\)。</p>
<p>注意：这里的公式虽然与共公式\((exp.ml.1.5.3)\)相同，但符号意义完全不同。本式中的\(n\)表示邮件中词的数目；\(x_i\)表示\(|V|\)中的一个值；\(x\)向量中元素是词典中的位置。</p>
</blockquote>
<p>形式化表示如下：</p>
<p>\(m\)个训练样本：\(\{(x^{(i)}, y^{(i)}); i=1,\cdots,m\}\)；第\(i\)条样本表示为：\(x^{(i)} = (x_1^{(i)}, x_2^{(i)}, \cdots, x_{n_i}^{(i)})\)，第\(i\)个样本有\(n_i\)个词。</p>
<p>我们仍然按照朴素贝叶斯的方法求最大似然估计，似然函数表示为：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\mathcal{L} (\phi, \phi_{i|y=0}, \phi_{i|y=1}) &amp; = \prod_{i=1}^{m} P(x^{(i)}, y^{(i)}) \\<br>&amp; = \prod_{i=1}^{m} \left(\prod_{j=1}^{n_i} P(x_j^{(i)}|y^{(i)}; \phi_{i|y=0}, \phi_{i|y=1}) \right) \cdot P(y^{(i)}; \phi_y)<br>\end{align} \qquad(n.ml.1.5.8)<br>$$</p>
</blockquote>
<p>求解结果为：</p>
<p>$$<br>\begin{align}<br>\phi_{k|y=1} &amp;= \frac{\sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)} = k \wedge y^{(i)}=1\}} {\sum_{i=1}^{m} 1\{y^{(i)} = 1\} \cdot n_i} \\<br>\phi_{k|y=0} &amp;= \frac{\sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)} = k \wedge y^{(i)}=0\}} {\sum_{i=1}^{m} 1\{y^{(i)} = 0\} \cdot n_i} \\<br>\phi_y &amp; = \frac { \sum_{i=1}^{m} 1\{y^{(i)} = 1\} } {m}<br>\end{align}<br>$$</p>
<p>与之前的分子相比，分母多了\(n_i\)，分子由\(0/1\)变成了\(k\).</p>
<p><br></p>
<h3 id="Next_…"><br><strong>Next …</strong></h3><hr>
<ul>
<li>贝叶斯网络</li>
<li>概率图模型</li>
</ul>
<p><br></p>
<h3 id="参考资料">参考资料</h3><hr>
<ul>
<li><a href="http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971903.html" target="_blank" rel="external">判别模型、生成模型与朴素贝叶斯方法</a></li>
<li>微博：<a href="http://weibo.com/p/1005051667005453/home?" target="_blank" rel="external">@JerryLead</a></li>
</ul>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Gaussian-Discriminant-Analysis/">Gaussian Discriminant Analysis</a><a href="/tags/Naive-Bayes/">Naive Bayes</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter4-kernel-based-family/" title="第04章：深入浅出ML之Kernel-Based家族" itemprop="url">第04章：深入浅出ML之Kernel-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-11-20T14:42:16.000Z" itemprop="datePublished"> 发表于 2015-11-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-11-12</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>对偶优化问题</li>
<li>核方法</li>
<li>支持向量机</li>
</ul>
<p><br></p>
<h3 id="写在前面"><strong>写在前面</strong></h3><hr>
<p>机器学习中有这样一个结论：低维空间线性不可分的数据通过<strong>非线性映射</strong>到高维特征空间则可能线性可分。极端地，假设数据集中有\(m\)个样本，一定可以在\(m-1\)维空间中线性可分。</p>
<blockquote>
<p>每一个样本在特征空间中对应一个点，试想一下，2个点可以用一维空间分开；3个点可以在2维空间（平面）上线性可分；… ；\(m\)个点，可以在\(m-1\)维空间中线性可分。</p>
</blockquote>
<p>高维空间可以让样本线性可分，这固然是优点。但是如果直接采用非线性映射技术在高维空间进行一个学习任务（如分类或回归），则需要确定<strong>非线性映射函数的形式和参数、高维特征空间维数等问题</strong>；而最大的问题在于在高维特征空间运算时可能存在的<strong>“<a href="http://baike.baidu.com/link?url=9DrIsp0paMuo49hhywnp2SXTvo1hKSrFI5nGkapFvgq8O1bI2PoUTEjVcKQw1pAePZOqFquqHVp_yUV8NPVZRK" target="_blank" rel="external">维数灾难</a>“</strong>。How to solve it?</p>
<p>本章要介绍的核函数方法可以有效地解决这类问题。这里先给出核函数工作的基本思路：</p>
<blockquote>
<p>设样本集\(X\)（\(X \in R^k\)）中有两条样本\(x和z\)，非线性函数\(\phi\)实现输入空间\(R^k\)到特征空间\(R^n\)的映射（即\(\phi(z) \in R^n\)），\( k &lt;&lt; n\)。核函数公式：</p>
<p>$$<br>k(x,z) = &lt;\phi(x), \phi(z)&gt;<br>$$</p>
<p>其中，<code>&lt; , &gt;</code>表示内积计算，\(k(x,z)\)为核函数。</p>
</blockquote>
<p>从上式可以看出，<strong>核函数将\(n\)维高维特征空间的内积计算转化为\(k\)维低维输入空间的核函数计算，巧妙地解决了在高维特征空间中计算可能出现的“维数灾难”等问题。从而为高维特征空间解决复杂的学习问题奠定了理论基础。</strong></p>
<p>本章的安排是这样。首先介绍一些对偶优化问题的基本形式，引出核函数并且为后面的SVM做铺垫；然后给出核函数的发展、常用的核函数等方法；最后详细介绍核函数在分类方法中的经典应用－支持向量机。</p>
<p><br></p>
<h3 id="对偶优化问题"><strong>对偶优化问题</strong></h3><hr>
<p>我们都知道，机器学习模型参数学习过程，大多要通过《最优化算法》中的一些方法来求解。其中，带约束条件的最优化问题（极值求解问题）一般是引入拉格朗日乘子法，构建拉格朗日对偶函数，通过求其对偶函数的解，从而得到原始问题的最优解。</p>
<p>其实不仅是带约束的最优化问题，任何一个最优化问题都伴随着一个”影子”最优化问题。其中一个称为原始问题，另一个称为对偶问题。</p>
<p>这里，先以回归模型的目标函数为例，给出其对偶形式；然后在简要描述带约束的最优化问题求解的一般思路，引出核函数。</p>
<p><br></p>
<h4 id="无约束优化问题的对偶形式"><strong>无约束优化问题的对偶形式</strong></h4><hr>
<p>在<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>中，我们给出了回归模型的目标函数：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\min_{w} \quad J(w) &amp;= \frac{1}{2} \sum_{i=0}^{m} \left(h_{w}(x^{(i)}) - y^{(i)} \right)^2 ＋ \frac{\lambda}{2} {\Vert w \Vert}_{2}^{2} \\<br>&amp;= \frac{1}{2} \sum_{i=0}^{m} \left(w^T \phi(x^{(i)}) - y^{(i)} \right)^2 ＋ \frac{\lambda}{2} w^T w<br>\end{align}  \qquad(n.ml.1.4.1)<br>$$</p>
</blockquote>
<p>回归模型通用形式：\(h_w(x) = w^T \phi(x)\)，其中\(\phi(x)\)是一个\(n \times 1\)的向量。对参数向量\(w\)求导，可得：</p>
<p>$$<br>\begin{align}<br>w = -\frac{1}{\lambda} \sum_{i=1}^{m} \underline{ \left(w^T \phi(x^{(i)}) - y^{(i)}\right) } \cdot \phi(x^{(i)}) = \sum_{i=1}^{m} \alpha^{(i)} \cdot \phi(x^{(i)}) = \Phi^T \alpha<br>\end{align} \qquad(ml.1.4.1)<br>$$</p>
<blockquote>
<p>其中<br>\(<br>\alpha^{(i)} = -\frac{1}{\lambda} \left( w^T \phi(x^{(i)}) - y^{(i)} \right) \qquad<br>\)</p>
<p>\(\alpha_{m \times 1} = (\alpha^{(1)}, \alpha^{(2)}, \cdots, \alpha^{(m)})^T\)是一个向量；\(\Phi_{m \times n} = (\phi(x^{(1)}), \phi(x^{(2)}), \cdots, \phi(x^{(m)}))\) 称为<a href="https://en.wikipedia.org/wiki/Design_matrix" target="_blank" rel="external">设计矩阵</a>。</p>
</blockquote>
<p>接下来，按照对偶表示的思路，重新定义目标函数。将\(w = \Phi^T \alpha\)带入公式\((n.ml.1.4.1)\)可得：</p>
<blockquote>
<p>$$<br>J(\alpha) = \frac{1}{2} a^T \Phi \Phi^T \Phi \Phi^T a - \frac{1}{2} a^T \Phi \Phi^T \cdot Y + \frac{1}{2} Y^T Y + \frac{\lambda}{2} a^T \Phi \Phi^T \alpha \qquad(n.ml.1.4.2)<br>$$</p>
</blockquote>
<p>其中真实结果集合\(Y = (y^{(1)}, y^{(2)}, \cdots, y^{(m)})^T\)。定义<a href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="external">格拉姆矩阵(Gramian Matrix)</a> \(K = \Phi \Phi^T\)一个\(m \times m\)的对称矩阵，每一个元素：</p>
<p>$$<br>K_{ij} = \phi(x^{(i)})^T \cdot \phi(x^{(j)}) = k(x^{(i)}, x^{(j)}) \qquad (ml.1.4.2)<br>$$</p>
<blockquote>
<p>格拉姆矩阵形式：</p>
<p>$$<br>K =<br>\begin{bmatrix}<br>(x^{(1)}, x^{(1)}) &amp; (x^{(1)}, x^{(2)}) &amp; \cdots &amp; (x^{(1)}, x^{(m)}) \\<br>(x^{(2)}, x^{(1)}) &amp; (x^{(2)}, x^{(2)}) &amp; \cdots &amp; (x^{(2)}, x^{(m)}) \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>(x^{(m)}, x^{(1)}) &amp; (x^{(m)}, x^{(2)}) &amp; \cdots &amp; (x^{(m)}, x^{(m)})<br>\end{bmatrix} \qquad(n.ml.1.4.3)<br>$$</p>
<p>格拉姆矩阵的一个重要应用是计算线性无关：一组向量线性无关 当且仅当 格拉姆矩阵行列式不等于0。<br>（如果\(\phi(x)\)是随机变量，得到的格拉姆矩阵是协方差矩阵。）</p>
</blockquote>
<p>公式\((ml.1.4.4)\)是一个表示任意两个样本之间关系的函数。后面会提到，每一个玉树值可通过定义核函数来计算。</p>
<p><br></p>
<h4 id="约束优化问题求解一般思路"><strong>约束优化问题求解一般思路</strong></h4><hr>
<ul>
<li><p><strong>拉格朗日乘子（Lagrange Multiplier）</strong></p>
<p>  拉格朗日乘子法通常用于求解带等式约束的极值问题，一个典型的最优化问题形式化表示如下：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \min_{w} \quad f(w) \\<br>  &amp; s.t. \; h_i(w) = 0, \quad i=1,\cdots,k<br>  \end{align}        \qquad\quad(ml.1.4.3)<br>  $$</p>
<p>  目标函数是\(f(w)\)，\(w \in R^n\) （\(n\)表示参数向量个数）；下面是等式约束，其中\(k\)为等式约束的个数。这类问题通常解法是引入拉格朗日乘子（又称算子），这里用\(\beta\)表示乘子，得到的拉格朗日公式为：</p>
<blockquote>
<p>$$<br>  \mathcal{L}(w,\beta) = f(w) + \sum_{i=1}^{k} \beta_i \cdot h_i(w) \qquad(n.ml.1.4.4)<br>  $$</p>
</blockquote>
<p>  然后分别对\(w\)和\(\beta\)求偏导，使得偏导数等于0，进而求解出\(w\)和\(\beta\)。</p>
<blockquote>
<p>为什么引入拉格朗日乘子就可以求解出极值？</p>
<p>主要原因是\(f(w)\)的切线方向（\(dw\)变化方向）受其它等式的约束，\(dw\)的变化方向与\(f(w)\)的梯度方向垂直时才能获得极值。并且<strong>在极值点处，\(f(w)\)的梯度与其它等式梯度的线性组合平行</strong>。因此它们之间存在线性关系。具体可参考《最优化算法》系列。</p>
</blockquote>
</li>
<li><p><strong>拉格朗日对偶（Lagrange Duality）</strong></p>
<p>  对于带有不等式约束的极值问题，形式化表示如下：</p>
<blockquote>
<p>$$<br>  \begin{align}<br>  &amp; \min_{w} \quad f(w) \\<br>  &amp; s.t. \; g_i(w) \leq 0, \quad i = 1, \cdots, l \\<br>  &amp; \quad\;\;\, h_i(w) = 0, \quad i =1, \cdots, k<br>  \end{align}        \qquad(n.ml.1.4.5)<br>  $$</p>
</blockquote>
<p>  定义拉格朗日公式：</p>
<blockquote>
<p>$$<br>  \mathcal{L}(w, \alpha, \beta) = f(w) + \sum_{i=1}^{l} \alpha_i g_i(w) + \sum_{i=1}^{k} \beta_i h_i(w)  \quad(n.ml.1.4.6)<br>  $$</p>
</blockquote>
<p>  公式\((n.ml.1.4.6)\)中的\(\alpha_i\)和\(\beta_i\)都是拉格朗日乘子。但如果按照该公式求解会出现以下问题。</p>
<blockquote>
<p>因为目标函数要求的是最小值，而约束条件\(h_i(w) \le 0\)，如果将\(\alpha_i\)调整为很大的正数，会使得最后的函数结果为负无穷（\(-\infty\)）。</p>
</blockquote>
<p>  因此，我们需要排除上述情况的发生。策略如下，定义函数：</p>
<blockquote>
<p>$$<br>  \theta_{\mathcal{P}}(w) = \max_{\alpha, \beta;\, \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)  \qquad\quad (n.ml.1.4.7)<br>  $$</p>
</blockquote>
<p>  公式\((n.ml.1.4.5)\)中的\(\mathcal{P}\)表示原问题，即primal。_假设\(g_i(w) &gt; 0\)或者\(h_i(w) \neq 0\)，那么我们总可以通过调整\(\alpha_i\)和\(\beta_i\)，使得\(\theta_{\mathcal{P}}(w)\)趋向正无穷。而只有当函数\(g\)和\(h\)满足约束条件时，\(\theta_{\mathcal{P}}(w)\)为\(f(w)\)。_ </p>
<blockquote>
<p>公式\((n.ml.1.4.7)\)精妙之处就在于\(\alpha_i \ge 0\)，而且是求极大值。因此公式\((n.ml.1.4.7)\)可以写为：</p>
<p>$$<br>  \theta_{\mathcal{P}}(w) =<br>  \begin{cases}<br>  f(w), \quad &amp; 如果w满足原问题约束; \\<br>  \; \infty, \quad &amp; otherwise.<br>  \end{cases}        \qquad(n.ml.1.4.8)<br>  $$</p>
<p>公式\((n.ml.1.4.7)\)和\((n.ml.1.4.8)\)是理解拉格朗日对偶的关键。</p>
</blockquote>
<p>  如此，我们原来要求解的\(\min_w f(w)\)可以转化为求解\(\min_w \theta_{\mathcal{P}}(w)\)了。即：</p>
<blockquote>
<p>$$<br>  \min_w f(w) = \min_w \theta_{\mathcal{P}}(w) = \min_{w} \max_{\alpha, \beta;\, \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)  \qquad (n.ml.1.4.9)<br>  $$</p>
</blockquote>
<p>  这里先用\(\mathcal{P}^{\ast}\)表示\(\min_w \theta_{\mathcal{P}}(w)\)。如果直接求解，又会面临如下问题：</p>
<blockquote>
<p>首先面对两个参数\(\alpha、\beta\)，并且参数\(\alpha_i\)也是一个不等式约束；然后再在\(w\)上求极小值。这个过程不容易做，可否有相对容易的解法呢？</p>
</blockquote>
<p>  我们换一个角度考虑该问题，令\(\theta_{\mathcal{D}}(\alpha, \beta) = \min_{w} \mathcal{L}(w, \alpha, \beta)\)。\(\mathcal{D}\)是对偶的意思，\(\theta_{\mathcal{D}}(\alpha, \beta)\)将问题转化为**先求拉格朗日关于\(w\)的最小值，将\(\alpha\)和\(\beta\)看作是固定值。然后再求\(\theta_{\mathcal{D}}(\alpha, \beta)\)的极大值**。即：</p>
<blockquote>
<p>$$<br>  \max_{\alpha, \beta;\, \alpha_i \geq 0} \theta_{\mathcal{D}}(\alpha, \beta) = \max_{\alpha, \beta;\, \alpha_i \geq 0} \min_{w} \mathcal{L}(w, \alpha, \beta)   \qquad (n.ml.1.4.10)<br>  $$</p>
</blockquote>
<p>  问题转化为原问题的对偶问题来求解。其实，相对于原问题来说只是更换了\(max\)和\(min\)的顺序，而一般更换顺序的结果是：\(max \; min \; f(x) \le min \; max \; f(x)\)。用\(\mathcal{D}^{\ast}\)表示对偶问题，与原问题\(\mathcal{P}^{\ast}\)关系如下：</p>
<p>  $$<br>  \mathcal{D}^{\ast} = \max_{\alpha,\beta; \; \alpha_i \ge 0} \min_{w} \mathcal{L}(w, \alpha, \beta) \le \min_{w} \max_{\alpha,\beta; \; \alpha_i \ge 0} \mathcal{L}(w, \alpha, \beta) = \mathcal{P}^{\ast} \quad (ml.1.4.4)<br>  $$</p>
<blockquote>
<p>即将_最小最大问题_转化为_最大最小问题。_ </p>
<p>(这部分可与<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/" target="_blank" rel="external">《第02章：深入浅出ML之Entropy-Based家族》</a>结合起来理解，更容易理解并能建立起优化问题、最大熵以及后面要介绍的SVM之间的公式关联。)</p>
</blockquote>
<p>  这里我们总结下拉格朗日对偶的精髓：</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><strong>将原有参数\(w\)的计算提前并消除\(w\)，使得优化函数变为拉格朗日乘子的单一参数优化问题。</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><br></p>
<h3 id="核方法"><strong>核方法</strong></h3><hr>
<p>在机器学习中，核方法被形式化为特征空间的向量内积。又被称为核技巧（kernel trick），或核置换（kernel substitution）。</p>
<p><br></p>
<h4 id="核函数介绍"><strong>核函数介绍</strong></h4><hr>
<p>早在机器学习学科成立之前，核函数相关理论、核函数技术以及核函数的有效性定理就已经存在。1964年Aizerman等人把势函数（potential function）相关研究引入到模式识别领域。中间过去很多年，直到1992年Boser等人在研究机器学习中最大间隔分类器中再次将核函数引入进来，产生了支持向量机技术。从这开始，核函数在机器学习的理论和应用得到了更多地关注和极大的兴趣。</p>
<p>例如，Scholkopf等人将核函数应用到主成分分析PCA中产生了核主元分析法（kernel PCA），Mika等人将核函数引入到Fisher判别中产生了核Fisher判别分析（kernel Fisher discriminant）等等。这些技术在机器学习、模式识别等不同领域中起到了很重要的作用。</p>
<blockquote>
<p>核函数有效性Mercer定理可追溯至1909年。</p>
</blockquote>
<p><br></p>
<h4 id="核函数引入"><strong>核函数引入</strong></h4><hr>
<p>Andrew Ng男神在《机器学习》课程第一讲的线性回归中提到例子，用回归模型拟合房屋面积（\(x\)表示）与价格（\(y\)表示）之间的关系。示例中的回归模型用\(y=w_0 + w_1 x\)方程拟合。假设从样本的分布中可以看到\(x\)与\(y\)符合3次曲线关系，那么我们希望使用\(x\)的3次多项式来逼近这些样本点。</p>
<p>首先要做的是将一维特征\(x\)映射至三维\((x,x^2,x^3)\)，然后再根据新特征与结果之间的关系模型。我们将这种特征变换称作<strong>特征映射（Feature Mapping）</strong>。映射函数这里用\(\phi(x)\)表示，该例中可表示为：</p>
<blockquote>
<p>$$<br>    \phi(x): \;\; x \longrightarrow<br>    \begin{bmatrix}<br>    x \\<br>    x^2 \\<br>    x^3 \\<br>    \end{bmatrix} \qquad\qquad(exp.ml.1.4.1)<br>$$</p>
</blockquote>
<p>我们希望将映射后的特征应用于后面要介绍的SVM分类中，而不直接用原始特征（Raw Feature）。</p>
<blockquote>
<p>为什么要用映射后的特征，而不是原始特征来计算呢?</p>
<p>为了更好的拟合数据是其中一个原因。另外一个重要原因是本章<strong>写在前面</strong>提到的：样本可能存在在低维空间线性不可分的情况，而将其映射到高维空间中往往就可分。</p>
</blockquote>
<p><br></p>
<h4 id="核函数方法"><strong>核函数方法</strong></h4><hr>
<p>核函数形式化定义：如果原始特征内积\(\langle x,z \rangle\)，映射后为\(\langle \phi(x), \phi(z) \rangle\)，那么核函数定义为：</p>
<p>$$<br>K(x,z) = \phi(x)^T \phi(z)    \qquad (ml.1.4.5)<br>$$</p>
<blockquote>
<p>如果要实现\(\phi(x)^T \phi(z)\)的计算，只需要先计算\(\phi(x)\)，然后再计算\(\phi(z)^T \phi(z)\)即可。然而这种计算方式是非常低效的。如果最初特征是\(n\)维的，现在将其映射到\(n^2\)维，然后再计算，此时时间复杂度从\(O(n)\)上升为\(O(n^2)\)。</p>
</blockquote>
<p><strong>核函数无需知道非线性映射函数\(\phi\)的形式和参数，它会隐式地改变从输入空间到特征空间的映射，进而对特征空间产生影响。</strong> 举例：假设\(x\)和\(z\)都是\(n\)维的，有: </p>
<p>$$<br>K(x,z) = (x^Tz)^2    \qquad\quad(ml.1.4.6)<br>$$</p>
<p>公式\((ml.1.4.6)\)称为<strong>多项式核函数（polynomial kernel）</strong>，它是对原始输入空间到特征空进行多项式映射（非线性变换）。假设\(x,z\)都是\(n\)维向量，对公式\((ml.1.4.6)\)展开后，可得：</p>
<blockquote>
<p>$$<br>\begin{align}<br>K(x,z) &amp;= (x^Tz)^2 = \left( \sum_{i=1}^{n} x_i z_i \right) \cdot \left( \sum_{i=1}^{n} x_i z_i \right) \\<br>&amp; = \sum_{i=1}^{n} \sum_{j=1}^{n} (x_i x_j) (z_i z_j) = \phi(x)^T \phi(z)<br>\end{align}  \qquad(n.ml.1.4.11)<br>$$</p>
</blockquote>
<p>从公式\((n.ml.1.4.11)\)可以看出，只要计算原始特征\(x\)和\(z\)的平方（时间复杂度\(O(n)\)），就等价于映射后特征的内积。</p>
<blockquote>
<p>举例：当\(n\)=2时，根据上面的公式可得映射函数：</p>
<p>$$<br>\phi(x) =<br>\begin{bmatrix}<br>x_1^2 \\<br>\sqrt{2} x_1 x_2 \\<br>x_2^2 \\<br>\end{bmatrix}     \qquad\qquad(exp.ml.1.4.2)<br>$$</p>
<p>令\(x = (x_1, x_2), z = (z_1, z_2)\)</p>
<p>$$<br>\begin{align}<br>K(x,z) &amp;= (x^T z)^2 = (x_1 z_1 + x_2 z_2)^2  \\<br>&amp;= (x_1^2 z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2) \\<br>&amp;= \underline { (x_1^2, \sqrt{2} x_1 x_2, x_2^2) } \cdot \underline { (z_1^2, \sqrt{2} z_1 z_2, z_2^2)^T } \\<br>&amp;= \phi(x)^T \phi(z)<br>\end{align} \qquad(exp.ml.1.4.3)<br>$$</p>
</blockquote>
<p>也就是说，核函数\(K(x,z) = (x^Tz)^2\)只能选择这样的映射函数\(\phi\)时，才能等价于映射后特征的内积。下面再看一个核函数：</p>
<blockquote>
<p>$$<br>\begin{align}<br>K(x,z) &amp; = (x^Tz + c)^2 \\<br>&amp; = \sum_{i=1}^{n} \sum_{j=1}^{n} (x_i x_j)(z_i z_j) + \sum_{i=1}^{n} (\sqrt{2c} x_i) (\sqrt{2c} z_i) + c^2<br>\end{align} \qquad(n.ml.1.4.12)<br> $$</p>
<p>那么其对应的映射函数应该为：</p>
<p>$$<br>\phi(x) =<br>\begin{bmatrix}<br>x_1 x_1 \\<br>x_1 x_2 \\<br>x_1 x_3 \\<br>x_2 x_1 \\<br>x_2 x_2 \\<br>x_2 x_3 \\<br>x_3 x_1 \\<br>x_3 x_2 \\<br>x_3 x_3 \\<br>\sqrt{2c}x_1 \\<br>\sqrt{2c}x_2 \\<br>\sqrt{2c}x_3 \\<br>c \\<br>\end{bmatrix} \qquad\qquad(exp.ml.1.4.4)<br>$$</p>
</blockquote>
<p>由于计算的是内积，我们联想一下余弦相似度：如果\(x\)和\(z\)向量夹角越小，那么么核函数值就越大，反之就越小。因此，核函数值大小与 \(\phi(x)\)和\(\phi(z)\)相似度 成正相关。下面再看一个核函数:</p>
<p>$$<br>K(x,z) = \exp \left( -\frac{|x-z|^2}{2\sigma^2} \right)  \qquad(ml.1.4.7)<br>$$</p>
<p>从公式\((ml.1.4.7)\)可以看出，如果\(x\)和\(z\)很近（\(|x-z| \thickapprox 0\)），那么核函数值为1；如果\(x\)和\(z\)相差很大（\(|x-z| \gg 0\)），那么核函数值约等于0。由于这个函数类似于高斯分布，因此称为<strong>高斯核函数</strong>，也叫做<strong>径向基函数（Radial Basic Function，简称RBF）</strong>。它能够把原始函数映射到无穷维。</p>
<p>下面在看一个核函数：</p>
<p>$$<br>K(x,z) = \tanh(a x^T z + b) \qquad\qquad(ml.1.4.8)<br>$$</p>
<p>上式称为<strong>Sigmoid核函数（sigmoidal kenel）</strong>。Sigmoid核多用于多层感知机神经网络，隐含层节点数目、隐含节点对输入节点的权值都是在训练过程中自动确定。</p>
<blockquote>
<p>注意：sigmoid核对应的核函数矩阵不是正定的。之所以仍作为kernel的一种是因为给了kernel在实际应用中更多的扩展，包括在svm、神经网络等方法中的应用。</p>
</blockquote>
<p><br></p>
<h4 id="核函数有效性判定"><strong>核函数有效性判定</strong></h4><hr>
<p>问题：</p>
<blockquote>
<p>给定一个函数\(K\)，我们能否使用\(K\)来替代计算\(\phi(x)^T \phi(z)\)？也就是说，能否找出一个\(\phi\)，使得对于所有的\(x\)和\(z\)，都有\(K(x,z) = \phi(x)^T \phi(z)\)？</p>
<p>公式\((n.ml.1.4.11)\)描述的核函数\(K(x,z) = (x^T z)^2\)，是否能够认为\(K\)是一个有效的核函数？</p>
</blockquote>
<ul>
<li><p>Mercer定理</p>
<p>  在介绍Mercer定理之前，我们先解决上面的问题：</p>
<blockquote>
<p>给定\(m\)个训练样本\(\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}\)，每一个\(x^{(i)}\)对应一个特征向量。那么，我们可以将任意两个样本\(x^{(i)}\)和\(x^{(j)}\)带入函数\(K\)中，计算得到\( K_{ij} = K(x^{(i)}, x^{(j)})\)，其中\(1 \le i \le m\)，\(1 \le j \le m\)。</p>
<p>这样，我们就可以计算得到一个\(m*m\)的核函数矩阵（Kernel Matrix）。这里为了方便，将和函数矩阵和核函数\(K(x,z)\)都用\(K\)来表示。</p>
</blockquote>
<p>  假设\(K\)是有效的核函数，那么根据核函数定义：</p>
<p>  $$<br>  \begin{align}<br>  K_{ij} &amp; = K(x^{(i)}, x^{(j)}) \\<br>  &amp; = \phi(x^{(i)})^T \phi(x^{(j)}) = \phi(x^{(j)})^T \phi(x^{(i)}) \\<br>  &amp; = K(x^{(j)}, x^{(i)}) = K_{ji}<br>  \end{align}  \qquad(ml.1.4.9)<br>  $$</p>
<p>  可见，核函数矩阵\(K\)应该是个对称阵。下面的公式推导会给出更明确的结论。</p>
<blockquote>
<p>首先，使用符号\(\phi_k(x)\)来表示映射函数\(\phi(x)\)的第\(k\)维属性值。那么，对于任意向量\(z\)，可得：</p>
<p>  $$<br>  \begin{align}<br>  z^T K z &amp; = \sum_{i=1}^{m} \sum_{j=1}^{m} z_i K_{ij} z_j \\<br>  &amp; = \sum_{i=1}^{m} \sum_{j=1}^{m} z_i \phi(x^{(i)})^T \phi(x^{(j)}) z_j \\<br>  &amp; = \sum_{i=1}^{m} \sum_{j=1}^{m} z_i \sum_{k=1}^{n} \phi_k(x^{(i)})\phi_k(x^{(j)}) z_j \\<br>  &amp; = \sum_{k=1}^{n} \sum_{i=1}^{m} \sum_{j=1}^{m} z_i \phi_k(x^{(i)})\phi_k(x^{(j)}) z_j \\<br>  &amp; = \sum_{k=1}^{n} \left( \sum_{i=1}^{m} z_i \phi(x^{(i)}) \right)^2 \\<br>  &amp; \ge 0<br>  \end{align}  \qquad\quad(n.ml.1.4.13)<br>  $$</p>
</blockquote>
<p>  从公式\((n.ml.1.4.13)\)可以看出，如果\(K\)是个有效的核函数（即\(K(x,z)\)与\(\phi(x)^T \phi(z)\)等价），那么，在训练集上得到的核函数矩阵\(K\)应该是半正定的（\(K \ge 0\))。</p>
<p>  如此，我们可以得出核函数的必要条件是：</p>
<blockquote>
<p><strong>\(K\)是有效的核函数 \(\Longrightarrow\) 核函数矩阵\(K\)半正定。</strong></p>
</blockquote>
<p>  比较幸运的是，这个条件也是充分的。具体由<strong>Mercer定理</strong>来表达:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Mercer定理：</th>
</tr>
</thead>
<tbody>
<tr>
<td>如果函数\(K\)是\(R^n \times R^n \rightarrow R\)上的映射（即从两个\(n\)维向量映射到实数域）并且如果\(K\)是个有效的核函数（也称为Mercer核函数），那么当且仅当对于训练样例\( \{x^{(1)}, \cdots, x^{(m)}\}\)来说，其相应的核函数矩阵是半正定的。</td>
</tr>
</tbody>
</table>
<p>Mercer定理可以说明：为了证明\(K\)是有效的核函数，我们不用直接去寻找\(\phi\)，只需要在训练集上求出各个\(K_{ij}\)，然后判定矩阵\(K\)是否是半正定即可。</p>
<blockquote>
<p>注：使用矩阵左上角主子式 \(\ge 0\)等方法可判定矩阵是否是半正定的。</p>
</blockquote>
<p>当然，Mercer定理证明过程中可以通过\(L2\)范数和再生希尔伯特空间等概念，但在\(n\)维情况下，这里给出的证明是等价的。</p>
<p>Kernel方法不仅用在SVM上，只要在模型的计算过程中出现\(\langle x,z \rangle\)，我们都可以使用\(K(x,z)\)去替换，通过特征空间变换解决本章开头所提到的特征映射问题。</p>
<p><br></p>
<h3 id="支持向量机"><strong>支持向量机</strong></h3><hr>
<p>支持向量机（Support Vector Machine，简称SVM）是建立在统计学习理论之上的学习算法，其主要优势体现在解决线性不可分问题，通过引入核函数，巧妙地解决了在高维空间的内积运算复杂度的问题，从而很好的解决了非线性分类问题。</p>
<p>这里我们先通过LR模型与SVM比较，建立二者之间的基本认知。</p>
<p><br></p>
<h4 id="LR与SVM"><strong>LR与SVM</strong></h4><hr>
<ul>
<li><p>LR模型回顾</p>
<p>  在第1章，我们详细阐述了Logistic Regression模型。我们直接地可理解为，LR的目的是从特征中学习出一个0/1分类或回归模型。</p>
<blockquote>
<p>LR特点：</p>
<p>LR模型将特征的线性组合作为自变量（取值范围\((-\infty, \infty)\)），使用logistic函数（又称sigmoid函数）将自变量映射到\((0,1)\)上，映射后的值被认为是y=1的概率（如果是分类问题），表达式 \(P(y=1|x;w)=h_w(x)\)。（具体细节可参考<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>）</p>
</blockquote>
<p>  当我们要判别一个样本（用一组特征表示）属于哪个类时，只需求出\(h_w(x)\)即可；若\(h_w(x)&gt;0.5\)，则属于y=1的类；反之属于y=0的类。（阈值0.5的前提条件：当正负样本基本均衡时）</p>
<blockquote>
<p>再次审视\(h_w(x)\)：</p>
<p>  $$<br> P(Y=1|X=x; w) = h_w(x) = \frac{1}{1+e^{-w^T \cdot x}}<br> $$</p>
<p>我们可以发现， \(h_w(x)\)只与\(w^Tx\)有关：若\(w^Tx \geq 0\)，则\(h_w(x) \geq 0.5\)；\(w^Tx &lt; 0\)，则\(h_w(x) &lt; 0.5\)。也就是说，一个样本真实类别的决定权还在\(w^Tx\)。</p>
<p>从几何角度可这样理解：当\(w^Tx \gg 0\)时，\(h_w(x)=1\)；反之，\(h_w(x)=0\)。</p>
</blockquote>
<p>  如果我们从\(w^Tx\)出发，希望LR模型最终达到的目标就是：<strong>让训练数据中y=1的特征组合尽可能远大于0（即\(w^Tx \gg 0\)）；让训练数据中y=0的特征组合尽可能远小于0（即\(w^Tx \ll 0\)）</strong>。</p>
<p>  LogReg模型要用<strong>全部训练数据</strong>学习参数\(w\)，学习过程中尽可能使正样本的特征组合远大于0，负样本的特征组合远小于0。</p>
<blockquote>
<p><strong>注意：LR强调的是在全部训练样本上达到这个目标，采用MLE作为求参准则。</strong></p>
</blockquote>
</li>
<li><p>SVM直观引入</p>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_1_svm_introduction.png" width="360" height="300" alt="SVM几何间隔" align="right"></p>
<p>  假设\(w^Tx=0\)是正负样本的分割线（或分割面，SVM中称为超平面），存在a、b、c三个样本点（如右图所示），它们到\(w^Tx=0\)的距离分别表示为\(h(a)、h(b)、h(c)\)（其中\(h(a) &gt; h(b) &gt; h(c)\)），即a点距离分割面的距离最远，c点距离最近。</p>
<p>  相对于c点来说，我们更有把握的确定a点的类别，b点类别基本也可以确定。c点因为距离分割面比较近，没有把握确定其类别。</p>
<blockquote>
<p>因为c点距离\(w^Tx=0\)比较近，而训练出来的模型存在一定的误差。参数\(w\)稍微变动一些，可能直接导致c点类别的判断结果截然不同。</p>
</blockquote>
<p>  因此我们可以得出如下总结：<strong>我们更关心的是靠近中间分割面的点，如果让它们尽可能的远离分割面即可，没必要在所有点上达到最优。</strong> 如果这样的话，就需要使得一部分点（距离中间线较远的点）靠近中间线来换取另一部分点（距离中间线较近的点）更加远离中间线。</p>
<p>  也许这就是SVM与LR不同的学习思想和出发点：<strong>前者考虑局部</strong>（只关心距离较近的点，不太关心已经确定远离的点），<strong>后者考虑全局</strong>（整体上达到最远，会出现已经远离的点调整中间线后更加远离）。    </p>
<blockquote>
<p><strong>其实SVM仅利用支持向量（少部分样本）表示训练样本，非全部样本。</strong></p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="函数间隔与几何间隔"><strong>函数间隔与几何间隔</strong></h4><hr>
<ul>
<li><p>形式化表示</p>
<p>  这里，我们先定义SVM中使用的样本标签为\(\{-1, 1\}\)（与LR中的\(\{0, 1\}\)不同）。同时LR中的参数\(w \in R^{n+1}\)，在这里\(w \in R^{n}, b \in R\)。SVM中要求的分类器可表示为：</p>
<p>  $$<br>  h_{w,b}(x) = g(z) = g(w^Tx + b)        \qquad\qquad(ml.1.4.10)<br>  $$</p>
<blockquote>
<p>由于是分类问题，我们只需要考虑\(w^Tx + b\)的正负问题即可，不用关心\(g(z)\)绝对大小。因此我们这里将\(g(z)\)做一个简化，将其简单映射到\(y=1\)和\(y=-1\)上。映射关系如下：</p>
<p>  $$<br>  g(z) =<br>  \begin{cases}<br>  \quad 1, &amp; z \ge 0 \\<br>  \;\, -1, &amp; z &lt; 0<br>  \end{cases}     \qquad\quad(n.ml.1.4.14)<br>  $$</p>
</blockquote>
</li>
<li><p><strong>函数间隔（Functional Margin）</strong></p>
<p>  任意给定一个训练样本\((x^{(i)}, y^{(i)})\)，用\(x^{(i)}\)表示样本特征（向量），\(y^{(i)}\)表示样本标签，\(i\)表示第\(i\)个样本。定义函数间隔：</p>
<p>  $$<br>  \hat{\gamma}^{(i)} = y^{(i)} \left(w^T x^{(i)} + b\right)  \qquad(ml.1.4.11)<br>  $$</p>
<p>  当\(y^{(i)}=1\)时，在公式\((n.ml.1.4.14)\)定义中，\(w^Tx^{(i)}+b \ge 0\)，\(\hat{\gamma}^{(i)}\)的值实际上就是\(\left|w^Tx^{(i)}+b\right|\)，反之亦然。</p>
<blockquote>
<p>为了使函数间隔最大（如此我们就更有信心确认该样本是正例还是反例），当\(y^{(i)}=1\)时，\(w^Tx^{(i)}+b\)应该是个比较大的正数，反之是个较大的负数。</p>
</blockquote>
<p>  因此，<strong>函数间隔可以表示我们认为样本（一组特征表示）是正例还是反例的确信度。</strong></p>
<p>  公式\((ml.1.4.11)\)定义的是一个样本的函数间隔，现在我们定义在全局样本上的函数间隔：</p>
<p>  $$<br>  \hat{\gamma} = \min_{i=1,2,\cdots, m} \hat{\gamma}^{(i)} \qquad\quad(ml.1.4.12)<br>  $$</p>
<p>  即全局函数间隔就是在所有训练样本上分类正例和负例确信度最小的那个函数间隔。</p>
<p>  继续考虑参数\(w、b\)，如果同时加大这两个值，比如在\(w^Tx+b\)前面乘一个系数\(n\)(\(n&gt;1\))，那么所有点的函数间隔都会增加\(n\)倍，参数倍数缩放对求解问题不应该有影响。</p>
<blockquote>
<p>因为我们要求的是\(w^Tx+b=0\)，参数同比例缩放对结果是无影响的。</p>
</blockquote>
<p>  为了限制\(w和b\)，需要加入归一化条件，毕竟求解的目标是确定唯一一个\(w和b\)，而不是多组线性相关的向量。</p>
</li>
<li><p><strong>几何间隔（Geometric Margin）</strong></p>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_2_svm_geometric_margin.png" width="300" height="250" alt="SVM几何间隔" align="right"></p>
<p>  如右图所示，假设分割面\(w^Tx+b=0\)上有了点B，\(n\)维空间任意一点A\((x^{(i)}, y^{(i)})\)到该分割面的距离用\(\gamma^{(i)}\)表示，假设B点就是A在分割面上的投影。利用中学的几何知识可知，向量\(\overrightarrow{BA}\)的方向是\(w\)（分割面的梯度），单位向量是\(\frac{w}{| w |}\)。</p>
<blockquote>
<p>那么B点的坐标可以求得：</p>
<p>  $$<br>  x = x^{(i)} - \gamma^{(i)} \frac{w}{|w|}  \qquad (n.ml.1.4.15)<br>  $$</p>
<p>将公式\((n.ml.1.4.15)\)带入\(w^Tx+b=0\)得到：</p>
<p>  $$<br>  w^T (x^{(i)} - \gamma^{(i)} \frac{w}{|w|}) + b =0 \qquad(n.ml.1.4.16)<br>  $$</p>
<p>进一步整理得到：</p>
<p>  $$<br>  \gamma^{(i)} = \left(\frac{w}{|w|}\right)^T x^{(i)} + \frac{b}{|w|} \qquad(n.ml.1.4.17)<br>  $$</p>
<p>\(\gamma^{(i)}实际上就是n维空间中点到超平面的距离。\) </p>
</blockquote>
<p>  <strong>主要考查点：</strong></p>
<p>  在我们广告算法组的相关面试中，经常会请求职者推导\(n\)维空间中点到超平面的距离公式（如果求职者知道SVM），但是能完整推导出来的只有十之二三 …</p>
<p>  几何间隔在分类问题中，可表示为：</p>
<p>  $$<br>  \gamma^{(i)} = y^{(i)} \left( \left(\frac{w}{|w|}\right)^T x^{(i)} + \frac{b}{|w|} \right) \qquad(ml.1.4.13)<br>  $$</p>
<p>  可以发现，当\(|w|=1\)时，就是函数间隔。其实<strong>几何间隔是函数间隔归一化的结果</strong>。那么，全局几何间隔定义为：</p>
<p>  $$<br>  \gamma = \min_{i=1,2,\cdots, m} \gamma^{(i)} \qquad(ml.1.4.14)<br>  $$</p>
</li>
</ul>
<p><br></p>
<h4 id="最优间隔分类器"><strong>最优间隔分类器</strong></h4><hr>
<p>在前面也提到，SVM的目标是寻找一个超平面，使得距离超平面最近的点能有更大的间距。不考虑所有的点都必须远离超平面，只关心求得的超平面能够让所有的点中离它最近的点具有最大间距。因此，我们需要数学化表示最优间隔（optimal margin classifier）。</p>
<ul>
<li><p>形式化表示</p>
<p>  最优间隔形式化可表示为：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \gamma \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge \gamma, \; i=1,\cdots,m \\<br>  &amp; \qquad |w|=1<br>  \end{align}  \qquad(ml.1.4.15)<br>  $$</p>
<blockquote>
<p>这里利用\(|w|=1\)约束\(w\)，使得\(w^Tx+b\)是几何间隔。</p>
</blockquote>
<p>  到这里其实已经将SVM的模型定义出来了，求解出来的模型称为最优间隔分类器。如果求得了\(w\)和\(b\)，对于任意一个样本\(x\)，我们就能够分类了。那么，<strong>接下来主要工作就是围绕如何求解参数\(w\)和\(b\)来展开的</strong>。</p>
<p>  由于\(|w|=1\)不是凸函数，那么先利用几何间隔和函数间隔的关系转化一下\(\gamma=\frac{\hat{\gamma}}{|w|}\)，因此公式\((ml.1.4.6)\)可改写为：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \frac {\hat{\gamma}} {|w|} \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge \gamma, \; i=1,\cdots,m<br>  \end{align}  \qquad(ml.1.4.16)<br>  $$</p>
<blockquote>
<p>上式所求的极大值仍然是几何间隔，只不过此时的\(w\)不再受\(|w|=1\)的约束了。</p>
</blockquote>
<p>  然而此时的目标函数仍然不是凸函数，无法方便求解。继续改写目标函数…</p>
<p>  前面说过同时缩放\(w\)和\(b\)对结果没有影响，但最后希望求得的是确定值，不是一组倍数值。为了达到这个目的，需要对\(\hat{\gamma}\)做一些限制，以保证最终解是唯一的。为了简便，取\(\hat{\gamma}=1\)。</p>
<blockquote>
<p><strong>将全局函数间隔定义为1</strong>，即将离超平面最近的点的距离定义为\(\frac{1}{|w|}\)（当然定义为其它非0常数也可以，不影响最终的参数求解）。</p>
</blockquote>
<p>  由于求\(\frac{1}{|w|}\)的极大值等价于求\(\frac{1}{2} |w|^2\)的极小值，因此目标函数可进一步改写为：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \frac{1}{2} |w|^2 \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge 1, \; i=1,\cdots,m<br>  \end{align}  \qquad(ml.1.4.17)<br>  $$</p>
<p>  公式\((ml.1.4.17)\)是一个典型的带不等式约束的二次规划求解问题（目标函数是自变量的二次函数）。</p>
<p>  到此为止，我们完成了目标函数由非凸到凸的转变.</p>
</li>
</ul>
<p><br></p>
<h4 id="最优间隔分类器学习过程">最优间隔分类器学习过程</h4><hr>
<ul>
<li><p>KKT条件</p>
<p>  第一节对偶优化问题中，公式\((ml.1.4.4)\)有一个问题：在什么条件下\(\mathcal{P}^{\ast}\)与\(\mathcal{D}^{\ast}\)    两者等价？</p>
<p>  暂时先不回答这个问题，我们假设函数\(f(w)\)和\(g(w)\)是凸函数，\(h(w)\)是仿射的（affine，仿射的含义是指存在\(a_i、b_i\)，能够使得\(h_i(w)=a_i^T w + b_i成立\)）。并且存在\(w\)使得对于所有的\(i\)，\(g_i(w)&lt;0\)。在这种假设下，一定存在\(w^{\ast}、\alpha^{\ast}、 \beta^{\ast}\)使得\(w^{\ast}\)是原问题的解，\(\alpha^{\ast}、\beta^{\ast}\)是对偶问题的解。此时也满足\(\mathcal{P}=\mathcal{D}=\mathcal{L}(w^{\ast},\alpha^{\ast},\beta^{\ast})\)。</p>
<p>  另外，解\(w^{\ast}、\alpha^{\ast}、 \beta^{\ast}\)满足<strong>库恩-塔克条件(Karush-Kuhn-Tucker, 简称KKT条件)</strong>，该条件表示如下：</p>
<p>  $$<br>  \begin{align}<br>  \frac{\partial} {\partial w_i} \mathcal{L}(w^{\ast}, \alpha^{\ast}, \beta^{\ast}) &amp; = 0, \quad i=1,\cdots,n \qquad(1)\\<br>  \frac{\partial}{\partial \beta_i} \mathcal{L}(w^{\ast}, \alpha^{\ast}, \beta^{\ast}) &amp; = 0, \quad i =1, \cdots, k \qquad(2)\\<br>  a^{\ast} g_i(w^{\ast}) &amp; = 0, \quad i=1,\cdots, l \qquad\;(3)\\<br>  g_i(w^{\ast}) &amp; \le 0, \quad i = 1,\cdots, l \qquad\;(4)\\<br>  a^{\ast} &amp; \geq 0, \quad i=1,\cdots, l        \qquad\;(5)<br>  \end{align}  \qquad (ml.1.4.18)<br>  $$</p>
<p>  所以，如果\(w^{\ast}、\alpha^{\ast}、\beta^{\ast}\)满足了库恩-塔克条件，那么它们就是原问题与对偶问题的解。</p>
<p>  在这里我重点关注公式\((ml.1.4.18)-(3)\)，这个条件称作是KKT Dual Complementarity条件。这个条件隐含了<strong>如果\(\alpha^{\ast} &gt; 0\)</strong>，那么\(g_i(w) = 0\)。</p>
<blockquote>
<p>从集合的角度可这样理解：\(g_i(w^{\ast})=0\)时，\(w\)处在可行域的边界上，这时才是真正起作用的约束。而位于可行域内部\(（即g_i(w^{\ast})&lt;0）\)的点都是不起作用的约束，对应的\(\alpha^{\ast}=0\)。</p>
</blockquote>
<p>  这个KKT双重补足条件会用来解释SVM中的<strong>支持向量和SMO的收敛测试</strong>。KKT思想可总结如下：</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><strong>KKT总体思想是认为极值会在可行域边界上取得，也就是不等式为0或等式约束的条件下取得，而最优下降（或上升）方向一般是这些等式的线性组合，其中每个元素要么是不等式为0的约束，要么是等式约束。对于在可行域边界内的点，对目标函数最优解不起作用，因此前面的系数为0。</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li><p>优化目标</p>
<p>  重新前面的SVM的优化问题：</p>
<blockquote>
<p>$$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \frac{1}{2} |w|^2 \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge 1, \; i=1,\cdots,m<br>  \end{align}  \qquad(n.ml.1.4.18)<br>  $$</p>
<p>首先将约束条件改写为：</p>
<p>  $$<br>  g_i(w) = -y^{(i)} (w^T x^{(i)} + b) + 1 \le 0   \qquad\quad(n.ml.1.4.19)<br>  $$</p>
</blockquote>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_5_optimal_margin_classifier.png" width="360" height="250" alt="最优间隔分类器" align="right"></p>
<p>  根据上一节中的KKT条件可知，只有函数间隔是1（离超平面最近的点）的线性约束式前面的系数\(\alpha_i &gt; 0\)，也就是说这些对应的约束式\(g_i(w) = 0\)。对于其它的不在线上的点\(g_i(w) &lt; 0\)，极值不会在它们所在的范围内取得，因此前面的系数\(\alpha_i = 0\)。</p>
<blockquote>
<p>这里每一个约束式对应一个训练样本。</p>
</blockquote>
<p>  如右图所示，实线是最大间隔超平面，用”×”表示正样本，”o”表示负样本。在虚线上的点（两个”×”和一个”o”）就是函数间隔是1的点（又称支持向量），那么它们前面的系数\(\alpha_i &gt; 0\)，其它点都是\(\alpha_i = 0\)。</p>
<p>  因此，可构造拉格朗日函数如下：</p>
<p>  $$<br>  \mathcal{L}(w,b,\alpha) = \frac{1}{2} |w|^2 - \sum_{i=1}^{m} \alpha_i \cdot  \left(y^{(i)} (w^Tx + b) - 1\right)  \qquad(ml.1.4.19)<br>  $$</p>
<blockquote>
<p>说明：这里面只有\(\alpha_i\)而没有\(\beta_i\)，是因为SVM原问题中没有等式约束，只有不等式约束。</p>
</blockquote>
</li>
<li><p>SVM对偶问题</p>
<p>  下面我们看着拉格朗日的对偶问题来求解，对偶函数表达式如下：</p>
<p>  $$<br>  \mathcal{D} = \max_{\alpha;\; \alpha \ge 0} \min_{w,b} \mathcal{L}(w,b,\alpha)  \qquad\qquad(ml.1.4.20)<br>  $$</p>
<ul>
<li><p>首先，极小化过程。固定参数\(\alpha\)，求\(\mathcal{L}(w,b,\alpha)\)关于\(w\)和\(b\)的最小值。分别对\(w\)和\(b\)求偏导：</p>
<p>$$<br>\begin{align}<br>\nabla_w \mathcal{L}(w,b,\alpha) = w - \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} = 0 \qquad(1)\\<br>\frac{\partial}{\partial b} \mathcal{L}(w,b,\alpha) = \sum_{i=1}^{m} \alpha_i y^{(i)} = 0  \qquad\qquad\;\;\;(2)<br>\end{align}        \qquad(ml.1.4.21)<br>$$</p>
<blockquote>
<p>根据公式\((ml.1.4.21)\)-\((1)\)可得到：</p>
<p>\(<br>\qquad\qquad w = \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} \qquad(n.ml.1.4.20)<br>\)</p>
</blockquote>
<p>将公式\((n.ml.1.4.20)\)带入\((ml.1.4.18)\)，此时得到的是该目标函数的最小部分（因为目标函数是凸函数）。公式推导如下：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\mathcal{L}(w,b,\alpha) &amp; = \frac{1}{2} w^T w - \sum_{i=1}^{m} \alpha_i y^{(i)} w^T x^{(i)} - b \cdot \sum_{i=1}^{m} \alpha_i y^{(i)}  + \sum_{i=1}^{m} \alpha_i \qquad\quad (1)\\<br>&amp; = \frac{1}{2} w^T \cdot \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} - w^T \cdot \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} + \sum_{i=1}^{m} \alpha_i - b \sum_{i=1}^{m} \alpha_i y^{(i)} \quad(2) \\<br>&amp; = -\frac{1}{2} w^T \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} + \sum_{i=1}^{m} \alpha_i - b \sum_{i=1}^{m} \alpha_i y^{(i)} \qquad\quad(3) \\<br>&amp; = -\frac{1}{2} \left(\sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} \right)^T \cdot\sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} + \sum_{i=1}^{m} \alpha_i - b\sum_{i=1}^{m} \alpha_i y^{(i)} \quad(4) \\<br>&amp; = -\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \cdot (x^{(i)})^T x^{(j)} + \sum_{i=1}^{m} \alpha_i - b \sum_{i=1}^{m} \alpha_i y^{(i)} \quad(5)<br>\end{align} \;(n.ml.1.4.21)<br>$$</p>
<p>说明：第\((3)\)步到第\((4)\)步使用了线性代数中的转置运算，由于\(\alpha_i\)和\(y^{(i)}\)都是实数，因此转置后一样。第\((4)\)步到第\((5)\)步使用了乘法运算规则: \((a+b)(a+b) = aa + ab + ba + bb\)。</p>
<p>公式\((n.ml.1.4.21)\)主要目的是将\(\mathcal{L}(w,b,\alpha)\)转化为关于拉格朗日乘子\(\alpha\)的函数。</p>
</blockquote>
<p>又因为有\((ml.1.4.21)-(2)\)成立，所以公式\((n.ml.1.4.21)-(5)\)最后一项为0。因此，目标函数最后整理为：</p>
<p>$$<br>\mathcal{L}(w,b,\alpha) = -\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} + \sum_{i=1}^{m} \alpha_i \qquad(ml.1.4.22)<br>$$</p>
<blockquote>
<p>这里我们将向量内积\((x^{(i)})^T x^{(j)}\)表示为\(\langle x^{(i)}, x^{(j)} \rangle\)。<strong>正因为有向量内积的（复杂）计算，才有后面Kernal的出现</strong>。</p>
</blockquote>
</li>
<li><p>接着，求极大化过程。此时极值问题可表示为：</p>
<p>$$<br>\begin{align}<br>&amp; \max_{\alpha} \quad -\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} + \sum_{i=1}^{m} \alpha_i \\<br>&amp; s.t. \quad \alpha_i \ge 0, \quad i=1,\cdots, m \\<br>&amp; \qquad \sum_{i=1}^{m} \alpha_i y^{(i)} = 0.<br>\end{align} \qquad\quad(ml.1.4.23)<br>$$</p>
<blockquote>
<p>从公式\((ml.1.4.23)\)中可以看出，目标函数和线性约束都是凸函数，并且这里不存在不等式约束。根据4.1.4节中介绍的KKT条件可知：存在\(w\)使得对于所有的\(i，g_i(w) &lt; 0\)。因此，一定存在\(w^{\ast}、\alpha^{\ast}\)使得\(w^{\ast}\)是原问题的解，\(\alpha^{\ast}\)是对偶问题的解。（在这里，求\(\alpha_i\)就是求\(\alpha^{\ast}\)了。）</p>
</blockquote>
</li>
</ul>
</li>
<li><p>对偶问题求解思路</p>
<p>  如果求出\(\alpha_i\)，根据\((n.ml.1.4.20)\)可求出\(w\)（即\(w^{\ast}\)，原问题的解）。参数\(b\)的求解可利用下面的公式：</p>
<blockquote>
<p>$$<br>  b^{\ast} = - \frac{\max_{i:y^{(i)}=-1} {w^{\ast}}^T x^{(i)} + \min_{i:y^{(i)}=1} {w^{\ast}}^T x^{(i)}} {2}  \qquad\quad (n.ml.1.4.22)<br>  $$</p>
</blockquote>
<p>  即<strong>离超平面最近的正的函数间隔要等于离超平面最近的负的函数间隔</strong>。这样，我们就可以求出最优间隔分类器\(w^T x + b = 0\)。</p>
<blockquote>
<p>此外，我们根据公式\((n.ml.1.4.20)\)得到了参数\(w\)的表达式。但别忘了，我们SVM通篇考虑的是最优间隔分类器\(w^x+b=0\)的求解。根据\(w\)的表达式，我们带入方程可得：</p>
<p>  $$<br>  w^T x + b = \left(\sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)}\right)^T x + b = \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x \rangle + b  \qquad(n.ml.1.4.23)<br>  $$</p>
<p>也就是说，如果按照\(w^T x + b\)为新来的样本做分类，首先根据\(w\)和\(b\)做一次线性运算，然后看求出来的结果是大于0还是小于0，以此来判断样本是正例还是负例。<strong>现在有了\(\alpha_i\)，我们不需要求\(w\)，只需要将新来的样本与训练数据中的所有样本做内积即可</strong>。</p>
<p><strong>或许存在这样一个疑问：与所有样本做内积是否太耗时了？</strong> 其实不然，根据KKT条件可知，只有支持向量的参数\(\alpha_i\)是大于0的，其它情况都等于0。因此，我们只需要求新样本与支持向量的内积，然后运算即可。</p>
</blockquote>
<p>  上面的讨论都是建立在样本线性可分的假设前提下，当样本不可分时，我们可以尝试利用核函数将特征映射到高维空间，这样很可能就可分了。然而，映射后也不能保证样本100%线性可分，那又该如何？这就是下面通过引入松弛变量的软间隔模型和正则化要解决的问题。</p>
</li>
</ul>
<p><br></p>
<h4 id="软间隔与正则化"><strong>软间隔与正则化</strong></h4><hr>
<p>为了解决上述问题，这里需要将模型做一个调整，以保证在不可分的情况下，也能够尽可能地找出分隔超平面。</p>
<ul>
<li><p>软间隔与惩罚项</p>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_6_soft_margin_and_non_seperate.png" width="270" height="225" alt="soft-margin" align="right"></p>
<p>  从右图可以看到，如果多一个离群点（也许是噪声，黑圆圈表示）可能造成超平面的移动，求出来的最优间隔在缩小（实线部分），那么之前的模型对噪声非常敏感。更有甚者，如果离群点在另一个类中，此时就线性不可分了。</p>
<p>  如何解决这个问题呢？</p>
<p>  我们可以考虑允许一些点游离并在模型求解中违背约束条件（函数间隔\(\ge 1\)）。那么新的模型就可以定义如下：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \min_{w,b,\gamma} \quad \frac{1}{2} |w|^2 + C \sum_{i=1}^{m} \xi_i \\<br>  &amp; s.t \quad y^{(i)} (w^T x^{(i)} + b) \ge 1-\xi_i, \; i=1,2,\cdots,m \\<br>  &amp; \qquad \xi_i \ge 0, \quad i=1,2,\cdots, m<br>  \end{align}    \qquad(ml.1.4.24)<br>  $$</p>
<p>  公式\((ml.1.4.24)\)被称为SVM的<strong>软间隔</strong>模型，非负参数\(\xi_i\)称为松弛变量。</p>
<blockquote>
<p>引入松弛变量就是允许样本点的函数间隔小于1，即在最大间隔区间里面，或者函数间隔可以为负数，即样本点在对方的区域中。</p>
</blockquote>
<p>  而放松限制条件后，我们需要重新调整目标函数，以对离群点进行惩罚。目标函数后面的\(C\sum_{i=1}^{m} \xi_i\)就表示离群点越多，目标函数值就越大，而我们要求的事尽可能小的目标函数值。</p>
<blockquote>
<p>这里的参数\(C\)表现离群点的权重（在《深入浅出ML》系列第7章正则化部分有详细介绍，那里称为惩罚因子，与这里的离群点权重是同一个意思）。\(C\)越大，表明离群点对目标函数影响越大，也就越不希望看到离群点。</p>
</blockquote>
<p>  我们可以给出以下总结：</p>
<p>  <strong>目标函数控制了离群点的数据和程度，使大部分样本点仍然遵守限制条件。同时，考虑了离群点对模型的影响。</strong></p>
<p>  模型修改后，同时也需要对\((ml.1.4.19)\)拉格朗日公式进行修改如下：</p>
<p>  $$<br>  \mathcal{L}(w,b,\alpha) = \frac{1}{2} |w|^2 + C\sum_{i=1}^{m} \xi_i - \sum_{i=1}^{m} \alpha_i \cdot  \left(y^{(i)} (w^Tx + b) - 1 + \xi_i \right) - \sum_{i=1}^{m} \gamma_i \xi_i \quad(ml.1.4.25)<br>  $$</p>
<p>  这里的\(\alpha_i、\gamma_i\)都是拉格朗日乘子。</p>
<blockquote>
<p>回想4.1.4节中介绍的拉格朗日对偶中提到的求法。<br></p>
<ol>
<li>首先，写出拉格朗日公式（如\((ml.1.4.25)\)）；<br></li>
<li>然后，将其看作是变量\(w\)和\(b\)的函数，分别对其求偏导，得到\(w\)和\(b\)的表达式；<br></li>
<li>最后，把表达式带入拉格朗日公式中，求带入公式后的极大值。</li>
</ol>
</blockquote>
<p>  整体推导过程如4.1.5节类似，这里只给出最后结果：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{\alpha} \quad  W(\alpha) = - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} y^{(i)} y^{(j)} \alpha_i \alpha_j \langle x^{(i)}, x^{(j)}\rangle + \sum_{i=1}^{m} \alpha_i \\<br>  &amp; s.t. \quad 0 \le \alpha_i \le C, \quad i=1,\cdots,m \\<br>  &amp; \qquad \sum_{i=1}^{m} \alpha_i y^{(i)} = 0<br>  \end{align} \qquad(ml.1.4.26)<br>  $$</p>
<p>  此时我们会发现，新的目标函数中没有了参数\(\xi_i\)，与之前的模型（公式\((ml.1.4.23)\)）唯一的不同在于\(\alpha_i\)又多了\(\alpha_i \le C\)的限制条件。</p>
<blockquote>
<p>需要注意的是：参数\(b\)的求值公式也发生了变化，改变结果在SMO算法里面介绍。</p>
</blockquote>
<p>  软间隔模型对应的KKT条件，变化如下：</p>
<p>  $$<br>  \begin{align}<br>  \alpha_i = 0 &amp; \Rightarrow y^{(i)} (w^T x^{(i)} + b ) \ge 1 \qquad (1) \\<br>  \alpha_i = C &amp; \Rightarrow y^{(i)} (w^T x^{(i)} + b ) \le 1 \qquad (2) \\<br>  0 &lt; \alpha_i &lt; C &amp; \Rightarrow y^{(i)} (w^T x^{(i)} + b ) = 1 \qquad (3) \\<br>  \end{align} \qquad(ml.1.4.27)<br>  $$</p>
<p>  公式\((ml.1.4.27)\)第\((1)\)式子表明，在两条间隔线外的样本点前面的系数为0；第\((2)\)个式子表明，在间隔线内的样本点（离群点）前面的系数为\(C\)；第\((3)\)个式子表明，支持向量（即在超平面两边的最大间隔线上）的样本想前面系数在\((0, C)\)上。</p>
<blockquote>
<p>通过KKT条件克制，某些在最大间隔线上的样本点也不是支持向量，相反可能是离群点（\((1)、(2)中等号成立时\)）。</p>
</blockquote>
<p>  SMO算法（Sequential Minimal Optimization）用于求解目标函数\(W(\alpha)\)的极大值。</p>
<blockquote>
<p>SMO算法将在《最优化算法》系列给出详细介绍。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="总结"><strong>总结</strong></h4><hr>
<p>这里中带你介绍了SVM的基本公式推导，并就其中的关键点给予了介绍。</p>
<p>首先，通过LR与SVM两个经典模型的比较，前者全局，后者局部（从对参数起作用的训练样本角度），引入SVM；通过函数间隔和几何间隔的介绍，明确了最优间隔的公式表达以及带优化的目标函数；并且将非凸函数转化为凸函数，以便于后续求解；</p>
<p>其次，我们详细地给出了求解带约束的极值优化问题的求解方法：拉格朗日乘子法与拉格朗日对偶法。并且详细推导了最优间隔分类器的优化学习过程，发现参数\(w\)可以用特征向量内积表示，进而引入核函数。此时，我们通过调整核函数就可以完成特征从低维到高维的映射，在低维上计算，实则表现在高位空间上。</p>
<blockquote>
<p>核函数主要用来解决特征空间变换（线性不可分、特征映射等）以及在高维空间计算效率问题。</p>
</blockquote>
<p>由于在实际场景中，训练样本不一定都线性可分。为了提升SVM算法的通用性，引入了松弛变量对优化模型进行软间隔处理，导致的结果就是优化问题变得更加复杂。然而，拉格朗日对偶求解结果中，松弛变量并没有出现在带优化的目标函数中。最后的优化求解问题，也通过SMO算法得到了解决。</p>
<p>最后，值得一提的是：支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不容易出现过拟合现象。</p>
<ul>
<li>参考资料：<ul>
<li>斯坦福大学《机器学习》课程与讲义（Andrew Ng男神）；</li>
<li>台湾大学《机器学习技法》课程与讲义 （林老师）；</li>
<li>《Pattern Recognition and Machine Learning》</li>
<li>技术博客：<a href="http://www.cnblogs.com/jerrylead" target="_blank" rel="external">http://www.cnblogs.com/jerrylead</a></li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/KKT/">KKT</a><a href="/tags/Kernel/">Kernel</a><a href="/tags/SVM/">SVM</a><a href="/tags/VC维/">VC维</a><a href="/tags/对偶优化/">对偶优化</a><a href="/tags/支持向量机/">支持向量机</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter10-clustering-family/" title="第10章：深入浅出ML之Clustering家族" itemprop="url">第10章：深入浅出ML之Clustering家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-10-20T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-10-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-10-15</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>期望最大值算法</li>
<li>高斯混合模型</li>
<li>K-Means聚类</li>
</ul>
<h3 id="写在前面">写在前面</h3><p>聚类属于无监督学习体系下的一类算法，数据样本通常表示为\(D=\{(x^{(1)}, ?), (x^{(2)}, ?), \cdots, (x^{(m)}, ?)\}\)。与分类相比，最明显的特征就是标签\(y\)未知（这里用\(?\)表示）。</p>
<p>那么如何把相近的样本点聚合在一起，同样不相近的样本尽可能不在同一个簇中？一个思路就是假设每个样本有标签，只是“隐藏”起来了，把它当作<strong>隐变量（latent variable）</strong>。然后用监督学习的思路去求解，把相同标签的样本聚合在一起即可。</p>
<p>如此一来，会发现整个过程出现两类变量：</p>
<ul>
<li>样本类别变量</li>
<li>模型参数变量</li>
</ul>
<p>传统的参数学习算法无法解决该类问题，How to do it? 下面要介绍的期望最大值算法可以很好的解决该类问题。</p>
<p><br></p>
<h3 id="期望最大值算法"><strong>期望最大值算法</strong></h3><p>期望最大值（Expectation Maximization，简称EM算法）是<strong>在概率模型中寻找参数最大似然估计或者最大后验估计的算法</strong>，其中概率模型依赖于无法观测的隐藏变量。</p>
<p>其主要思想就是<strong>通过迭代来建立完整数据的对数似然函数的期望界限，然后最大化不完整数据的对数似然函数</strong>。</p>
<p>本节将尽可能详尽地描述EM算法的原理。并结合下一节的高斯混合模型介绍EM算法是如何求解的。</p>
<p><br></p>
<h4 id="数学铺垫"><strong>数学铺垫</strong></h4><ul>
<li><p><strong>Jensen不等式</strong></p>
<p>  在《最优化》相关的资料中，通常会提到凸函数与凹函数的概念。假设\(f\)是定义域为实数的函数，如果对于所有的实数\(x\)，函数的二阶导数\(f^{\prime\prime}(x) \ge 0\)（如果存在的话），那么\(f\)就是凸函数。</p>
<blockquote>
<p> 当\(x\)是向量时，如果其对应的Hessian矩阵H是半正定的（\(H \ge 0\)），那么\(f\)是凸函数。如果\(f^{\prime\prime}(x) &gt; 0\)或\(H&gt;0\)，那么称\(f\)是严格凸函数。</p>
<p>(将\(\ge 0\)换成\(\le 0\)（或者\(&gt;0\)换成\(&lt;0\)）得到的是凹函数的概念。)</p>
</blockquote>
<p>  Jensen不等式表述如下：</p>
<p>  <strong>如果\(f\)是凸函数，\(X\)是随机变量，那么有 \(E[f(X)] \ge f (E[X])\).</strong></p>
<p>  特别的，如果\(f\)是凸函数，那么 \(E[f(X)] = f[E(X)]\)当且仅当\(p(x=E[X])=1\)，此时随机变量\(X\)就是常量。</p>
<p>  </p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_10_1_1_jensen_inequality.png" width="550" height="440" alt="Jensen不等式与凸函数"></p>
<p>  在图中，实现\(f\)是凸函数，\(X\)是随机变量，假设其取值为\(x\)和\(y\)（概率分别为0.5）。那么\(X\)的期望值就是\([x,y]\)的中值，从图中可以看到\(E[f(x)] \ge f(E[x])\)成立。</p>
<blockquote>
<p>Jensen不等式同样可应用于凹函数，不等号方向相反即可，即\(E[f(X)] \le f(E[X])\)。</p>
</blockquote>
</li>
</ul>
<p><br>    </p>
<h4 id="EM算法描述"><strong>EM算法描述</strong></h4><ul>
<li><p><strong>EM初探</strong></p>
<p>  假设给定训练样本\(\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}\)，且样本间独立。我们想寻找每个样本隐变量（类别\(z\)），使得\(P(x,z)\)最大。\(P(x,z)\)的最大似然估计（取对数）如下： </p>
<p>  $$<br>  \ell(w) = \sum_{i=1}^{m} \log P(x^{(i)}; w) = \sum_{i=1}^{m} \log \left( \sum_{z} P(x^{(i)}, z; w) \right) \qquad(ml.1.10.1)<br>  $$</p>
<blockquote>
<p>解释公式\((ml.1.10.1)\)，首先对极大似然函数取对数，然后对每个样本的每一种可能类别\(z\)求<strong>联合分布概率之和</strong>。</p>
</blockquote>
<p>  直接求参数\(w\)会比较困难，因为有隐变量\(z\)的存在，如果确定了\(z\)之后，再求解就容易了（不同于监督学习过程，这里存在两个变量：模型参数变量\(w\)和模型类别变量\(z\)）。</p>
<p>  <strong>EM算法是一种求解存在隐变量的参数优化问题的有效方法</strong>。既然不能直接最大化\(\ell(w)\)，我们可以不断的建立\(\ell\)的下界（E步），然后优化下界（M步）。</p>
</li>
<li><p><strong>公式推导</strong></p>
<p>  对于每一个样本\(x^{(i)}\)，这里用\(Q_i\)表示该样本隐变量\(z\)的某种分布，\(Q_i\)满足的条件\(\sum_{z} Q_i(z) = 1\)且\(Q_i(z)\ge 0\)。</p>
<blockquote>
<p>如果\(z\)是连续型的，那么\(Q_i\)就是概率密度函数，需要将求和符号换成积分符号。比如要将班上学生聚类，假设隐含变量\(z\)表示身高，那么可以认为该变量服从高斯分布。如果隐含变量表示性别，那么\(z\)就服从伯努利分布了。</p>
</blockquote>
<p>  根据前面的数学铺垫和公式\((ml.1.10.1)\)，可得下面的公式：</p>
<p>  $$<br>  \begin{align}<br>  \sum_{i=1}^{m} \log P(x^{(i)}; w) &amp; = \sum_{i=1}^{m} \log \left( \sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; w) \right) \qquad\qquad(1)\\<br>  &amp; = \sum_{i=1}^{m} \underline{ \log \left( \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{P(x^{(i)}, z^{(i)}; w)}{Q_i(z^{(i)})}\right) } \;\,\quad(2)\\<br>  &amp; \ge \sum_{i=1}^{m} \underline{\sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{P(x^{(i)}, z^{(i)}; w)}{Q_i(z^{(i)})} } \;\;\,\qquad(3)<br>   \end{align} \quad(ml.1.10.2)<br>  $$</p>
<blockquote>
<p>公式\((ml.1.10.2)\)解释：</p>
<p>(1)到(2)比较直接，分子分母同乘以一个相等的函数；(2)到(3)利用了Jensen不等式。这里面函数\(log(x)\)是凹函数（二阶导数小于0），并且</p>
<p>  $$<br>  \sum_{z^{(i)}} Q_i(z^{(i)}) \left[\frac{p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right] \qquad(n.ml.1.10.1)<br>  $$</p>
<p>就是\(\left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right]\)的期望。</p>
<p>设\(Y\)是随机变量\(X\)的函数，\(Y=g(X)\)（\(g\)是连续函数），那么有：</p>
<p>①. \(X\)是离散型随机变量，它的分布律为\(P(X=x_k) = p_k\)（\(k=1,2,\cdots\)）。若\(\sum_{k=1}^{\infty} g(x_k)p_k\)绝对收敛，则有</p>
<p>  $$<br>  E[Y] = E[g(X)] = \sum_{i=1}^{\infty} g(x_k) p_k  \qquad(n.ml.1.10.2)<br>  $$</p>
<p>②. \(X\)是连续型随机变量，其对应的概率密度为\(f(x)\)，若\(\int_{-\infty}^{\infty} g(x) f(x) dx\)绝对收敛，则有</p>
<p>  $$<br>  E[Y] = E[g(X)] = \int_{-\infty}^{\infty} g(x) f(x) dx \qquad(n.ml.1.10.3)<br>  $$</p>
<p>回到公式\((ml.1.10.2)\)，\(Y\)相当于\(\left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right]\)；\(X\)是\(z^{(i)}\)；\(Q_i(z^{(i)})\)是\(p_k\)； \(g\)是\(z^{(i)}\)到\(\left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right]\)的映射。</p>
<p>这样就解释了\((2)\)的期望，然后根据Jensen不等式，得到：</p>
<p>  $$<br>  f \left(E_{z^{(i)} \sim Q_i} \left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right] \right) \ge E_{z^{(i)} \sim Q_i} \left[ f\left( \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right)\right]  \quad(n.ml.1.10.4)<br>  $$</p>
</blockquote>
<p>  公式\((ml.1.10.2)-(3)\)可以看作是对对数似然函数\(\ell(w)\)求了下界。对于\(Q_i\)的选择，有很多种可能，那么哪种更好呢？</p>
</li>
<li><p><strong>E步</strong></p>
<p>  为了回答上面的问题，我们<strong>先假设参数\(w\)已经给定</strong>，那么\(\ell(w)\)的值就决定于\(Q_i(z^{(i)})\)和\(P(x^{(i)}, z^{(i)})\)了。这样我们就可以通过调整这两个概率使下界不断上升，以逼近\(\ell(w)\)的真实值。那么，什么时候算是调整好了呢？当公式\((ml.1.10.2)\)不等式变成等式时，说明我们调整后的结果等价于\(\ell(w)\)。</p>
<p>  按照上述思路，我们需要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，即：</p>
<p>  $$<br>  \frac {P(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} = c \quad（等式成立条件）\quad(ml.1.10.3)<br>  $$</p>
<p>  这里\(c\)是常数，不依赖\(z^{(i)}\)。对上式进一步推导，我们知道有\(\sum_{z} Q_i(z^{(i)})=1\)，那么就有\(\sum_{z} P(x^{(i)}, z^{(i)}; w) = c\)。进而可以得到：</p>
<p>  $$<br>  \begin{align}<br>  Q_i(z^{(i)}) &amp; = \frac {P(x^{(i)}, z^{(i)}; w)} {\sum_z P(x^{(i)}, z^{(i)}; w)} = \frac {P(x^{(i)}, z^{(i)}; w)} {P(x^{(i)};w)} \\<br>  &amp; = P(z^{(i)}|x^{(i)};w)<br>  \end{align}    \qquad(ml.1.10.4)<br>  $$</p>
<blockquote>
<p>这里，\(Q_i\)是第\(i\)个样本隐变量\(z\)的某种分布假设。通过建立\(\ell(w)\)的下界，并根据Jensen不等式的等式成立条件，推导出\(Q_i\)对应的计算公式就是后验概率\(P(z|x;w)\)。</p>
</blockquote>
<p>  到此，我么推出了在固定参数\(w\)后，\(Q_i(z^{(i)})\)的计算公式就是后验概率，解决了\(Q_i(z^{(i)})\)如何选择的问题。这一步称为E步：建立对数似然函数\(\ell(w)\)的下界。</p>
</li>
<li><p><strong>M步</strong></p>
<p>  接下来，在给定\(Q_i(z^{(i)})\)后，调整参数\(w\)，极大化\(\ell(w)\)的下界（固定\(Q_i\)后，下界还可以调整的更大）。因此，EM算法的步骤可描述如下（伪代码）：</p>
<blockquote>
<p>\(<br>  \begin{align}<br>  &amp; Loop \; until \; convergence \; \{ \\<br>  &amp; \qquad E－Step: for \; each \; i, \; calculate: \\<br>  &amp; \qquad\qquad\qquad\qquad \underline{ Q_i(z^{(i)}) := P(z^{i}|x^{(i)};w)} \qquad(n.ml.1.10.5)\\<br>  &amp; \qquad M－Step: calculate: \\<br>  &amp; \qquad\qquad\qquad \underline{ w := \arg \max_w \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{P(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} } \quad(n.ml.1.10.6)\\<br>  &amp; \}<br>  \end{align}<br>  \)</p>
</blockquote>
<p>  伪代码中提到，直到收敛循环才退出。那么如何保证收敛？这是下一节要介绍的内容。</p>
</li>
</ul>
<p><br></p>
<h4 id="EM算法收敛性证明"><strong>EM算法收敛性证明</strong></h4><p>假定\(w^{(t)}\)和\(w^{(t+1)}\)是EM算法在第\(t\)次和第\(t+1\)次得到的结果。如果能够证明\(\ell(w^{(t)}) \le \ell(w^{(t+1)})\)，即极大似然估计单调递增，那么最终我们会达到最大似然估计的最大值。</p>
<ul>
<li><p>收敛性证明</p>
<blockquote>
<p>［第\(t\)次：E步］选定 \(w^{(t)}\)后，可以得到E步：</p>
<p>  $$<br>  Q_i^{(t)}(z^{(i)}) = P(z^{(i)} | x^{(i)}; w^{(t)}) \qquad(n.ml.1.10.7)<br>  $$</p>
<p>这一步保证了再给定\(w^{(t)}\)时，Jensen不等式中的等式成立，也就是</p>
<p>  $$<br>  \ell(w^{(t)}) = \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t)})} {Q_i^{(t)} (z^{(i)})} \qquad(n.ml.1.10.8)<br>  $$</p>
<p>［第\(t\)次：M步］然后进行M步，固定\(Q_i^{(t)}(z^{(i)}) \)，并将\(w^{(t)}\)视为变量，对上面的\(\ell(w^{(t)})\)求导后，得到\(w^{(t+1)}\)。经过一些推导，整理后会得到如下式子：</p>
<p>  $$<br>  \begin{align}<br>  \ell(w^{(t+1)}) &amp; \ge \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (x^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t+1)})} {Q_i^{(t)} (x^{(i)})} \quad(1)\\<br>  &amp; \ge \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (x^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t)})} {Q_i^{(t)} (x^{(i)})} \;\;\,\quad(2)\\<br>  &amp; = \ell(w^{(t)})<br>  \end{align} \qquad(n.ml.1.10.9)<br>  $$</p>
<p>公式\((n.ml.1.10.9)\)的\((1)\)，得到\(w^{(t+1)}\)时，只是最大化了\(\ell(w^{(t)})\)，也就是\(\ell(w^{(t+1)})\)的下界，而没有使等式成立。(在第\(t+1\)次的E步时等式才成立，即固定\(w^{(t+1)}\)，并按照Jensen不等式得到\(Q_i\)时。)</p>
<p>第\((1)\)步的成立可以根据公式\((ml.1.10.2)\)来论证。这里的第\((2)\)步直接利用了M步的定义，<strong>M步就是将\(w^{(t)}\)调整为\(w^{(t+1)}\)，使得下界最大化</strong>。</p>
</blockquote>
</li>
</ul>
<table>
<thead>
<tr>
<th>EM算法思想总结：</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>E步会将下界拉到与\(\ell(w)\)一个特定值(这里是\(w^{(t)}\))一样的高度，而此时发现下界仍然可以上升，因此经过M步后，下界又被拉升；但达不到与\(\ell(w)\)另外一个特定值(这里是\(w^{(t+1)}\))一样的高度，然后又进行E步…，如此重复下去，直到最大值。</strong></td>
</tr>
</tbody>
</table>
<p>  这样我们就证明了\(\ell(w)\)会单调增加。收敛方式可以用两种：一种是\(\ell(w)\)不再变化，另一种是\(\ell(w)\)变化幅度很小。</p>
<p>  为了更好的理解EM与其它优化算法的关系，下面我们定义一个公式：</p>
<p> $$<br>J(Q,w) = \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t)})} {Q_i^{(t)} (z^{(i)})} \qquad(ml.1.10.5)<br>$$</p>
<p>从前面的推导中我们知道了\(\ell(w) \ge J(Q,w)\)，那么EM算法可以看作是函数\(J\)的坐标上升法：<strong>E步固定w,优化Q；M步固定Q，优化w</strong>。</p>
<p><br></p>
<h3 id="高斯混合模型"><strong>高斯混合模型</strong></h3><p>密度估计(Density Estimation）是无监督学习非常重要的一个应用方向。本节主要介绍高斯混合模型（Gaussian Mixture Model，简称GMM）如何求解，进而做密度估计的？。</p>
<ul>
<li><p><strong>GMM介绍</strong></p>
<p>  假设给定训练样本\(\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}\)，且样本间独立，将隐含类别标签用\(z^{(i)}\)表示（即EM中的隐变量）。我们首先认为\(z^{(i)}\)是满足一定概率分布的（不是绝对的一个固定值），这里假设满足多项式分布：\(z^{(i)} \sim Miltinomial(\phi)\)，其中\(P(z^{(i)} = j) = \phi_j, \phi_j \ge 0, \sum_{j=1}^{k} \phi_j = 1，z^{(j)}\)有\(k\)个值\(\{1,2,\cdots,k\}\)可选。</p>
<p>  并且我们认为在给定\(z^{(i)}\)后，<strong>\(x^{(i)}\)满足多变量高斯分布</strong>，即\(x^{(i)}|z^{(i)}=j \sim N(\mu_j, \Sigma)\)。由此可以得到联合分布\(P(x^{(i)}, z^{(i)}) = P(x^{(i)}|z^{(i)}) \cdot P(z^{(i)})\)。</p>
<blockquote>
<p>多变量高斯分布介绍可参考《深入浅出ML》系列第05章高斯判别分析模型部分。</p>
</blockquote>
<p>  高斯混合模型可以描述为：</p>
<p>  <strong>对每个样本\(x^{(i)}\)，我们先从\(k\)个类别中按照多项式分布抽取一个\(z^{(i)}\)，然后根据\(z^{(i)}\)所对应的<strong>_\(k\)个多变量高斯分布中的一个_</strong>生成样本\(x^{(i)}\)。整个建模过程得到的模型称为高斯混合模型。</strong></p>
<p>  需要注意的是，这里的\(z^{(i)}\)仍然是隐随机变量（不同于高斯判别分析中的类别已知，监督学习）。模型中还有三个变量\(\phi, \mu\)和\(\Sigma\)。最大似然估计为联合概率分布\(P(x,z)\)，取对数后：</p>
<p>  $$<br>  \begin{align}<br>  \ell(\phi, \mu, \Sigma) &amp; = \sum_{i=1}^{m} \log \underline{ P(x^{(i)}; \phi, \mu, \Sigma) } \\<br>  &amp; = \sum_{i=1}^{m} \log \underline{ \left(\sum_{z^{(i)}=1}^{k} P(x^{(i)}|z^{(i)}; \mu, \Sigma) \cdot P(z^{(i)}; \phi)\right)}<br>  \end{align}    \qquad(ml.1.10.6)<br>  $$</p>
<blockquote>
<p>\(x^{(i)}\)是观测值，已经存在的。那么它是如何生成的呢？是以最大概率生成的… (MLE的思想)。</p>
</blockquote>
<p>  公式\((ml.1.10.6)\)的最大值不能通过高斯判别分析中直接求参数偏导并置为0的方法解决。但是我们假设已经知道了每个样本的标签\(z^{(i)}\)，那么公式\((ml.1.10.6)\)可以简化为：</p>
<p>  $$<br>  \ell(\phi, \mu, \Sigma) = \sum_{i=1}^{m} \left( \log P(x^{(i)}|z^{(i)}; \mu, \Sigma) + \log P(z^{(i)}; \phi) \right)  \qquad(ml.1.10.7)<br>  $$</p>
<p>  此时，我们再对\(\phi, \mu\)和\(\Sigma\)求导，可得：</p>
<p>  $$<br>  \begin{align}<br>  \phi_j &amp; = \frac{1}{m} \sum_{i=1}^{m} 1\{z^{(i)}=j\} \qquad\qquad(1)\\<br>  \mu_j &amp; = \frac{\sum_{i=1}^{m} 1\{z^{(i)}=j\}x^{(i)}} {\sum_{i=1}^{m} 1\{z^{(i)}=j\}} \quad\qquad(2) \\<br>  \Sigma_j &amp; = \frac {\sum_{i=1}^{m} 1\{z^{(i)}=j\} \cdot (x^{(i)} -\mu_j) (x^{(i)} -\mu_j)^T} {\sum_{i=1}^{m} 1\{z^{(i)}=j\}}  \quad(3)<br>  \end{align} \qquad(ml.1.10.8)<br>  $$</p>
<p>  \(\phi_j\)就是样本类别中\(z^{(i)}=j\)的占比。\(\mu_j\)是类别为\(j\)的样本特征均值，\(\Sigma_j\)是类别为\(j\)的样本的特征对应的协方差矩阵。</p>
<blockquote>
<p>公式\((2)\)中的\(\mu_j\)是一个\(n\)维向量：类标号为\(j\)的样本的特征累加和对应的均值。因此\(\mu = (\mu_1, \mu_2, \cdots, \mu_k)^T\)是一个\(k_n\)的矩阵。\(\Sigma_j\)是一个\(n_n\)的方阵，<strong>所有类标号为\(j\)的样本（特征集合）都服从相同参数\((\mu_j, \Sigma_j)\)的多变量高斯分布。</strong></p>
</blockquote>
</li>
<li><p><strong>高斯混合模型与高斯判别分析</strong></p>
<p>  实际上，当知道样本的隐含变量\(z^{(i)}\)（类标号）后，极大似然估计就等同于高斯判别分析模型了。从表面上看，二者不同的是GDA中的类别\(y\)服从伯努利分布，而这里的\(z\)服从多项式分布；此外，这里的每个样本都有不同的协方差矩阵，而GDA中只有一个。</p>
<p>  公式\((ml.1.10.6)\)与第5章中的\((ml.1.5.6)\)表达的是同一个意思。包括\((ml.1.10.8)与(ml.1.5.7)\)的求解结果，在物理意义也是等价的。 </p>
</li>
</ul>
<table>
<thead>
<tr>
<th>GMM与GDA相同点：</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>模型的概率假设相同</strong>：类别\(z^{(i)}\)服从多项式分布，样本特征服从多变量高斯分布；</td>
</tr>
<tr>
<td>2. <strong>目标函数一致</strong>：都是联合概率分布（在训练集上）的极大似然估计</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>GMM与GDA不同点：</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>学习方式不同</strong>：GDA是有监督学习，而GMM用于无监督学习；</td>
</tr>
<tr>
<td>2. <strong>求解算法不同</strong>：GDA直接通过MLE求参数偏导即可得到闭式解；GMM因为有隐变量，需要用EM算法不断迭代，得到结果。</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>GMM参数学习过程</strong></p>
<p>  公式\((ml.1.10.7)\)假设给定了\(z^{(i)}\)，而实际上\(z^{(i)}\)是不知道的，那怎么求解参数呢？</p>
<p>  回到EM算法思想：<strong>用E步猜测隐含变量\(z^{(i)}\)的分布；用M步更新其它参数，以获得更大的似然函数估计值。</strong></p>
<p>  因此用EM算法求解高斯混合模型的步骤如下（伪代码）：</p>
<blockquote>
<p>\(<br>  \begin{align}<br>  &amp; Loop \; until \; convergence \; \{ \\<br>  &amp; \qquad E－Step: for \; each \; i \; and \; j, \; calculate: \\<br>  &amp; \qquad\qquad\qquad\qquad w_j^{(i)} := P(z^{(i)}=j|x^{(i)}; \phi, \mu, \Sigma) \\<br>  &amp; \qquad M－Step: \; update \; parameters: \\<br>  &amp; \qquad\qquad\qquad\qquad \phi_j : = \frac{1}{m} \sum_{i=1}^{m} w_j^{(i)} \\<br>  &amp; \qquad\qquad\qquad\qquad \mu_j := \frac {\sum_{i=1}^{m} w_j^{(i)} x^{(i)}} {\sum_{i=1}^{m} w_j^{(i)}}  \\<br>  &amp; \qquad\qquad\qquad\qquad \Sigma_j := \frac {\sum_{i=1}^{m} w_j^{(i)} \cdot (x^{(i)} -\mu_j) (x^{(i)} -\mu_j)^T} {\sum_{i=1}^{m} w_j^{(i)}}\\<br>   &amp; \}<br>  \end{align}<br>  \)</p>
<p>这里，在E步中，我们将其它参数\(\phi, \mu, \Sigma\)看作常量，计算\(z^{(i)}\)的后验概率，也就是估计隐变量，即为观测样本”划分”类标号，更新每一个样本\(i\)可能的每一个类标号\(j\)的概率分布。</p>
<p><strong>M步是最大似然估计，通过更新参数实现</strong>。这里根据估计出来的属于每一个类的样本，重新计算属于这个类的参数。计算好后发现最大化似然估计时，\(w_j^{(i)}\)值又不对了，需要重新计算，如此循环，直至收敛。</p>
</blockquote>
<p>  参数\(w_j^{(i)}\)的计算公式如下：</p>
<p>  $$<br>  P(z^{(i)}=j|x^{(i)}; \phi, \mu, \Sigma) = \frac {P(x^{(i)}|z^{(i)}=j; \mu, \Sigma) \cdot P(z^{(i)}=j; \phi)} {\sum_{l=1}^{k} P(x^{(i)}|z^{(i)}=l; \mu, \Sigma) \cdot P(z^{(i)}=l; \phi)} \quad(ml.1.10.9)<br>  $$    </p>
<p>  \((ml.1.10.9)\)利用了贝叶斯公式。之所以是条件概率，是因为该式由Jensen不等式推导而来，通过建立似然函数的下界。</p>
<p>  在伪代码中使用了\(w_j^{(i)}\)代替了公式\((ml.1.10.8)\)中的\(1\{z^{(i)}=j\}\)，由0/1值变成了概率值。</p>
<p>  跟下面要介绍的经典聚类算法K-menas相比，<strong>这里使用类别“软”指定</strong>，为每个样例分配的类别\(z^{(i)}\)是有概率分布的，同时计算量也变大了，每个样例\(i\)都要计算属于每一个类别\(j\)的概率。与K-means相同的是，结果仍然是局部最优解。对其它参数取不同的初始值进行多次计算不失为一种好方法。</p>
</li>
</ul>
<p><br></p>
<h3 id="K-means聚类"><strong>K-means聚类</strong></h3><p>K-menas是聚类算法中最经典的一个。这里首先介绍K-means算法的流程，然后阐述其背后包含的EM思想。</p>
<ul>
<li><p><strong>关于聚类</strong></p>
<p>  聚类属于无监督学习，之前章节中的Regression，Naive Bayes，SVM等都是有类别标签的，也就是样本中已经给出了真实分类标签。而聚类样本中没有给定标签，只有特征（向量）。</p>
<blockquote>
<p>比如假设宇宙中的星星可以表示成三维空间中的点集\((x_1,x_2,x_3)\)。<strong>聚类的目的就是找个到每个样本\(\vec{x}\)潜在的类别\(y\)，并将同类别\(y\)的样本\(\vec{x}\)放在一起</strong>。比如上面的星星，聚类后结果是一个个星团，星团里面的点相互距离比较近，星团间的星星距离就比较远了。</p>
</blockquote>
<p>  在聚类问题中，训练样本是\(x^{(1)}, x^{(2)}, \cdots, x^{(m)}\)，每个\(x^{(i)} \in R^n\)，没有类别\(y^{(i)}\)。</p>
</li>
<li><p><strong>K-means算法流程</strong></p>
<p>  K-means算法是将样本聚类成k个簇，算法描述如下：</p>
<blockquote>
<p>\(\{ \\<br>  \qquad\, Step1: 随机选区k个聚类质心点(cluster centroids)，为\mu_1, \mu_2, \cdots, \mu_k \in R^n. \\<br>  \qquad Step2: Loop \; Until \; Convergence \{ \\<br>  \qquad\qquad 对于每一个样例i，计算其应该属于的类。计算公式 \\<br>  \qquad\qquad\qquad c^{(i)} := \arg \min_j |x^{(i)} - \mu_j|^2 \\<br>  \qquad\qquad 对于每一个类j，重新计算该类的质心 \\<br>  \qquad\qquad\qquad \mu_j := \frac{\sum_{i=1}^{m} 1\{c^{(i)}=j\}x^{(i)}} {\sum_{i=1}^{m} 1\{c^{(i)}=j\}}  \\<br>  \qquad \} \\<br>  \}<br>  \)</p>
</blockquote>
</li>
<li><p>\(Step1\)中的k是事先给定的聚类数，\(c^{(i)}\)代表样例\(i\)和\(k\)个类中距离较近的那个类。</p>
<p>  \(c^{(i)}\)的值为\({1,2,\cdots,k}\)。质心\(\mu_j\)表示我们对属于同一个类的样本中心点的猜测。以星团模型为例，就是要将所有的星星聚成\(k\)个星团，首先随机选取\(k\)个宇宙中的点（即\(k\)个星星）作为\(k\)个星团的质心。代码示例如下：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;iostream&gt;</span></span><br><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;cmath&gt;</span></span><br><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;vector&gt;</span></span><br><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;ctime&gt;</span></span><br><span class="line">	<span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">typedef</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> uint;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">struct</span> Cluster</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; centroid;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; samples;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">vector</span>&lt;Cluster&gt; k_means(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; &gt; trainX, <span class="keyword">int</span> k, <span class="keyword">int</span> maxepoches)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">const</span> <span class="keyword">int</span> row_num = trainX.size();</span><br><span class="line">		<span class="keyword">const</span> <span class="keyword">int</span> col_num = trainX[<span class="number">0</span>].size();</span><br><span class="line"></span><br><span class="line">		<span class="comment">/*初始化聚类中心*/</span></span><br><span class="line">		<span class="built_in">vector</span>&lt;Cluster&gt; clusters(k);</span><br><span class="line">		<span class="keyword">int</span> seed = (<span class="keyword">int</span>)time(NULL);</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			srand(seed);</span><br><span class="line">			<span class="keyword">int</span> c = rand() % row_num;</span><br><span class="line">			clusters[i].centroid = trainX[c];</span><br><span class="line">			seed = rand();</span><br><span class="line">		&#125;</span><br><span class="line">		......</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">+ 然后，迭代中的每一步计算每个星星到k个质心中的每一个的距离，选取距离最近的那个星团作为\\(c^&#123;(i)&#125;\\)。</span><br><span class="line"></span><br><span class="line">	这样经过第一步每个星星都有了所属的星团；第二步对于每个星团，重新计算它的质心\\(\mu_j\\)（对里面所有的星星坐标求平均）。重复迭代第一步和第二步直至质心不变或变动很小。示例代码如下：</span><br></pre></td></tr></table></figure>
<p>  vector<cluster> k_means(vector<vector<double> &gt; trainX, int k, int maxepoches)<br>  {</vector<double></cluster></p>
<pre><code><span class="keyword">const</span> size_t row_num = trainX.size();
<span class="keyword">const</span> size_t col_num = trainX[<span class="number">0</span>].size();

<span class="comment">/*初始化聚类中心*/</span>
(在此省略)

<span class="comment">/*多次迭代直至收敛，这里固定迭代次数（100次）*/</span>
<span class="keyword">for</span> (<span class="keyword">int</span> it = <span class="number">0</span>; it &lt; maxepoches; it++)
{
    <span class="comment">/*每一次重新计算样本点所属类别之前，清空原来样本点信息*/</span>
    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)
    {
        clusters[i].samples.clear();
    }
    <span class="comment">/*求出每个样本点距应该属于哪一个聚类*/</span>
    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; row_num; j++)
    {
        <span class="comment">/*都初始化属于第0个聚类*/</span>
        <span class="keyword">int</span> c = <span class="number">0</span>;
        <span class="keyword">double</span> min_distance = cal_distance(trainX[j], clusters[c].<span class="keyword">centroid</span>);
        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; k; i++)
        {
            <span class="keyword">double</span> <span class="built_in">distance</span> = cal_distance(trainX[j], clusters[i].<span class="keyword">centroid</span>);
            <span class="keyword">if</span> (<span class="built_in">distance</span> &lt; min_distance)
            {
                min_distance = <span class="built_in">distance</span>;
                c = i;
            }
        }
        clusters[c].samples.push_back(j);
    }

    <span class="comment">/*更新聚类中心*/</span>
    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)
    {
        vector&lt;<span class="keyword">double</span>&gt; val(col_num, <span class="number">0.0</span>);
        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; clusters[i].samples.size(); j++)
        {
            <span class="keyword">int</span> <span class="keyword">sample</span> = clusters[i].samples[j];
            <span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; col_num; d++)
            {
                val[d] += trainX[<span class="keyword">sample</span>][d];
                <span class="keyword">if</span> (j == clusters[i].samples.size() - <span class="number">1</span>)
                {
                    clusters[i].<span class="keyword">centroid</span>[d] = val[d] / clusters[i].samples.size();
                }
            }
        }
    }
} <span class="comment">// 多次迭代，直至收敛</span>
<span class="keyword">return</span> clusters;
</code></pre><p>  }</p>
<p>  double cal_distance(vector<double> a, vector<double> b)<br>  {</double></double></p>
<pre><code>size_t da = a.size();
size_t db = b.size();

<span class="keyword">if</span> (da != db)
{
    cerr &lt;&lt; <span class="string">"Dimensions of two vectors must be same!\n"</span>;
}

double <span class="function"><span class="keyword">val</span> =</span> <span class="number">0.0</span>;
<span class="keyword">for</span> (size_t i = <span class="number">0</span>; i &lt; da; i++)
{
    <span class="function"><span class="keyword">val</span> <span class="title">+=</span> <span class="title">pow</span>(</span>(a[i] - b[i]), <span class="number">2</span>);
}
<span class="keyword">return</span> pow(<span class="function"><span class="keyword">val</span>, 0.5);</span>
</code></pre><p>  }</p>
</li>
</ul>
<ul>
<li><p><strong>K-means收敛性</strong></p>
<p>  k-means面对的第一个问题是如何保证收敛，算法描述中抢到结束条件就是收敛，可以证明的是k-means完全可以保证收敛。下面是定性的描述，先定义畸变函数（Distortion Function）如下：</p>
<p>  $$<br>  J(c,\mu) = \sum_{i=1}^{m} |x^{(i)} - \mu_{c^{(i)}}|^2 \qquad(ml.1.10.10)<br>  $$</p>
<p>  \(J\)函数表示每个样本点到其质心\(\mu\)的距离平方和。k-means的任务是将\(J\)调整到最小。</p>
<blockquote>
<p>假设当前\(J\)没有达到最小值，此时可以先固定每个类的质心\(\mu_j\)，调整每个样例所属的类别\(c^{(i)}\)来让\(J\)函数减小；同样的，固定\(c^{(i)}\)，调整每个类的质心\(\mu_j\)也可以使\(J\)减小。这两个过程就是<strong>内循环中使\(J\)单调递减的过程</strong>。当\(J\)递减到最小时，\(\mu\)和\(c\)也同时收敛。（理论上，可以存在多组不同的\(\mu\)和\(c\)值能够使得\(J\)取得最小值，但这种情况比较少见）。</p>
</blockquote>
<p>  由于畸变函数\(J\)是非凸函数，意味着我们不能保证取得的最小值是全局最小值，也就是k－menas对质心初始位置的选取比较敏感（其实在多数情况下k-means达到的局部最优已经满足需求，如果担心陷入局部最优，可以选区不同的初始值跑多遍k-means，然后去骑中最小的\(J\)对应的\(\mu\)和\(c\)输出）。</p>
</li>
<li><p><strong>K-means与EM算法</strong></p>
<p>  这里主要是想阐述清楚k-means和EM之间的关系。首先回到初始问题：我们的目标是将样本划分为\(k\)个类。其实说白了就是求每个样本\(x^{(i)}\)的隐含类别\(y^{(i)}\)，然后利用隐含类别\(y^{(i)}\)（有k个取值）将\(x^{(i)}\)归类。</p>
<p>  由于我们事先不知道类别\(y^{(i)}\)，那就先对每个样本假定一个\(y^{(i)}\)吧。此时新的问题来了：怎么知道假定的对不对？如何评价假定的好不好？我们可以使用极大似然估计来度量，这里就是\(x\)和\(y\)的联合分布\(P(x,y)\)。</p>
<blockquote>
<p>如果我们找到的\(y\)能够使\(P(x,y)\)最大，那么可以说\(y\)是样本\(x\)的最佳类别了，\(x\)归类为\(y\)就顺理成章了。但是我们第一次指定的\(y\)不一定会让\(P(x,y)\)最大。而且\(P(x,y)\)还依赖于其它未知参数，当然在给定\(y\)的情况下，我们可以调整其它参数让\(P(x,y)\)最大。但是调整参数后，我们发现有更好的\(y\)可以指定。那么我们重新指定\(y\)，然后再计算\(P(x,y)\)最大时的参数，反复迭代知道没有更好的\(y\)可以指定。</p>
</blockquote>
<p>  联合概率分布\(P(x,y)\)最大所对应的\(y\)就是\(x\)的最佳类别。</p>
</li>
</ul>
<p><br></p>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/EM算法/">EM算法</a><a href="/tags/K-means/">K-means</a><a href="/tags/高斯混合模型/">高斯混合模型</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter3-tree-based-family/" title="第03章：深入浅出ML之Tree-Based家族" itemprop="url">第03章：深入浅出ML之Tree-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-09-28T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-09-28</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-09-15</li>
<li>sina weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>决策树学习过程</li>
<li>特征选择方法</li>
<li>决策树解读</li>
<li>分类与回归树</li>
<li>随机森林</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<p>本章我想以一个例子作为直观引入，来介绍决策树的结构、学习过程以及具体方法在学习过程中的差异。（注：构造下面的成绩示例数据，来说明决策树的构造过程）</p>
<blockquote>
<p>假设某次学生的考试成绩，第一列表示学生编号，第2列表示成绩，第3、4列分别划分两个不同的等级。数据如下表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">Score</th>
<th style="text-align:center">等级1</th>
<th style="text-align:center">等级2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">82</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">74</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">68</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">91</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">88</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">53</td>
<td style="text-align:center">较差</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">76</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">62</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">58</td>
<td style="text-align:center">较差</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">97</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">过关</td>
</tr>
</tbody>
</table>
<p>定义划分等级的标准：</p>
<p>“等级1”把数据划分为4个区间：</p>
<table>
<thead>
<tr>
<th style="text-align:center">分数区间</th>
<th style="text-align:center">[90, 100]</th>
<th style="text-align:center">[75, 90)</th>
<th style="text-align:center">[60, 75)</th>
<th style="text-align:center">[0, 60)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">等级1</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">较差</td>
</tr>
</tbody>
</table>
<p>“等级2”的划分 假设这次考试，成绩超过75分算过关；小于75分不过关。得到划分标准如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">分数区间</th>
<th style="text-align:center">\(score \ge 75\)</th>
<th style="text-align:center">\(0 \le score \lt 75\)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">等级2</td>
<td style="text-align:center">过关</td>
<td style="text-align:center">不过关</td>
</tr>
</tbody>
</table>
</blockquote>
<p>我们按照树结构展示出来，如下图所示：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/head_first_ml/ml_3_0_1_decision_tree_graph.png" width="600" height="500" alt="Decision Tree"></p>
<p>如果按照“等级1”作为划分标准，取值为<code>“优秀”，“良好”，“中等”和“较差”</code>分别对应4个分支，如图4.1所示。由于只有一个划分特征，它对应的是一个单层决策树，亦称作“决策树桩”（Decision Stump）。</p>
<blockquote>
<p>决策树桩的特点是：只有一个非叶节点，或者说它的根节点等于内部节点（我们在下面介绍决策树多层结构时再介绍）。</p>
</blockquote>
<p>“等级1”取值类型是category，而在实际数据中，一些特征取值可能是连续值（如这里的score特征）。如果用决策树模型解决一些回归或分类问题的话，在学习的过程中就需要有将连续值转化为离散值的方法在里面，在特征工程中称为特征离散化。</p>
<blockquote>
<p>在图4.2中，我们把<strong>连续值划分为两个区域</strong>，分别是\(score \ge 75\) 和 \(0 \le score \lt 75\) </p>
</blockquote>
<p>图4.3和图4.4属于CART（Classification and Regression Tree，分类与回归树）模型。<strong>CART假设决策树是二叉树</strong>，根节点和内部节点的特征取值为”是”或”否”，节点的左分支对应”是”，右分支对应“否”，<strong>每一次划分特征选择都会把当前特征对应的样本子集划分到两个区域。</strong></p>
<blockquote>
<p>在CART学习过程中，不论特征原有取值是连续值（如图4.2）或离散值（图4.3，图4.4），也要转化为离散二值形式。</p>
</blockquote>
<p>直观上看，回归树与分类树的区别取决于实际的应用场景（回归问题还是分类问题）以及对应的“Label”取值类型。</p>
<blockquote>
<p><code>Label</code>是连续值，通常对应的是回归树；当<code>Label</code>是category时，对应分类树模型；</p>
</blockquote>
<p>后面会提到，CART学习的过程中最核心的是<strong>通过遍历选择最优划分特征及对应的特征值</strong>。那么二者的区别也体现在具体的最优划分特征的方法上。</p>
<p>同样，为了直观了解本章要介绍的内容，这里用下述表格来说明：</p>
<table>
<thead>
<tr>
<th style="text-align:center">决策树算法</th>
<th style="text-align:center">特征选择方法</th>
<th style="text-align:center">作者信息</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ID3</td>
<td style="text-align:center">信息增益</td>
<td style="text-align:center">Quinlan. 1986. <br>(Iterative Dichotomiser 迭代二分器)</td>
</tr>
<tr>
<td style="text-align:center">C4.5</td>
<td style="text-align:center">增益率</td>
<td style="text-align:center">Quinlan. 1993. </td>
</tr>
<tr>
<td style="text-align:center">CART</td>
<td style="text-align:center">回归树： 最小二乘<br> 分类树： 基尼指数</td>
<td style="text-align:center">Breiman. 1984. <br> (Classification and Regression Tree 分类与回归树)</td>
</tr>
</tbody>
</table>
<p>除了介绍这3个具体算法对应的特征选择方法外，还会简要地介绍决策树学习过程出现的<strong>模型以及数据问题</strong>，如过拟合问题，连续值和缺失值问题等。</p>
<p><br></p>
<h3 id="决策树学习过程">决策树学习过程</h3><hr>
<p><code>图4.1~图4.4</code>给出的仅仅是单层决策树，只有一个非叶节点（对应一个特征）。那么对于含有多个特征的分类问题来说，决策树的学习过程通常是_一个通过递归选择最优划分特征，并根据该特征的取值情况对训练数据进行分割，使得切割后对应的**_数据子集有一个较好的分类_**的过程_。</p>
<blockquote>
<p>为了更直观的解释决策树的学习过程，这里参考《数据挖掘－实用机器学习技术》一书中P69页提供的天气数据，根据天气情况决定是否出去玩，数据信息如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">ID</th>
<th>阴晴</th>
<th>温度</th>
<th>湿度</th>
<th>刮风</th>
<th style="text-align:center">玩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td>overcast</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td>rainy</td>
<td>mild</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td>overcast</td>
<td>cool</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td>sunny</td>
<td>mild</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td>sunny</td>
<td>cool</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td>rainy</td>
<td>mild</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td>sunny</td>
<td>mild</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td>overcast</td>
<td>mild</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td>overcast</td>
<td>hot</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td>rainy</td>
<td>mild</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
</blockquote>
<p>利用ID3算法中的<strong>信息增益</strong>特征选择方法，递归的学习一棵决策树，得到树结构，如图4.5所示：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/head_first_ml/ml_3_1_1_decision_tree_info_gain.png" width="550" height="400" alt="ID3-决策树示意图"></p>
<p>假设 训练数据集\(D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(m)},y^{(m)}) \} \) (特征用离散值表示)，候选特征集合\(F=\{f^1, f^2, \cdots, f^n\} \)。开始，建立根节点，将所有训练数据都置于根节点（\(m\)条样本）。从特征集合\(F\)中选择一个最优特征\(f^{\ast}\)，按照\(f^{\ast}\)取值将训练数据集切分成若干子集，使得各个数据子集有一个在当前条件下最好的分类。</p>
<p>如果子集中样本类别基本相同，那么构建叶节点，并将数据子集划分给对应的叶节点；如果子集中样本类别差异较大，不能被基本正确分类，需要在剩下的特征集合（\(F-\{f^{\ast}\}\)）中选择新的最优特征，构建相应的内部节点，继续对数据子集进行切分。如此递归地进行下去，直至所有数据子集都能被基本正确分类，或者没有合适的最优特征为止。</p>
<p>这样，最终结果是每个子集都被分到叶节点上，对应着一个明确的类别。那么，递归生成的层级结构即为一棵决策树。我们将上面的文字描述用伪代码形式表达出来，如下：</p>
<p>\(<br>\{ \\<br>\quad输入: 训练数据集D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(m)},y^{(m)}) \} \; (特征用离散值表示); \\<br>\qquad\quad\; 候选特征集F=\{f^1, f^2, \cdots, f^n\} \\<br>\quad输出：一颗决策树T(D,F) \\<br>\quad学习过程：\\<br>\qquad 01. \;\; 创建节点node; \\<br>\quad\;\;\;02. \;\; if \; D中样本全属于同一类别C； \; then \\<br>\qquad 03. \qquad 将node作为叶节点，用类别C标记，返回； \\<br>\qquad 04. \; endif \\<br>\qquad 05. \;\; if \; F为空（F=\emptyset）or \; D中样本在F的取值相同；\; then \\<br>\qquad 06. \qquad 将node作为叶节点，其类别标记为D中样本数最多的类（多数表决），返回； \\<br>\qquad 07. \;\underline{ 选择F中最优特征，得到f^{\ast}(f^{\ast} \in F) }； \\<br>\qquad 08. \;标记节点node为f^{\ast} \\<br>\qquad 09. \;\; for \; f^{\ast} \;中的每一个已知值f_{i}^{\ast}; \; do \\<br>\qquad 10. \quad\;\; 为节点node生成一个分支；令D_i表示D中在特征f^{\ast}上取值为f_i^{\ast}的样本子集； \; //划分子集 \\<br>\qquad 11. \quad\;\; if \; D_i为空；\; then \\<br>\qquad 12. \qquad\quad 将分支节点标记为叶节点，其类别标记为D_i中样本最多的类；\; then \\<br>\qquad 13. \quad\;\; else \\<br>\qquad 14. \qquad\quad 以T(D_i, F-\{f^{\ast}\})为分支节点；\quad // 递归过程 \\<br>\qquad 15. \quad\;\; endif \\<br>\qquad 16. \; done<br>\\\}<br>\)</p>
<p>决策树学习过程中递归的每一步，在选择最优特征后，根据特征取值切割当前节点的数据集，得到若干数据子集。由于决策树学习过程是递归地选择最优特征，因此可以理解为这是一个<strong>特征空间划分</strong>的过程。每一个特征子空间对应决策树中的一个叶子节点，特征子空间相应的类别就是叶子节点对应数据子集中样本数最多的类别。</p>
<blockquote>
<p>决策树学习过程可以简要的概括为：通过递归地选择最优特征，实现在整个特征空间维度对样本的划分（每个空间对应一个类别），进而完成了整个分类过程。</p>
</blockquote>
<p><br></p>
<h3 id="特征选择方法"><strong>特征选择方法</strong></h3><hr>
<p>上面多次提到递归地选择最优特征，根据特征取值切割数据集，使得对应的数据子集有一个较好的分类。从伪代码中也可以看出，在决策树学习过程中，最重要的是第07行，即如何选择最优特征？也就是我们常说的特征选择问题。</p>
<p>顾名思义，<strong>特征选择就是将特征的重要程度进行量化后，得到的结果再进行选择</strong>，而如何量化特征的重要性，就成了各种方法间最大的区别。</p>
<blockquote>
<p>例如卡方检验、斯皮尔曼法（Spearman）、<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/#互信息" target="_blank" rel="external">互信息</a>等使用<code>&lt;feature, label&gt;</code>之间的<strong>关联性</strong>来进行量化<code>feature</code>的重要程度。关联性越强，特征得分越高，该特征越应该被优先选择。</p>
</blockquote>
<p>决策树中，我们希望随着特征选择过程地不断进行，决策树的分支节点所包含的样本尽可能属于<br>同一类别，即<strong>希望节点的”纯度（purity）”越来越高</strong>。</p>
<blockquote>
<p>如果子集中的样本都属于同一个类别，当然是最好的结果；如果说大多数的样本类型相同，只有少部分样本不同，也可以接受。</p>
</blockquote>
<p>那么如何才能做到选择的特征对应的样本子集纯度最高呢？</p>
<p>ID3算法用<strong>信息增益</strong>来刻画样例集的纯度; C4.5算法采用<strong>增益率</strong>; CART算法采用<strong>基尼指数</strong>来刻画样例集纯度。</p>
<p><br></p>
<h4 id="信息增益"><strong>信息增益</strong></h4><hr>
<p>信息增益（Information Gain，简称IG）衡量特征的重要性是根据<strong>当前特征为划分带来多少信息量，带来的信息越多，该特征就越重要，此时节点的”纯度”也就越高。</strong></p>
<p>分类系统的信息熵，我们在<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/#熵与信息熵" target="_blank" rel="external">第02章：熵与信息熵</a>部分已经给出计算公式，这里再复习一下：</p>
<blockquote>
<p>对一个分类系统来说，假设类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别总数），每一个类别出现的概率分别是\(p(c_1),p(c_2), \cdots, p(c_k)\)。此时，分类系统的熵可以表示为:</p>
<p>$$<br>H(C ) = - \sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) \qquad (n.ml.1.3.1)<br>$$</p>
<p>分类系统的作用就是输出一个特征向量（文本特征、ID特征、特征特征等）属于哪个类别的值，而这个值可能是\(c_1, c_2, \cdots, c_k\)，因此这个值所携带的信息量就是公式\((n.ml.1.3.1)\)这么多。</p>
</blockquote>
<p>假设离散特征\(t\)的取值有\(I\)个，\(H(C|t=t_i)\) 表示特征\(t\)被取值为\(t_i\)时的条件熵；\(H(C|t)\)是指特征\(t\)被固定时的条件熵。二者之间的关系是：</p>
<blockquote>
<p>$$<br>\begin{align}<br>H(C|t) &amp; = p_1 \cdot H(C|t=t_1) + p_2 \cdot H(C|t=t_2) + \cdots + p_k \cdot H(C|t=t_{n}) \\<br>&amp; = \sum_{i=1}^{I} p_i \cdot H(C|t=t_i)<br>\end{align}        \quad (n.ml.1.3.2)<br>$$</p>
<p>假设总样本数有\(m\)条，特征\(t=t_i\)时的样本数\(m_i\)，\(p_i=\frac{m_i}{m}\).</p>
</blockquote>
<p>接下来，如何求\(P(C|T=t_i)？\)</p>
<blockquote>
<p>以二分类为例（正例为1，负例为0），总样本数为\(m\)条，特征\(t\)的取值为\(I\)个，其中特征\(t=t_i\)对应的样本数为\(m_i\)条，其中正例\(m_{i1}\)条，负例\(m_{i0}\)条（即\(m_i=m_{i0} + m_{i1}\)）。那么有：</p>
<p>$$<br>\begin{align}<br>P(C|T=t_i) &amp; = - \frac{m_{i1}}{m_i} \cdot log_{2} \frac{m_{i1}}{m_i} - \frac{m_{i0}}{m_i} \cdot log_{2} \frac{m_{i0}}{m_i} \\<br>&amp; = -\sum_{j=0}^{k-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i}<br>\end{align} \qquad (n.ml.1.3.3)<br>$$</p>
<p>这里\(k=2\)表示分类的类别数，公式\(\frac{m_{ij}}{m_i}\)物理含义是当\(t=t_i\)且\(C=c_j\)的概率，即条件概率\(p(c_j|t_i)\)。</p>
<p>因此，条件熵计算公式为：</p>
<p>$$<br>\begin{align}<br>H(C|t) &amp; = \sum_{i=1}^{I} p(t_i) \cdot H(C|t=t_i) \\<br>&amp; = - \sum_{i=1}^{I} p(t_i) \cdot \underline { \sum_{j=0}^{k-1} p(c_j|t_i) \cdot log_2 p(c_j|t_i) } \\<br>&amp; = - \sum_{i=1}^{I} \sum_{j=o}^{k-1} p(c_j,t_i) \cdot log_2 p(c_j|t_i)<br>\end{align} \qquad (n.ml.1.3.4)<br>    $$</p>
</blockquote>
<p>特征\(t\)给系统带来的信息增益等于<strong>系统原有的熵与固定特征\(t\)后的条件熵之差</strong>，公式表示如下:</p>
<p>$$<br>\begin{align}<br>IG(T) &amp; = H(C) - H(C|T) \\<br>&amp; = -\sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) + \sum_{i=1}^{n} \sum_{j=1}^{k} p(c_j,t_i) \cdot \log_2 p(c_j|t_i)<br>\end{align}  \qquad(ml.1.3.1)<br>    $$</p>
<p>\(n表示特征t取值个数，k表示类别C个数，\sum_{j=0}^{n-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i}表示每一个类别对应的熵。\)</p>
<p>下面以天气数据为例，介绍通过信息增益选择最优特征的工作过程：</p>
<blockquote>
<p>根据阴晴、温度、湿度和刮风来决定是否出去玩。样本中总共有14条记录，取值为“是”和“否”的yangebnshu分别是9和5，即9个正样本、5个负样本，用\(S(9+,5-)\)表示，S表示样本(sample)的意思。<br> <br></p>
<p>(1). 分类系统的熵:</p>
<p>$$<br>Entropy(S) = info(9,5) = -\frac{9}{14} _ log_2 (\frac{9}{14}) - \frac{5}{14} _ log_2 (\frac{5}{14}) = 0.940位    \quad(exp.1.3.1)<br>$$</p>
<p>(2). 如果以特征”阴晴”作为根节点。“阴晴”取值为{sunny, overcast, rainy}, 分别对应的正负样本数分别为(2+,3-), (4+,0-), (3+,2-)，那么在这三个节点上的信息熵分别为：</p>
<p>$$<br>\begin{align}<br>&amp; Entropy(S| “阴晴”=sunny) = info(2,3) = 0.971位  \quad(exp.1.3.2.1) \\<br>&amp; Entropy(S| “阴晴”=overcast) = info(4,0) = 0位  \;\;\quad(exp.1.3.2.2) \\<br>&amp; Entropy(S| “阴晴”=rainy) = info(3,2) = 0.971位  \;\quad(exp.1.3.2.3)<br>\end{align}<br>$$</p>
<p>以特“阴晴”为根节点，平均信息值（即条件熵）为：<br></p>
<p>$$<br>Entropy(S| “阴晴”) = \frac{5}{14} _ 0.971 + \frac{4}{14} _ 0 + \frac{5}{14} * 0.971 = 0.693位 \quad (exp.1.3.2)<br>$$</p>
<p>以特征\( “阴晴”\)为条件，计算得到的条件熵代表了期望的信息总量，即对于一个新样本判定其属于哪个类别所必需的信息量。<br><br><br>(3). 计算特征\( “阴晴”\)对应的信息增益:</p>
<p>$$<br>IG( “阴晴”) = Entropy(S) - Entropy(S| “阴晴”) = 0.247位 \quad(exp.1.3.3.1)<br>$$</p>
<p>同样的计算方法，可得每个特征对应的信息增益，即</p>
<p>$$<br>IG(“刮风”) = Entropy(S) - Entropy(S|“刮风”) = 0.048位 \qquad\qquad(exp.1.3.3.2) \\<br>IG(“湿度”) = Entropy(S) - Entropy(S|“湿度”) = 0.152位 \qquad\qquad(exp.1.3.3.3) \\<br>IG(“温度”) = Entropy(S) - Entropy(S|“温度”) = 0.029位 \qquad\qquad(exp.1.3.3.4)<br>$$</p>
</blockquote>
<p>显然，特征“阴晴”的信息增益最大，于是把它作为划分特征。基于“阴晴”对根节点进行划分的结果，如图4.5所示（决策树学习过程部分）。决策树学习算法对子节点进一步划分，重复上面的计算步骤。</p>
<p>用信息增益选择最优特征，并不是完美的，存在问题或缺点主要有以下两个：</p>
<ul>
<li><p><strong>倾向于选择拥有较多取值的特征</strong></p>
<blockquote>
<p>尤其特征集中包含ID类特征时，ID类特征会最先被选择为分裂特征，但在该类特征上的分支对预测未知样本的类别并无意义，降低了决策树模型的泛化能力，也容易使模型易发生过拟合。</p>
</blockquote>
</li>
<li><p><strong>只能考察特征对整个系统的贡献，而不能具体到某个类别上</strong></p>
<blockquote>
<p>信息增益只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而无法做“本地”的特征选择（对于文本分类来讲，每个类别有自己的特征集合，因为有的词项（word item）对一个类别很有区分度，对另一个类别则无足轻重）。</p>
</blockquote>
</li>
</ul>
<p>为了弥补信息增益这一缺点，一个被称为<strong>增益率（Gain Ratio）</strong>的修正方法被用来做最优特征选择。</p>
<p><br>     </p>
<h4 id="增益率"><strong>增益率</strong></h4><hr>
<p>与信息增益不同，信息增益率的计算考虑了<strong>特征分裂数据集后所产生的子节点的数量和规模，而忽略任何有关类别的信息</strong>。</p>
<blockquote>
<p>以信息增益示例为例，按照特征“阴晴”将数据集分裂成3个子集，规模分别为5、4和5，因此不考虑子集中所包含的类别，产生一个分裂信息为：</p>
<p>$$<br>SplitInfo(“阴晴”) = info(5,4,5) = 1.577位 \qquad(exp.1.3.4)<br>$$</p>
<p>分裂信息熵（Split Information）可简单地理解为<strong>表示信息分支所需要的信息量</strong>。 <br><br>那么信息增益率：</p>
<p>$$<br>IG_{ratio}(T) = \frac{IG(T)}{SplitInfo(T)} \qquad(n.ml.1.3.5)<br>$$</p>
<p>在这里，特征 “阴晴”的信息增益率为\(IG_{ratio}( “阴晴”)=\frac{0.247}{1.577} = 0.157\)。减少信息增益方法对取值数较多的特征的影响。</p>
</blockquote>
<p>基尼指数（Gini Index）是CART中分类树的特征选择方法。这部分会在下面的“分类与回归树－二叉分类树”一节中介绍。</p>
<p><br></p>
<h3 id="分类与回归树">分类与回归树</h3><hr>
<p>分类与回归树（Classification And Regression Tree, 简称CART）模型在Tree-Based家族中是应用最广泛的学习方法之一。它既可以用于分类也可以用于回归，在<a href="http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/" target="_blank" rel="external">第06章：Boosting家族</a>的核心成员－<a href="http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/#Gradient_Boosting" target="_blank" rel="external">Gradient Boosting</a>就是以该模型作为基本学习器(base learner)。</p>
<p>一句话概括CART模型：</p>
<table>
<thead>
<tr>
<th>CART模型是在给定输入随机变量\(X\)条件下求得输出随机变量\(Y\)的<strong>条件概率分布</strong>的学习方法。</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>在“写在前面”也提到，CART假设决策树时二叉树结构，内部节点特征取值为“是”和“否”，左分支对应取值为“是”的分支，右分支对应为否的分支，如图4.3所示。这样CART学习过程等价于递归地二分每个特征，将输入空间（在这里等价特征空间）划分为有限个字空间（单元），并在这些字空间上确定预测的概率分布，也就是在输入给定的条件下输出对应的条件概率分布。</p>
<blockquote>
<p>可以看出CART算法在叶节点表示上不同于ID3、C4.5方法，后二者叶节点对应数据子集通过“多数表决”的方式来确定一个类别（固定一个值）；而CART算法的叶节点对应类别的概率分布。如此看来，我们可以很容易地用CART来学习一个<code>multi-label / multi-class / multi-task</code>的分类任务。</p>
</blockquote>
<p>与其它决策树算法学习过程类别，CART算法也主要由两步组成：</p>
<ul>
<li>决策树的生成：基于训练数据集生成一棵<strong>二分决策树</strong>；</li>
<li>决策树的剪枝：用验证集对已生成的二叉决策树进行剪枝，剪枝的标准为损失函数最小化。</li>
</ul>
<p>由于分类树与回归树在递归地构建二叉决策树的过程中，选择特征划分的准则不同。二叉分类树构建过程中采用<strong>基尼指数（Gini Index）</strong>为特征选择标准；二叉回归树采用<strong>平方误差最小化</strong>作为特征选择标准。</p>
<p><br></p>
<h4 id="二叉分类树">二叉分类树</h4><hr>
<p>二叉分类树中用基尼指数（Gini Index）作为最优特征选择的度量标准。基尼指数定义如下：</p>
<p>同样以分类系统为例，数据集\(D\)中类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别数），一个样本属于类别\(c_i\)的概率为\(p(i)\)。那么<strong>概率分布的基尼指数</strong>公式表示为：</p>
<p>$$<br>Gini(D) = 1 - \sum_{i=1}^{k} {p_i}^2    \qquad(ml.1.3.2)<br>$$</p>
<blockquote>
<p>其中\(p_i = \frac{类别属于c_i的样本数}{总样本数}\)。如果所有的样本类别相同，则\(p_1 = 1, p_2 = p_3 = \cdots = p_k = 0\)，则有\( Gini(C)=0\)，此时数据不纯度最低。\(Gini(D)\)的物理含义是表示数据集\(D\)的不确定性。数值越大，表明其不确定性越大（这一点与信息熵相似）。</p>
<p>如果\(k=2\)（二分类问题，类别命名为正类和负类），若样本属于正类的概率是\(p\)，那么对应基尼指数为：</p>
<p>$$<br>Gini(D) = 2 p (1-p) \qquad\qquad (n.ml.1.3.6)<br>$$</p>
</blockquote>
<p>如果数据集\(D\)根据特征\(f\)是否取某一可能值\(f_{\ast}\)，将\(D\)划分为\(D_1\)和\(D_2\)两部分，即\(D_1=\{(x, y) \in D | f(x) = f_{\ast}\}, D_2=D-D_1\)。那么特征\(f\)在数据集\(D\)基尼指数定义为：</p>
<p>$$<br>Gini(D, f=f_{\ast}) = \frac{\vert D_1 \vert}{\vert D \vert} Gini(D_1) + \frac{\vert D_2 \vert}{\vert D \vert} Gini(D_2) \qquad\qquad (ml.1.3.3)<br>$$</p>
<p>在实际操作中，<strong>通过遍历所有特征（如果是连续值，需做离散化）及其取值</strong>，选择基尼指数最小所对应的特征和特征值。</p>
<p>这里仍然以天气数据为例，给出特征“阴晴”的基尼指数计算过程。</p>
<blockquote>
<p>(1). 当特征“阴晴”取值为”sunny”时，\(D_1 = \{1,2,8,9,11\}, |D_1|=5\); \(D_2=\{3,4,5,6,7,10,12,13,14\}, |D_2|=9\)。\(D_1、D_2\)数据自己对应的类别数分别为\((+2,-3)、(+7,-2)\)。因此\(Gini(D_1) = 2 \cdot \frac{3}{5} \cdot \frac{2}{5} = \frac{12}{25} \)；\(Gini(D_2) = 2 \cdot \frac{7}{9} \cdot \frac{2}{9} = \frac{28}{81}\). 对应的基尼指数为：</p>
<p>$$<br>Gini(C, “阴晴”=”sunny”) = \frac{5}{14} Gini(D_1) + \frac{9}{14} Gini(D_2) = \frac{5}{14} \frac{12}{25} + \frac{9}{14} \frac{28}{81} = 0.394 \quad(exp.1.3.5)<br>$$</p>
<p>(2). 当特征“阴晴”取值为”overcast”时，\(D_1 = \{2,7,12,13\}, |D_1|=4\); \(D_2=\{1,2,4,5,6,8,9,10,11,14\}, |D_2|=10\)。\(D_1、D_2\)数据自己对应的类别数分别为\((+4,-0)、(+5,-5)\)。因此\(Gini(D_1) = 2 \cdot 1 \cdot 0 = 0；Gini(D_2) = 2 \cdot \frac{5}{10} \cdot \frac{5}{10} = \frac{1}{2}\). 对应的基尼指数为：</p>
<p>$$<br>Gini(C, “阴晴”=”sunny”) = \frac{4}{14} Gini(D_1) + \frac{10}{14} Gini(D_2) = 0 + \frac{10}{14} \cdot \frac{1}{2} = \frac{5}{14} = 0.357 \quad(exp.1.3.6)<br>$$</p>
<p>(3). 当特征“阴晴”取值为”rainy”时，\(D_1 = \{4,5,6,10,14\}, |D_1|=5\); \(D_2=\{1,2,3,7,8,9,11,12,13\}, |D_2|=9\)。\(D_1、D_2\)数据自己对应的类别数分别为\((+3,-2)、(+6,-3)\)。因此\(Gini(D_1) = 2 \cdot \frac{3}{5} \cdot \frac{2}{5} = \frac{12}{25} \)；\(Gini(D_2) = 2 \cdot \frac{6}{9} \cdot \frac{3}{9} = \frac{4}{9}\). 对应的基尼指数为：</p>
<p>$$<br>Gini(C, “阴晴”=”sunny”) = \frac{5}{14} Gini(D_1) + \frac{9}{14} Gini(D_2) = \frac{5}{14} \frac{12}{25} + \frac{9}{14} \frac{4}{9} = \frac{4}{7} = 0.457 \quad(exp.1.3.7)<br>$$</p>
</blockquote>
<p>如果特征”阴晴”是最优特征的话，那么特征取值为”overcast”应作为划分节点。</p>
<p><br></p>
<h4 id="二叉回归树">二叉回归树</h4><hr>
<p>二叉回归树采用<strong>平方误差最小化</strong>作为特征选择和切分点选择的依据。一棵回归树对应着特征空间的若干个划分及其在划分单元上的输出值。假设将特征空间划分为\(J\)个单元（子空间），分别是\(\{R_1, R_2, \cdots, R_J\}\)，在每个单元\(R_j\)（对应回归树的一个叶子节点）上有一个固定的输出值\(v_j\)（连续变量）。给定训练数据集\(D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})\}\)，二叉回归树模型可表示为：</p>
<p>$$<br>f(x) = \sum_{j=1}^{J} v_j \cdot I(x \in R_j) \qquad (ml.1.3.4)<br>$$</p>
<blockquote>
<p>\(I(x \in R_j)\)为指示函数，如果输入样本\(x\)属于 \(R_j\)字空间，指示函数为1，对应的输出结果为\(v_j\)。<strong>每个输入\(x\)仅对应一个划分单元（字空间）</strong>。</p>
</blockquote>
<p>当特征空间的划分确定时，用平方误差来表示二叉回归树模型对于训练数据的预测误差，即表示如下：</p>
<p>$$<br>\hat{v_j} := \arg \min \sum_{x^{(i)} \in R_j} \left(y^{(i)} - f(x^{(i)}) \right)^2 \qquad(ml.1.3.5)<br>$$</p>
<p>每个单元的最优输出值可通过最小化平方误差求得。易知，划分单元\(R_j\)上的\(v_j\)的最优值\(\hat{v_j}\)是\(R_j\)上的所有输入样本\(x^{(i)}\)对应的输出\(y^{(i)}\)的均值，即\(\hat{c_j} = avg(y^{(i)} | x^{(i)} \in R_j)\)。</p>
<p>下面要解决的问题是：如何划分特征空间？</p>
<p>一个启发式的方式就是选择特征空间中第\(k\)个特征\(f_k\)和它的取值\(s\),作为划分特征和划分点。定义两个区域（对应内部节点两个分支）：</p>
<blockquote>
<p>$$<br>R_1(k, s) = \{x \,|\, f_k \le s\}, \quad R_2(k, s) = \{x \,|\, f_k \gt s\} \qquad(n.ml.1.3.7)<br>$$</p>
</blockquote>
<p>然后寻找最优划分特征\(f_k\)和最优划分点\(s\)。具体操作就是<strong>遍历所有未划分过的特征集合和对应的取值（集合）</strong>，求解：</p>
<p>$$<br>\min_{f_k, \, s} \left[ \min_{v_1} \sum_{x^{(i)} \in R_1(f_k, \, s)} (y^{(i)} - v_1)^2 + \min_{v_2} \sum_{x^{(i)} \in R_2(f_k, \, s)} (y^{(i)} - v_2)^2\right] \qquad(ml.1.3.6)<br>$$</p>
<p>第1步：固定特征\(f_k\)，最小化<code>[ ... ]</code>里的式子可得最优划分点\(s\)。即：</p>
<blockquote>
<p>$$<br>\hat{c_1} = avg (y^{(i)} \,|\, x^{(i)} \in R_1(f_k, s)), \quad \hat{c_2} = avg (y^{(i)} \,|\, x^{(i)} \in R_2(f_k, s)) \qquad(n.ml.1.3.8)<br>$$</p>
</blockquote>
<p>第1步：遍历所有特征集合，寻找最优的划分特征，构成\((f_k, s)\)对。这样就可生成一个内部节点，对应的特征\(f_k\)和划分值\(s\)。依此将特征空间划分为两个区域（对应两个数据子集）。</p>
<p>对每个子区域重复1、2两步的划分过程，直到满足划分停止条件为止（一般的，特征集合\(F = \emptyset\)时）。如此就得到一颗回归树，亦称为最小二乘回归树（Least Squares Regression Tree）。</p>
<p><br></p>
<h3 id="NEXT_\(\cdots\)">NEXT \(\cdots\)</h3><hr>
<ul>
<li>决策树解读</li>
<li>随机森林</li>
</ul>
<p><br></p>
<h3 id="参考资料">参考资料</h3><hr>
<ul>
<li>《机器学习导论》</li>
<li>《机器学习》－周志华</li>
<li>《统计学习方法》</li>
<li>《数据挖掘－实用机器学习技术》</li>
</ul>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/C4-5/">C4.5</a><a href="/tags/CART/">CART</a><a href="/tags/Decision-Stump/">Decision Stump</a><a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/ID3/">ID3</a><a href="/tags/IG/">IG</a><a href="/tags/Random-Forest/">Random Forest</a><a href="/tags/信息增益/">信息增益</a><a href="/tags/决策树/">决策树</a><a href="/tags/决策树桩/">决策树桩</a><a href="/tags/分类与回归树/">分类与回归树</a><a href="/tags/随机森林/">随机森林</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter2-entropy-based-family/" title="第02章：深入浅出ML之Entropy-Based家族" itemprop="url">第02章：深入浅出ML之Entropy-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-09-05T13:45:02.000Z" itemprop="datePublished"> 发表于 2015-09-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-08-04</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>基本概念<ul>
<li>熵与信息熵</li>
<li>条件熵</li>
<li>联合熵</li>
<li>相对熵、KL距离</li>
<li>互信息</li>
</ul>
</li>
<li>最大熵模型（Maximum Entropy Model） <ul>
<li>最大熵原理</li>
<li>最大熵模型定义</li>
<li>最大熵模型参数学习</li>
<li>对偶函数极大化与极大似然估计等价</li>
</ul>
</li>
<li>参数学习的最优化算法</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><p>记得在《Pattern Recognition And Machine Learning》一书中的开头有讲到：“概率论、决策论、信息论3个重要工具贯穿着《PRML》整本书，虽然看起来令人生畏…”。确实如此，其实这3大理论在机器学习的每一种技法中，或多或少都会出现其身影（不局限在概率模型）。</p>
<blockquote>
<p>《PRML》书中原话：”This chapter also provides a self-contained introduction to <strong>three important tools that will be used throughout the book, namely probability theory, decision theory, and information theory</strong>. Although these might sound like daunting topics, they are in fact straightforward, and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications.”</p>
<p>怀念好学生时代：<a href="http://photo.weibo.com/1707438033/albums/detail/album_id/3942924873266495#!/mode/2/page/1" target="_blank" rel="external">那些年－书本啃过的印记</a></p>
</blockquote>
<p>本章主要讨论《信息论》(Information Theory)中一个非常重要的概念：<strong>信息熵</strong>，以及概率模型的一个学习准则：<strong>最大熵理论</strong>。</p>
<p><br></p>
<h3 id="基本概念">基本概念</h3><p><br></p>
<h4 id="熵与信息熵">熵与信息熵</h4><ul>
<li><p>如何理解熵的含义？</p>
<p>  自然界的事物，如果任其自身发展，最终都会达到尽可能的平衡或互补状态。举例：</p>
<blockquote>
<p>一盒火柴，（人为或外力）有序地将其摆放在一个小盒子里，如果不小心火柴盒打翻了，火柴会“散乱”地洒在地板上。此时火柴虽然很乱，但这是它自身发展的结果。</p>
</blockquote>
<p>  上面描述的其实是自然界的熵。在自然界中，熵可以这样表述：</p>
<p>  <strong>熵是描述事物无序性的参数，熵越大则无序性越强。</strong></p>
<p>  那么，在信息论中，我们用熵表示一个随机变量的不确定性，那么如何量化信息的不确定性呢？</p>
</li>
<li><p>信息熵公式定义</p>
<p>  设一次随机事件（用随机变量\(X\)表示），它可能会有\(x_1, x_2, x_3, \cdots ,x_m\)共\(m\)个不同的结果，每个结果出现的概率分别为\(p_1, p_2, p_3, \cdots, p_m\)，那么\(X\)的不确定度，即信息熵为：</p>
<p>  $$<br>H(X) =\sum_{i=1}^{m} p_i \cdot \log_{2} \frac{1}{p_i} = - \sum_{i=1}^{m} p_i \cdot \log_{2} p_i         \qquad (ml.1.2.1)<br>  $$</p>
<blockquote>
<p>①. 信息熵的物理意义：<br><br>一个事件（用随机变量\(X\)表示）可能的变化越多，那么它携带的信息量就越大（与变量具体取值无关，只跟值的种类多少以及发生概率有关）。</p>
<p>②. 系统熵举例：<br><br>对于一个分类系统来说，假设类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别总数），每一个类别出现的概率分别是\(p(c_1),p(c_2), \cdots, p(c_k)\)。此时，分类系统的熵可以表示为:</p>
<p>  $$<br>  H(C) = - \sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) \qquad (n.ml.1.2.1)<br>  $$</p>
<p>分类系统的作用就是输出一个特征向量（文本特征、ID特征、属性特征等）属于哪个类别的值，而这个值可能是\(c_1, c_2, \cdots, c_k\)，因此这个值所携带的信息量就是公式\((n.ml.1.2.1)\)这么多。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="条件熵">条件熵</h4><p>设\(X,Y\)为两个随机变量，在\(X\)发生的前提下，\(Y\)发生所新带来的熵 定义为\(Y\)的条件熵（Conditional Entropy），用\(H(Y|X)\)表示，计算公式如下：</p>
<p>$$<br>H(Y|X) = - \sum_{x_i,y_j}^{m,n} p(x_i,y_j) \cdot log_2 p(y_j|x_i) \qquad(ml.1.2.2)<br>$$</p>
<p>其物理含义是当变量\(X\)已知时，变量\(Y\)的平均不确定性是多少。公式\((ml.1.2.2)\)推导如下：</p>
<blockquote>
<p>假设变量\(X\)取值有\(m\)个，那么\(H(Y|X=x_i)\)是指变量\(X\)被固定为值\(x_i\)时的条件熵；\(H(Y|X)\)时指变量\(X\)被固定时的条件熵。那么二者之间的关系时：</p>
<p>$$<br>\begin{align}<br>H(Y|X) &amp; = p(x_1) \cdot H(Y|X=x_1) +  \cdots + p(x_m) \cdot H(Y|X=x_m) \\<br>&amp; = \sum_{i=1}^{m} p(x_i) \cdot H(Y|X=x_i)<br>\end{align}    \quad(n.ml.1.2.2)<br>$$</p>
<p>根据公式\((n.ml.1.2.2)\)继续推导\(Y\)的条件熵：</p>
<p>$$<br>\begin{align}<br>H(Y|X) &amp; = \sum_{i=1}^{m} p(x_i) \cdot H(Y|X=x_i) \\<br>&amp; = -\sum_{i=1}^{m} p(x_i) \cdot \left( \sum_{j=i}^{n} p(y_j|x_i) \cdot log_2 p(y_j|x_i) \right) \\<br>&amp; = -\sum_{i=1}^{m} \sum_{j=1}^{n} p(y_j,x_i) \cdot log_2 p(y_j|x_i) \\<br>&amp; = - \sum_{x_i,y_j}^{m,n} p(x_i,y_j) \cdot log_2 p(y_j|x_i)<br>\end{align}    \qquad\qquad (n.ml.1.2.3)<br>$$</p>
<p>注：<strong>条件熵里面是联合概率分布累加</strong>，公式\((n.ml.1.2.3)\)推导过程可参考《第3章：深入浅出ML之Based-Tree Classification Family》中3.1.2节条件熵部分。</p>
</blockquote>
<p><br></p>
<h4 id="联合熵">联合熵</h4><p>一个随机变量的不确定性可以用熵来表示，这一概念可以直接推广到多个随机变量。</p>
<ul>
<li><p>联合熵计算（Joint Entropy）</p>
<p>  设\(X,Y\)为两个随机变量，\(p(x_i,y_j)\)表示其联合概率，用\(H(XY)\)表示联合熵，计算公式为：</p>
<p>  $$<br>  H(XY) = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_{2} p(x_i,y_j) \qquad(ml.1.2.3)<br>  $$</p>
<blockquote>
<p>条件熵、联合熵、熵之间的关系：</p>
<p>  $$<br>  H(Y|X) = H(X,Y) - H(X)         \qquad\qquad(n.ml.1.2.4)<br>  $$</p>
<p>公式推导如下：</p>
<p>$$<br>\begin{align}<br>H(X,Y) - H(X) &amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 p(x_i,y_j) + \sum_{i=1}^{m} \underline{p(x_i)} \cdot log_2 p(x_i) \\<br>&amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 p(x_i,y_j) + \sum_{i=1}^{m} \underline{ \left( \sum_{j=1}^{n} p(x_i,y_j) \right) } \cdot log_2 p(x_i) \\<br>&amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot \left(log_2 p(x_i,y_j) - log_2 p(x_i) \right) \\<br>&amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 p(y_j|x_i) \\<br>&amp; = H(Y|X)    \qquad\qquad\qquad\qquad\qquad\qquad (n.ml.1.2.5)<br>\end{align}<br>$$</p>
</blockquote>
</li>
<li><p>联合熵特点</p>
<ul>
<li>\(H(XY) \geq H(X)\)<ul>
<li>联合系统的熵不小于子系统的熵，即增加一个新系统不会减少不确定性。</li>
</ul>
</li>
<li>\(H(XY) \leq H(X)+H(Y)\)<ul>
<li>子系统可加性 </li>
</ul>
</li>
<li>\(H(XY) \geq 0\): 非负性。</li>
</ul>
</li>
</ul>
<p><br>    </p>
<h4 id="相对熵、KL距离">相对熵、KL距离</h4><ul>
<li><p>相对熵概念</p>
<p>  相对熵，又称为交叉熵或<strong>KL距离</strong>，是Kullback-Leibler散度（Kullback-Leibler Divergence）的简称。它主要用于衡量<strong>相同事件空间里</strong>的两个概率分布的差异。简单介绍其背景：</p>
<blockquote>
<p>根据香农的信息论，给定一个字符集的概率分布，我们可以设计一种编码，使得表示该字符集组成的（每个）字符串平均需要的比特数最少（比如Huffman编码）。假设字符集是\(X\)，对\(x \in X\)，其出现概率为\(P(x)\)，那么其最优编码平均需要的比特数（即每一个字符需要的比特数）等于这个字符集的熵（公式见\((ml.1.2.1)\)），即最优编码时，字符\(x\)的编码长度等于\(log_2{\frac{1}{P(x)}}\)。</p>
<p>在同样的字符集上，假设存在另一个概率分布\(Q(x)\)。如果根据\(Q(x)\)分布进行编码，那么表示这些字符就会比理想情况多用一些比特数。而<strong>KL距离</strong>就是用来衡量这种情况下平均每个字符多用的比特数，可用来度量两个分布的距离。</p>
</blockquote>
</li>
<li><p>KL距离计算公式</p>
<p>  这里用\(D(P||Q)\)表示KL距离，计算公式如下：</p>
<p>  $$<br>  D(P||Q) = \sum_{x \in X} P(x) \cdot log_2 \frac{P(x)}{Q(x)}  \qquad\qquad(ml.1.2.4)<br>  $$</p>
<p>  从公式\((ml.1.2.4)\)可以看出，当两个概率分布完全相同时，KL距离为0。概率分布\(P(x)\)的信息熵如公式\((ml.1.2.1)\)所示，说的是如果按照概率分布\(P(x)\)编码时，描述这个随机事件至少需要多少比特编码。</p>
<p>  因此，KL距离的物理意义可以这样表达：</p>
<blockquote>
<p>在相同的事件空间里，概率分布为\(P(x)\)的事件空间，若用概率分布\(Q(x)\)编码时，平均每个基本事件（符号）编码长度<strong>增加了</strong>多少比特数。</p>
</blockquote>
<p>  通过信息熵可知，不存在其它比按照随机事件本身概率分布更好的编码方式了，所以<strong>\(D(P||Q)\)始终是大于等于0的</strong>。</p>
<blockquote>
<p>虽然KL被称为距离，但是其不满足距离定义的3个条件：1) 非负性；2) 对称性(不满足)；3) 三角不等式(不满足)。</p>
</blockquote>
</li>
<li><p>KL距离示例</p>
<blockquote>
<p>假设有一个字符发射器，随机发出0和1两种字符，真实发出的概率分布为\(A\)。现在通过样本观察，得到概率分布\(B\)和\(C\)。各个分布的具体情况如下：</p>
<p>(1). \(A(0) = 1/2, A(1) = 1/2\); <br><br>(2). \(B(0) = 1/4, B(1) = 3/4\); <br><br>(3). \(C(0) = 1/8, C(1) = 7/8\); <br></p>
<p>那么可以计算出相对熵如下：<br></p>
<p>\(D(A||B) = 1/2 \cdot log_2 (\frac{1/2}{1/4}) + 1/2 \cdot log_2 (\frac{1/2}{3/4}) = 1/2 \cdot log_2 (4/3)\)  <br></p>
<p>\(D(A||C) = 1/2 \cdot log_2 (\frac{1/2}{1/8}) + 1/2 \cdot log_2 (\frac{1/2}{7/8}) = 1/2 \cdot log_2 (16/7)\)</p>
<p>可以看到，用\(B和C\)两种方式进行编码，其结果都是的平均编码长度增加了。同时也能发现，按照概率分布\(B\)进行编码，要比按照\(C\)进行编码，平均每个符号增加的比特数目要少。从分布熵也可以看出，实际上\(B\)要比\(C\)更接近实际分布。<br><br><br>如果实际分布为\(C\)，而用\(A\)分布来编码这个字符发射器的每个字符，同样可以得到：</p>
<p>\(D(C||A) = 1/8 \cdot log_2 (\frac{1/8}{1/2}) + 7/8 \cdot log_2 (\frac{7/8}{1/2}) = 7/8 \log_2{7} - 2 &gt; 0\)</p>
</blockquote>
<p> 从示例中，我们可以得出结论：<strong>对于一个信息源进行编码，按照其本身的概率分布进行编码，每个字符的平均比特数最少。</strong> 这也是信息熵的概念，用于衡量信息源本身的不确定性。</p>
<p> 此外可以看出，KL距离不满足对称性，即\(D(P||Q)\)不一定等于\(D(Q||P)\)。</p>
</li>
<li><p>相对熵应用场景</p>
<ul>
<li><p>推荐系统－物品之间相似度</p>
<blockquote>
<p>在使用LDA(Latent Dirichlet Allocation)<strong>计算物品之间的内容相似度</strong>时，我们可以先计算出物品在Topic上的分布，然后利用两个物品的Topic（话题）分布计算物品的相似度。比如，如果两个物品的Topic分布相似（<strong>处在同一个事件空间</strong>），则认为两个物品具有较高的相似度，反之则认为两个物品的相似度较低。<br><br><br>这种Topic分布的相似度可以利用KL散度来计算：</p>
<p>   $$<br>   D(P||Q) = \sum_{i \in X} p(x_i) \cdot log_2 {\frac{p(x_i)}{q(x_i)}}  \qquad(n.ml.1.2.6)<br>   $$</p>
<p>其中\(p\)和\(q\)是两个分布，\(X\)为话题集合，\(x_i\)表示第\(i\)个话题。<strong>KL散度越大说明分布的相似度越低</strong>。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="互信息">互信息</h4><p>如果说相对熵（KL）距离衡量的是相同事件空间里的两个事件的相似度大小，那么，互信息通常用来衡量不同事件空间里的两个信息（随机事件、变量）的相关性大小。</p>
<ul>
<li><p>互信息计算公式</p>
<p>  设\(X\)和\(Y\)为两个离散随机变量，事件\(Y=y_j\)的出现对于事件\(X=x_i\)的出现的互信息量\(I(x_i,y_j)\)定义为：</p>
<p>  $$<br>  I(x_i;y_j) = log_2 {\frac{p(x_i|y_j)}{p(x_i)}} = log_2 {\frac {p(x_i,y_j)}{p(x_i)p(y_j)}} \qquad(ml.1.2.5)<br>  $$</p>
<p>  对于事件\(X\)和\(Y\)来说，它们之间的互信息用\(I(X;Y)\)表示，公式为：</p>
<p>  $$<br>  I(X;Y) = \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 {\frac{p(x_i,y_j)}{p(x_i)p(y_j)}}     \qquad(ml.1.2.6)<br>  $$</p>
<blockquote>
<p>公式解释：<br><br><br>互信息就是随机事件\(X\)的不确定性（即熵\(H(X)\)），以及在给定随机变量\(Y\)条件下的不确定性（即条件熵\(H(X|Y)\)）<strong>之间的差异</strong>，即</p>
<p>  $$<br>  I(X;Y) = H(X) - H(X|Y) \qquad(n.ml.1.2.7)<br>  $$</p>
<p><strong>互信息与决策树中的信息增益等价: 互信息  \(\Longleftrightarrow\) 信息增益.</strong></p>
</blockquote>
<p>  所谓两个事件相关性的量化度量，就是在了解了其中一个事件\(Y\)的前提下，对消除另一个事件\(X\)不确定性所提供的信息量。</p>
</li>
<li><p>互信息与其它熵之间的关系</p>
<ul>
<li>\(H(X|Y) = H(X,Y) - H(Y)\) </li>
<li>\(I(X;Y) = H(X) + H(Y) - H(X,Y)\) </li>
<li>\(I(X;Y) = H(X) - H(X|Y)\)</li>
<li>\(I(X;X) = H(X)\)</li>
</ul>
</li>
<li>互信息应用场景<ul>
<li>机器学习－<code>&lt;feature，label&gt;</code>之间相关性<ul>
<li>计算随机事件之间（不同的事件空间）的<strong>相关性</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p><br></p>
<h3 id="最大熵模型（Maximum_Entropy_Model）">最大熵模型（Maximum Entropy Model）</h3><p><br></p>
<h4 id="最大熵原理">最大熵原理</h4><p>在介绍最大熵模型之前，我们先了解一下最大熵原理，因为<strong>最大熵原理是选择最优概率模型的一个准则</strong>。</p>
<ul>
<li><strong>最大熵原理</strong></li>
</ul>
<table>
<thead>
<tr>
<th><strong>在概率模型空间集合中，在满足给定约束条件的前提下，使信息熵最大化得到的概率模型，就是最优的模型。</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>通常用约束条件来确定概率模型的集合。</p>
</blockquote>
<ul>
<li><p><strong>理解最大熵原理</strong></p>
<p>  假设离散随机变量\(X\)的概率分布是\(P(X)\)，其信息熵可用公式\((ml.1.2.1)\) 表示，并且熵满足以下不等式：</p>
<p>  $$<br>  0 \leq H(X) \leq log_2 |X|         \qquad\quad(ml.1.2.7)<br>  $$</p>
<p>  其中，\(|X|\)是\(X\)的取值个数，当且仅当\(X\)的分布是均匀分布时右边的等号才成立。也就是说，当\(X\)服从均匀分布时，熵最大。</p>
<blockquote>
<p>根据最大熵原理学习概率模型坚持的原则：<strong>首先必须满足已有的事实，即约束条件；但对不确定的部分不做任何假设，坚持无偏原则</strong>。最大熵原理通过熵的最大化来表示等可能性。</p>
</blockquote>
</li>
<li><p><strong>最大熵原理举例</strong>（本示例来自《统计学习方法》第6章－李航老师）</p>
<blockquote>
<p>问题：假设随机变量\(X\)有5个取值\(\{A,B,C,D,E\}\), 要估计各个取值的概率\(P(A),P(B),P(C),P(D),P(E)\)。</p>
<p>首先这些概率只满足以下约束条件：</p>
<p>  $$<br>  P(A) + P(B) + P(C) + P(D) + P(E) = 1 \qquad(exp.ml.1.2.1)<br>  $$</p>
<p>满足这个约束条件的概率分布有无穷多个，但是在没有任何其它信息的情况下，根据最大熵原理和无偏原则，选择熵最大时对应的概率分布，即各个取值概率相等是一个不错的概率估计方法。即有：</p>
<p>  $$<br>  P(A) = P(B) = P(C) = P(D) = P(E) = \frac{1}{5} \qquad(exp.ml.1.2.2)<br>  $$</p>
<p>等概率坚持了最大熵的无偏原则，因为没有更多信息，此种判断是合理的。</p>
<p>现在从先验知识中得到一些<code>信息</code>：\(A和B\)的概率值之和满足以下条件：</p>
<p>  $$<br>  P(A) + P(B) = \frac{3}{10}        \qquad(exp.ml.1.2.3)<br>  $$</p>
<p>  同样的，满足公式\((exp.ml.1.2.1)和(exp.ml.1.2.3)\)两个约束条件的概率分布仍有无穷多个。在缺少其它信息的情况下，坚持无偏原则，得到：</p>
<p>  $$<br>  \begin{align}<br>  P(A) = P(B) = \frac{3}{20} \qquad (exp.ml.1.2.4) \\<br>  P(C) = P(D) = P(E) = \frac{7}{30} \qquad (exp.ml.1.2.5)<br>  \end{align}<br>  $$</p>
<p>…</p>
<p>还可以继续按照满足约束条件下的求等概率的方法估计概率分布。以上概率模型学习的方法正是遵循了最大熵原理。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="最大熵模型定义">最大熵模型定义</h4><p>最大熵原理是统计学习的一般原理，将它应用到分类问题中，即得到最大熵模型。</p>
<ul>
<li><p>最大熵模型引入</p>
<p>  训练数据集：\(D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(N)},y^{(N)})\}\)，学习的目标是：<strong>用最大熵原理选择最优的分类模型。</strong></p>
<blockquote>
<p>假设分类模型是一个条件概率分布\(P(y|x), x \in X \subseteq R^n\)表示输入（特征向量），\(y \in Y\), \(X\)和\(Y\)分别是输入（特征向量）和输出（标签）的集合。这个模型表示的是对于给定的输入\(x\)，以<strong>条件概率</strong>\(P(y|x)\)计算得到标签\(y\)。</p>
</blockquote>
<ul>
<li><p>首先，考虑模型应满足的条件</p>
<p>  给定训练集，可以计算得到经验联合分布\(P(x,y)\)和边缘分布\(P(x)\)的经验分布，分别以\(\tilde{P}(x,y)\)和\(\tilde{P}(x)\)表示，即：</p>
<p>  $$<br>  \begin{align}<br>  \tilde{P}(x=\tilde{x}, y = \tilde{y}) &amp;= \frac{freq(x=\tilde{x}, y = \tilde{y})}{N} \qquad(1)\\<br>  \tilde{P}(x=\tilde{x}) &amp;= \frac{freq(x=\tilde{x})}{N} \qquad\qquad\;(2)<br>  \end{align}  \qquad(ml.1.2.8)<br>  $$</p>
<p>  其中，\(freq(x=\tilde{x}, y=\tilde{y})\)表示训练集中样本\((\tilde{x}, \tilde{y})\)出现的频数，\(freq(\tilde{x})\)表示训练集中输入\(\tilde{x}\)（向量）出现的频数，\(N\)表示训练集容量。</p>
</li>
<li><p>特征函数（Feature Function）</p>
<p>  定义<strong>特征函数</strong> \(f(x,y)\)用于描述输入\(x\)和输出\(y\)之间满足的某一种事实：</p>
<p>  $$<br>  f(x,y) = \begin{cases}<br>  \displaystyle 1, &amp;x与y满足某一事实; \\<br>  0, &amp; 其它<br>  \end{cases}     \qquad\qquad(ml.1.2.9)<br>  $$</p>
<p>  这是一个二值函数（也可以是任意实值函数），当\(x\)与\(y\)满足这个事实时取值为1，否则为0.</p>
<blockquote>
<p>①. 特征函数\(f(x,y)\)关于经验分布\(\tilde{P}(x,y)\)的期望值，用\(E_{\tilde{P}}(f)\)表示如下：</p>
<p>  $$<br>  E_{\tilde{P}} = \sum_{x,y} \tilde{P}(x,y) \cdot f(x,y)    \qquad\qquad(n.ml.1.2.8)<br>  $$</p>
<p>②. 特征函数\(f(x,y)\)关于模型\(P(y|x)\)与经验分布\(\tilde{P}(x)\)的期望值，用\(E_P(f)\)表示如下：</p>
<p>  $$<br>  E_P(f) = \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot f(x,y)    \qquad(n.ml.1.2.9)<br>  $$</p>
<p>③. 如果模型能够获取训练数据中足够的信息，那么就可以假设这两个期望值相等。即：</p>
<p>  $$<br>  \sum_{x,y} \tilde{P}(x,y) \cdot f(x,y) ＝ \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot f(x,y)    \qquad(n.ml.1.2.10)<br>  $$</p>
<p>注：公式\((n.ml.1.2.10)\)是频率学派－点估计求参数套路，之所以假设相等，是因为有\(p(x,y)=p(y|x) \cdot p(x)\)</p>
</blockquote>
<p>  我们将公式\((n.ml.1.2.10)\)作为概率模型学习的约束条件。假如有\(n\)个特征函数\(f_{i} (x,y), i=1,2, \cdots, n\)，那么就有\(n\)个约束条件。</p>
</li>
</ul>
</li>
<li><p>最大熵模型定义</p>
<p>  假设满足所有约束条件的模型集合为：</p>
<p>  $$<br>  \mathcal{C} = \{P \in \mathcal{P} | E_{P}(f_i) = E_{\tilde{P}}(f_i), i=1,2, \cdots, n\} \qquad (ml.1.2.10)<br>  $$ </p>
<p>  定义在条件概率分布\(P(y|x)\)上的条件熵为：</p>
<p>  $$<br>  H(P) = - \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot \log {P(y|x)}     \qquad (ml.1.2.11)<br>  $$</p>
<p>  模型集合\(\mathcal{C}\)中条件熵\(H(P)\)最大的模型称为最大熵模型。 </p>
<blockquote>
<p>注：<strong>最大熵模型中\(\log\)是指以\(e\)为底的对数</strong>，与信息熵公式中以2为底不同。本文如无特殊说明，\(\log\)均指自然对数。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="最大熵模型参数学习">最大熵模型参数学习</h4><p>最大熵模型学习过程即为求解最大熵模型的过程，<strong>最大熵模型的学习问题可以表示为带有约束的最优化问题</strong>。</p>
<ul>
<li><p>示例：学习《最大熵原理》示例中的最大熵模型</p>
<p>   为了简便，这里分别以\(y_1,y_2,y_3,y_4,y_5\)表示\(A,B,C,D和E\)，最大熵模型学习的最优化问题可以表示为：</p>
<p>   $$<br>   \begin{align}<br>   &amp; min \quad -H(P) = \sum_{i=1}^{5} P(y_i) \cdot log{P(y_i)} \\<br>   &amp; s.t. \quad P(y_1) + P(y_2) = \tilde{P}(y_1) + \tilde{P}(y_2) = \frac{3}{10} \\<br>   &amp; \qquad \sum_{i=1}^{5} P(y_i) = \sum_{i=1}^{5} \tilde{P}(y_i) = 1<br>   \end{align} \qquad\quad (exp.ml.1.2.5)<br>   $$</p>
<blockquote>
<p>提示：这里面没有特征\(x\)和特征函数\(f_i(x,y)\)的约束。</p>
</blockquote>
<p>   将带约束优化问题转化为无约束优化问题：引入拉格朗日乘子\(w_0,w_1\)，定义朗格朗日函数：</p>
<p>   $$<br>   L(P,w) = \sum_{i=1}^{5} P(y_i) log{P(y_i)} + w_1 \left( P(y_1) + P(y_2) - \frac{3}{10} \right) + w_0 \left(\sum_{i=1}^{5} P(y_i) - 1 \right) \;(exp.ml.1.2.6)<br>   $$</p>
<p>   根据拉格朗日对偶性，可以通过求解对偶最优化问题得到原始最优化问题的解，所以求解（对偶问题）：\(\max_{w} \min_{P} L(P,w) \)。求解过程如下：</p>
<blockquote>
<p>首先求解\(L(P,w)\)关于\(P\)的极小化问题。为此，固定\(w_0,w_1\)，求偏导数：</p>
<p>   $$<br>   \begin{align}<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_1)} = 1 + log_2 P(y_1) + w_1 + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_2)} = 1 + log_2 P(y_2) + w_1 + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_3)} = 1 + log_2 P(y_3) + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_4)} = 1 + log_2 P(y_4) + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_5)} = 1 + log_2 P(y_5) + w_0 \\<br>   \end{align}<br>   $$</p>
<p>令各偏导数等于0，可解得：</p>
<p>   $$<br>   \begin{align}<br>   &amp; P(y_1) = P(y_2) = e^{-w_1 - w_0 - 1} \\<br>   &amp; P(y_3) = P(y_4) = P(y_5) = e^{-w_0 -1}<br>   \end{align}<br>   $$</p>
<p>于是，极小化结果为：</p>
<p>   $$<br>   \min_{P} \; L(P,w) = L(P_w, w) = -2 e^{-w_1 - w_0 - 1} -3 e^{-w_0 - 1} - \frac{3}{10} w_1 - w_0<br>   $$</p>
<p>下面再求解对偶函数\(L(P_w,w)\)关于\(w\)的极大化问题：</p>
<p>   $$<br>   \max_{w} \; L(P_w, w) = -2 e^{-w_1 - w_0 - 1} -3 e^{-w_0 - 1} - \frac{3}{10} w_1 - w_0        \qquad(exp.ml.1.2.7)<br>   $$</p>
<p>分别求\(L(P_w,w)\)对\(w_0,w_1\)的偏导数，并令其为0，得到：</p>
<p>   $$<br>   \begin{align}<br>   &amp; e^{-w_1 - w_0 - 1} = \frac{3}{20} \\<br>   &amp; e^{-w_0 - 1} = \frac{7}{30}<br>   \end{align}<br>   $$</p>
<p>于是得到所求的概率分布为：</p>
<p>   $$<br>   \begin{align}<br>   &amp; P(y_1) = P(y_2) = \frac{3}{20} \\<br>   &amp; P(y_3) = P(y_4) = P(y_5) = \frac{7}{30}<br>   \end{align}<br>   $$</p>
</blockquote>
</li>
<li><p>最大熵模型学习一般流程</p>
<p>  对于给定的训练\(D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(N)},y^{(N)})\}\)以及特征函数\(f_i(x,y),i=1,2,\cdots,n\)，最大熵模型的学习等价于带约束的最优化问题：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{P \in \mathcal{C}} \quad H(P) = -\sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot log P(y|x) \\<br>  &amp; s.t. \quad E_P(f_i) = E_{\tilde{P}} (f_i), \; i=1,2,\cdots,n \\<br>  &amp; \qquad \sum_{y} P(y|x) = 1<br>  \end{align}        \qquad\quad(ml.1.2.12)<br>  $$</p>
<p>  按照最优化问题的习惯思路，将求最大值问题改写为求等价的最小值问题，即：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \min_{P \in \mathcal{C}} \quad -H(P) = \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot log P(y|x) \\<br>  &amp; s.t. \quad E_P(f_i) - E_{\tilde{P}} (f_i) = 0, \; i=1,2,\cdots,n \\<br>  &amp; \qquad \sum_{y} P(y|x) = 1<br>  \end{align}        \qquad\quad(ml.1.2.13)<br>  $$</p>
<p>  求解约束最优化问题\((ml.1.2.13)\)所得出的解，就是最大熵模型学习的解。</p>
<p>  <strong>将约束最优化的原始问题转换为无约束最优化的对偶问题</strong>。具体推导过程如下：</p>
<ul>
<li><p><strong>首先，引入拉格朗日乘子\(w_0,w_1,\cdots,w_n\)，定义拉格朗日函数\(L(P,w)\)</strong></p>
<p>  表达式为：</p>
<p>  $$<br>\begin{align}<br>L(P,w) &amp; = -H(P) + w_0 \cdot \left( 1- \sum_{y} P(y|x) \right) + \sum_{i=1}^{n} w_i \cdot \left( E_{\tilde{P}}(f_i) - E_P (f_i) \right) \\<br>&amp; = \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot log {P(y|x)} + w_0 \cdot \left( 1- \sum_{y} P(y|x) \right) \\<br>&amp; \qquad + \sum_{i=1}^{n} w_i \cdot \left(\sum_{x,y} \tilde{P}(x,y) \cdot f_i(x,y) -  \sum_{x.y} \tilde{P}(x) \cdot P(y|x) \cdot f_i(x,y) \right)<br>\end{align} \quad(ml.1.2.14)<br>$$</p>
<p>  最优化的原始问题是：</p>
<p>  $$<br>\min_{P \in \mathcal{C}} \max_{w} L(P,w) \qquad\qquad(ml.1.2.15)<br>  $$</p>
<p>  对偶问题是：</p>
<p>  $$<br>  \max_{w} \min_{P \in \mathcal{C}} L(P,w) \qquad\qquad(ml.1.2.16)<br>  $$</p>
<blockquote>
<p>通俗的讲，由_最小最大问题_转化为_最大最小问题_。</p>
</blockquote>
<p>  <strong>由于最大熵模型对应的朗格朗日函数\(L(P,w)\)是参数\(P\)的凸函数，所以原始问题\((ml.1.2.15)\)的解与对偶问题\((ml.1.2.16)\)的解是等价的</strong>。因此，可以通过求解对偶问题来得到原始问题的解。</p>
</li>
<li><p><strong>其次，求对偶问题\((ml.1.2.16)\)内部的极小化问题\(\min_{P \in \mathcal{C}} L(P,w)\)</strong></p>
<p>  \(\min_{P \in \mathcal{C}} L(P,w)\)是乘子\(w\)的函数，将其记作：</p>
<p>  $$<br>\Psi(w) = \min_{P \in \mathcal{C}} L(P,w) = L(P_w, w) \qquad(ml.1.2.17)<br>$$</p>
<blockquote>
<p>\(\Psi(w)\)称为对偶函数（\(Latex: \Psi\) = <code>\Psi</code>）。将其解记作：</p>
<p>  $$<br>  P_w = arg \min_{P \in \mathcal{C}} L(P,w) = P_w (y|x) \qquad(n.ml.1.2.11)<br>  $$</p>
<p>具体地，固定\(w_i\)，求\(L(P,w)\)对\(P(y|x)\)的偏导数：</p>
<p>  $$<br>  \begin{align}<br>\frac{\partial L(P,w)} {\partial P(y|x)} &amp; = \sum_{x,y} \tilde{P}(x) \cdot \left(logP(y|x) + 1 \right) - \sum_{y} w_0 - \sum_{x,y} \left( \tilde{P}(x) \cdot \sum_{i=1}^{n} w_i \cdot f_i(x,y) \right) \\<br>&amp; = \sum_{x,y} \tilde{P}(x) \cdot \left(logP(y|x) + 1 - w_0 - \sum_{i=1}^{n} w_i \cdot f_i(x,y) \right)     \qquad(n.ml.1.2.12)<br>  \end{align}<br>  $$</p>
<p>令偏导数等于0，在\(\tilde{P}(x) &gt; 0\)的情况下，求得：</p>
<p>  $$<br>  P(y|x) = \exp {\left( \sum_{i=1}^{n} w_i \cdot f_i(x,y) + w_0 - 1 \right)} = \frac  {\exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y) \right)} {\exp(1-w_0)}  \quad(n.ml.1.2.13)<br>  $$</p>
<p>由于 \(\sum_{y} P(y|x) = 1\)，可得：</p>
<p>  $$<br>P_w (y|x) = \frac{1}{Z_w(x)} \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y) \right) \qquad\quad(n.ml.1.2.14)<br>  $$</p>
<p>其中，</p>
<p>  $$<br>Z_w(x) = \sum_{y} \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y) \right) \qquad\quad(n.ml.1.2.15)<br>  $$</p>
</blockquote>
<p>  \(Z_w(x)\)称为归一化因子；\(f_i(x,y)\)是特征函数；\(w_i\)是第\(i\)个参数（特征权值）。公式\((n.ml.1.2.14)\)、\((n.ml.1.2.15)\) 表示的模型\(P_w = P_w(y|x)\)就是最大熵模型（\(w\)是最大熵模型中的参数向量）。</p>
</li>
<li><p><strong>最后，求解对偶问题外部的极大化问题</strong></p>
<p>  对偶问题外部极大化表达式：</p>
<p>  $$<br>  \max_{w} \Psi(w)        \qquad\qquad(ml.1.2.18)<br>   $$    </p>
<p>   将其解记作\(w^@\)，即: \(w^@ = arg \max_{w} \Psi(w)\)。</p>
<p>   也就是说，可以应用最优化算法求对偶函数\(\Psi(w)\)的极大化，得到\(w^@\)，用其表示\(P^@ = P_{w^@} = P_{w^@}(y|x)\)是学习到的最优模型（最大熵模型）。</p>
<p>   <strong>最大熵模型的学习归结为对偶函数\(\Psi(w)\)的极大化。</strong></p>
</li>
</ul>
</li>
</ul>
<p><br>     </p>
<h4 id="对偶函数极大化与极大似然估计等价">对偶函数极大化与极大似然估计等价</h4><p>从最大熵模型的学习过程可以看出，最大熵模型是由\(n.ml.1.2.14\)和\(n.ml.1.2.15\)表示的条件概率分布。下面证明：<strong>对偶函数的极大化等价于最大熵模型的极大似然估计</strong>。</p>
<ul>
<li><p><strong><code>对偶函数极大化＝极大似然估计</code></strong></p>
<p>  已知训练数据的经验概率分布\(\tilde{P}(x,y)\)，条件概率分布分布\(P(y|x)\)的对数似然函数表示为：</p>
<p>  $$<br>  L_{\tilde{P}}(P_w) = \log \prod_{x,y} P(y|x)^{\tilde{P}(x,y)} = \sum_{x,y} \tilde{P}(x,y) \cdot \log P(y|x)  \qquad(ml.1.2.19)<br>  $$</p>
<p>  当<strong>条件概率分布\(P(y|x)\)是最大熵模型</strong>(公式\((n.ml.1.2.14)和n(.ml.1.2.15)\))时，对数似然函数\(L_{\tilde{P}}(P_w)\)为：</p>
<p>  $$<br>  \begin{align}<br>  L_{\tilde{P}}(P_w) &amp; = \sum_{x,y} \tilde{P}(x,y) \cdot \log P(y|x) \\<br>  &amp; = \sum_{x,y} \left (\tilde{P}(x,y) \cdot \sum_{i=1}^{n} w_i f_i(x,y)\right) - \sum_{x,y} \tilde{P}(x,y) \cdot log Z_w(x) \\<br>  &amp; = \sum_{x,y} \left (\tilde{P}(x,y) \cdot \sum_{i=1}^{n} w_i f_i(x,y)\right) - \sum_{x} \tilde{P}(x) \cdot log Z_w(x)<br>  \end{align} \quad(ml.1.2.20)<br>  $$</p>
<p>  再看对偶函数\(\Psi(w)\)，由公式\((ml.1.2.14)\)和公式\((ml.1.2.17)\)可得：</p>
<p>  $$<br>  \begin{align}<br>  \Psi(w) &amp; = \sum_{x,y} \tilde{P}(x) \cdot P_w(y|x) \cdot \log P_w(y|x) \\<br>  &amp; \qquad\quad + \sum_{i=1}^{n} w_i \cdot \left(\sum_{x,y} \tilde{P}(x,y) f_i(x,y) - \sum_{x,y} \tilde{P}(x) P_w(y|x)f_i(x,y) \right) \\<br>  &amp; = \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^{n} w_i f_i(x,y) + \sum_{x,y} \tilde{P}(x)P_w(y|x) \left(\underline{log P_w(y|x) - \sum_{i=1}^{n} w_i f_i (x,y)}\right) \\<br>  &amp; = \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^{n} w_i f_i(x,y) - \sum_{x,y} \tilde{P}(x) P_w(y|x) \cdot \underline{\log Z_w(x)} \\<br>  &amp; = \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^{n} w_i f_i(x,y) - \sum_{x} \tilde{P}(x) \log Z_w(x)<br>  \end{align} \quad(ml.1.2.21)<br>  $$</p>
<blockquote>
<p>其中， 第二步推导第三步中用到了:</p>
<p>  $$<br>  \sum_{i=1}^{n} w_i \cdot f_i(x,y) = \log P_w(y|x) \cdot Z_w(x) \qquad(n.ml.1.2.16)<br>  $$</p>
<p>根据公式\((n.ml.1.2.14)\)得到。在最后一步用到了\(\sum_{y} P(y|x) = 1\)的性质。即：</p>
<p>  $$<br>  \begin{align}<br>  \sum_{x,y} \tilde{P}(x) P_w(y|x) \log Z_w(x) &amp; = \sum_{x} \tilde{P}(x) \left( \sum_{y} P_w(y|x) \right) \log Z_w(x) \\<br>  &amp; = \sum_{x} \tilde{P}(x) \log Z_w(x)<br>  \end{align}    \qquad(n.ml.1.2.17)<br>  $$</p>
</blockquote>
<p>  比较公式\((ml.1.2.20)\)和\((ml.1.2.21)\)，可以发现：</p>
<p>  $$<br>  \Psi(w) = L_{\tilde{P}}(P_w)        \qquad\qquad(ml.1.2.22)<br>  $$</p>
<p>  <strong>即对偶函数\(\Psi(w)\)等价于对数似然函数\(L_{\tilde{P}}(P_w)\)，于是最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计</strong>的结论得以证明。</p>
<blockquote>
<p>总结：<strong>最大熵模型的学习问题就转化为具体求解对数似然函数极大化或对偶函数极大化</strong>的问题。</p>
</blockquote>
<p>  可以将最大熵模型写成更为一般的形式：</p>
<p>  $$<br>  \begin{align}<br>  P_w(y|x) &amp;= \frac{1}{Z_w(x)} \cdot \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y)\right) \\<br>  Z_w(x) &amp;= \sum_{y} \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y)\right)<br>  \end{align} \qquad(ml.1.2.23)<br>  $$</p>
<p>  这里，\(x \in R^n\)为输入（向量），\(y \in \{1,2, \cdots, K\}\)为输出，\(w \in R^n\)为权值向量，\(f_i(x,y), i=1,2, \cdots, n\)为任意实值特征函数。</p>
<blockquote>
<p>小结：<br><br>①. <strong>最大熵模型与LR模型有类似的形式，它们又称为对数线性模型（Log Linear Model）。</strong></p>
<p>②. <strong>模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计。</strong></p>
</blockquote>
</li>
</ul>
<p><br></p>
<h3 id="参数学习的最优化问题">参数学习的最优化问题</h3><p>已知偶函数极大化与极大似然估计等价，那么LR模型、最大熵模型的学习问题可以归结为<strong>以似然函数为目标函数的最优化问题，通常通过迭代算法求解（非闭式解）。</strong></p>
<blockquote>
<p>该部分在<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>中有详细介绍。</p>
</blockquote>
<p>从最优化的角度来，此时的目标函数具有良好的性质：光滑的凸函数。因此多种最优化方法都适用，并且能保证找到全局最优解。常用的方法有改进的迭代尺度法（Improved Iterative Scaling, IIS）、梯度下降法（SGD、mini-batch GD等）、共轭梯度法、拟牛顿法等。</p>
<blockquote>
<p><strong>该部分最优化求解方法会在《最优化算法》系列中详细阐述。</strong></p>
</blockquote>
<p><br></p>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/KL距离/">KL距离</a><a href="/tags/ME/">ME</a><a href="/tags/Maximum-Entropy/">Maximum Entropy</a><a href="/tags/互信息/">互信息</a><a href="/tags/信息熵/">信息熵</a><a href="/tags/信息论/">信息论</a><a href="/tags/最大熵原理/">最大熵原理</a><a href="/tags/最大熵模型/">最大熵模型</a><a href="/tags/条件熵/">条件熵</a><a href="/tags/相对熵/">相对熵</a><a href="/tags/联合熵/">联合熵</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/stats/beta-gamma-dirichlet-function/" title="概率与统计-chapter0-三个重要函数" itemprop="url">概率与统计-chapter0-三个重要函数</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-07-04T04:21:53.000Z" itemprop="datePublished"> 发表于 2015-07-04</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>Author: zhouyongsdzh@foxmail.com</li>
<li>Date: 2015-07-04</li>
<li>Sina Weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<h3 id="多产的数学家－欧拉"><strong>多产的数学家－欧拉</strong></h3><p>他是公认的数学史上4位最伟大的数学家之一，他的一生没有戏剧性的故事，但给后人留下宝贵的科学财富。</p>
<p>18世纪，数学家辈出的年代，他仍属于佼佼者，被认为是18世纪数学界最杰出的人物之一。</p>
<p>同时，他在数学领域最多产的一位，不仅在数学界做出伟大贡献，而且把数学应用到了几乎整个物理领域。他就是<strong>18世纪数学界之星-欧拉</strong>。</p>
<p><a href="http://baike.baidu.com/link?url=OFDCUSh42Bfc_tOI70iV9Zh3hRkxZww91RL_2zlyGNUccKlXAGZRZzLA47ZtTx_XwROYpptHjgsbIJmlAnGPCoRcRi_UIhtRWMQcoqb8zTP-5rLDaLwqvGxU_s2J7Qu596UmWqAJ-43oM5-70om12q" target="_blank" rel="external">莱昂哈德·欧拉</a>)（Leonhard Euler ，1707年4月15日～1783年9月18日），瑞士数学家、自然科学家。1707年出生于瑞士－巴塞尔的牧师家庭，15岁从巴塞尔大学毕业，翌年获得硕士学位。</p>
<p>简述其成就：</p>
<ol>
<li><strong>_微分方程_</strong>: 18世纪中叶，欧拉与其它数学家在解决物理问题过程中，创立了<strong>微分方程</strong>学科。偏微分方程的纯数学研究的第一篇论文是欧拉写的《方程的积分法研究》。此外，还提出了<strong>函数用三角级数表示</strong>的方法和<strong>解微分方程的级数法</strong>等。<br><br></li>
<li><strong>_微分几何学_</strong>：引入了空间曲线的参数方程，给出了空间曲线曲率半径的解析表达式；1766年出版了《关于曲面上曲线的研究》，建立了<strong>曲面理论</strong>，是微分几何发展史上的里程碑。<br><br></li>
<li><strong>_分析学_</strong>：1729年引入了<strong>Gamma函数和Beta函数</strong>，证明了椭圆积分的加法定理，最早引入了<strong>二重积分</strong>。<br><br></li>
<li><strong>_数论_</strong>：欧拉的一系列成果奠定了该数学分支。<br><br></li>
</ol>
<blockquote>
<p>注：数学史上公认的4名最伟大的数学家分别是：<strong>阿基米德、牛顿、欧拉和高斯</strong>。<br></p>
<p>阿基米德有“翘起地球”的豪言壮语，牛顿因为苹果闻名世界，高斯少年时就显露出计算天赋，唯独欧拉没有戏剧性的故事让人印象深刻。</p>
</blockquote>
<p>这里仅总结欧拉在分析学领域中的一个成就－<strong>欧拉积分</strong>及其对应的分布。欧拉积分是一种含参变量的积分，主要包括</p>
<ul>
<li>第一类欧拉积分：又称为Beta函数（简称B函数）</li>
<li>第二类欧拉积分：又称为Gamma函数</li>
</ul>
<h3 id="Gamma函数－第二类欧拉积分">Gamma函数－第二类欧拉积分</h3><blockquote>
<p>因为Beta函数和Dirichlet函数可以表示用Gamma函数表示，所以先介绍Gamma函数。</p>
</blockquote>
<h4 id="历史由来"><br><strong>历史由来</strong></h4><p>1728年，<a href="http://baike.baidu.com/view/7386144.htm?fromtitle=%E5%93%A5%E5%BE%B7%E5%B7%B4%E8%B5%AB&amp;fromid=1632207&amp;type=syn" target="_blank" rel="external">哥德巴赫</a>（Goldbach C. 德国数学家，1690-1764）在考虑<strong>数列插值</strong>的问题，通俗的说就是把数列的通项公式定义<strong>从整数集合延伸至实数集合</strong>，例如数列1,4,9,16…..可以用通项公式\(n^2\)自然地表达，即便\(n\)为实数时，此通项公式也具有良好的定义。另一个自然的解释就是可以找到一条平滑的曲线\(f(x)=x^2\)通过所有的整数点\([x, x^2]\)，从而可以把定义在整数集上的公式延伸至实数集合。</p>
<p>有一天，哥德巴赫开始处理<strong>阶乘序列</strong>\(1,2,6,24,120,720,\cdots\)，我们可以计算整数的阶乘（如\(2!,3!\)），那么是否可以计算实数的阶乘（如\(2.5!\)）呢？</p>
<p>遗憾的是，哥德巴赫当时无法解决<strong>阶乘的计算从整数集合延伸至实数集合上</strong>的问题，于是写信请教<a href="https://en.wikipedia.org/wiki/Nicolaus_II_Bernoulli" target="_blank" rel="external">尼古拉斯.伯努利</a>和其弟<a href="https://en.wikipedia.org/wiki/Daniel_Bernoulli" target="_blank" rel="external">丹尼尔.伯努利</a>，由于当时欧拉与丹尼尔.伯努利在一起，因此也得知了该问题。而欧拉与1729年完美的解决了这个问题，由此导致Gamma函数的诞生，当时的欧拉只有22岁。</p>
<h4 id="Gamma发现之旅"><br><strong>Gamma发现之旅</strong></h4><p>其实最早发现\(n!\)差值计算的是丹尼尔.伯努利。他发现：如果\(m,n\)都是正整数，且\(m\to\infty\)时，有下面公式成立：</p>
<p>$$<br>\frac {1 \cdot 2 \cdot 3 \cdots m} {(1+n)(2+n) \cdots (m-1+n)} \left(m+ \frac{n}{2}\right)^{n-1} \to n!<br>$$</p>
<p>用此<strong>无穷乘积</strong>的方式可以把\(n!\)定义延伸至实数集合。如\(n=2.5,m\)足够大时，上式基本可近似计算出\(2.5!\)的值。</p>
<p>欧拉偶然地发现\(n!\)可用如下无穷乘积表达：</p>
<p>$$<br>\left[\left(\frac{2}{1}\right)^n \frac{1}{n+1}\right] \left[\left(\frac{3}{2}\right)^n \frac{2}{n+2}\right]<br>\left[\left(\frac{4}{3}\right)^n \frac{3}{n+3}\right]<br>\cdots = n!<br>$$</p>
<p>于是他用一些简单的例子做一些计算，寻找其规律。当\(n=1/2\)时，整理上式可得：</p>
<p>$$<br>\left(\frac{1}{2}\right)!=\sqrt{\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots}<br>$$</p>
<p>此式恰好与著名的<a href="http://baike.baidu.com/link?url=FWbPfOA5BqiQxrCJ4KJnedzJRf6blsxbh2qn1XyGlPvUbGXufoVXf4xie6UMnWNUUAH2ewNaziQug2YzHQ9jpK" target="_blank" rel="external">Wallis公式</a>有关。</p>
<blockquote>
<p>关于Wallis公式：</p>
<p>\( \qquad \lim\limits_{k\to\infty} \left(\frac{2^{2k}(k!)^2}{2k!}\right)^2 \frac{1}{2k+1} = \frac{\pi}{2} \)</p>
<p>Wallis公式是关于圆周率的无穷乘积的公式，Wallis本人在1665年使用<strong>插值方法</strong>计算<strong>半圆曲线</strong>\(y=\sqrt{x(1-x)}\)下的面积（即直径为1的半圆面积）时，发现\(\pi/4\)等于下式：</p>
<p>\(<br>\qquad\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots = \frac{\pi}{4}<br>\)</p>
</blockquote>
<p>于是欧拉根据Wallis公式得到如下的结果：</p>
<p>$$<br>\left(\frac{1}{2}\right)! = \frac{\sqrt{\pi}}{2}<br>$$</p>
<p>欧拉发现\((\frac{1}{2})!\)中含有\(\pi\)，而\(\pi\)与圆相关的积分有关系。欧拉猜测\(n!\)一定可以用积分形式表示。</p>
<p>Wallis时代微积分还没问世，当时是使用插值方法做推导计算的，但Wallis公式的推导过程基本上都是在处理积分\(\int_{0}^{1} x^\frac{1}{2} (1-x)^\frac{1}{2} dx\)的问题。</p>
<blockquote>
<p>关于微积分诞生：<br><br>牛顿与莱布尼茨谁先发明的微积分在数学界有很大的争论。不过本人更认可是莱布尼茨，因为微积分的主要思想是他提出的，微分相关概念和符号是他定义的，并沿用至今。<br><br>1684年<a href="http://baike.baidu.com/link?url=TsKq7RAFzLHNnXSqoXKfEosO1ogw_FQnRME1vlTwvwKiJs0hudGCkvWkUJW1MHwpiGPGB7SHD49O2Nmea5m_i63qc9eJSMlDzZ7VAPo7D2p4d6oj9Bb0MVkLfDUaSbqvNCc5Opspb0YHMMGf8PJXaH5kmhwbnefwifkVxETybompTpK4IDEAoP9ZR83kyP4L" target="_blank" rel="external">莱布尼茨</a>发表了关于微积分的重要文献，标志着微积分作为独立学科正式诞生。莱布尼茨是微积分主要思想的独立发明人。当然，不可否认的是牛顿在此前同样做了很多微积分相关工作。</p>
</blockquote>
<p>受Wallis启发，欧拉开始考虑如下一般形式的积分：</p>
<p>$$<br>J(e,n)=\int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>这里，\(n\)为正整数，\(e\)为正实数。利用<strong>分部积分</strong>法，可得：</p>
<p>$$<br>J(e,n)=\frac{n}{e+1}J(e+1, n-1)<br>$$</p>
<blockquote>
<p><strong>关于<a href="http://baike.baidu.com/link?url=2UfHeRl5l_jvgTbPSA4APPlKvrivw6i7iTmXB6-phTsDM-VqyQtxfJyb8EpIFR5r_5vwkjedDtN5japSjucvqq" target="_blank" rel="external">分部积分</a></strong>：<br><br>设\(u=u(x)\)和\(v=v(x)\)均为可导函数，分部积分形式如下：<br></p>
<p>\( \qquad \int u \mathop{v’}dx = \int u dv = [uv] - \int v du \)</p>
</blockquote>
<p>重复迭代上式，可得：</p>
<p>$$<br>J(e,n) = \frac {1\cdot2\cdot3\cdots n}{(e+1)(e+2)\cdots (e+n+1)}<br>$$</p>
<p>于是欧拉得到如下重要的式子：</p>
<p>$$<br>n!=(e+1)(e+2)\cdots (e+n+1) \int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>接下来，欧拉使用一些计算技巧，即取\(e=f/g\) 且 \(f\to1, g\to0\)，然后对上式右边计算极限，得到如下简洁的结果：</p>
<p>$$<br>n! = \int_{0}^{1}(-\log{t})^n dt<br>$$</p>
<p>到此，欧拉成功地把\(n!\)表达为了积分形式！令\(t=e^{-u}\)，可得常见的Gamma函数形式：</p>
<p>$$<br>n! = \int_{0}^{\infty} u^n e^{-u} du<br>$$</p>
<p>注意，此时的\(n\)仍然为正整数，利用上式把<strong>阶乘延伸至实数集</strong>上，就得到Gamma函数的一般形式：</p>
<p>$$<br>\Gamma(x) = \int_{0}^{1} (-\log t)^{x-1}dt = \int_{0}^{\infty} t^{x-1} e^{-t} dt<br>$$</p>
<blockquote>
<p>注：使用MathJax引擎在浏览器上解析时，需要对”_”做转义，应写成下面形式：<br></p>
<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\<span class="keyword">Gamma</span>(x) = \int\_<span class="list">&#123;0&#125;</span>^<span class="list">&#123;1&#125;</span> (-\log t)^<span class="list">&#123;x-1&#125;</span>dt = \int_<span class="list">&#123;0&#125;</span>^<span class="list">&#123;\infty&#125;</span> t^<span class="list">&#123;x-1&#125;</span> e^<span class="list">&#123;-t&#125;</span>dt</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>为什么</strong>\(\Gamma(n)=(n-1)!\)<strong>，而不是</strong>\(\Gamma(n)=n!\)</p>
<p>如果对上述的Gamma函数式做一个调整，将\(t^{x-1}\)替换为\(t^x\)，可得如下函数：</p>
<p>$$<br>\Gamma(x) = \int_{0}^{\infty} t^x e^{-t}dx<br>$$</p>
<p>此时\(\Gamma(n)=n!\)，这才是欧拉最早的Gamma函数定义。但后来欧拉修改了原有Gamma函数的定义，使得\(\Gamma(n)=(n-1)!\)。后来的数学家们在对Gamma函数的进一步研究中，认可了这个定义，沿用至今。</p>
<h4 id="Gamma分布">Gamma分布</h4><ul>
<li>概率密度函数</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {x^{\alpha-1} e^{x/\beta}} {\beta^\alpha \Gamma(\alpha)}, &amp; 0 \leq x \leq \infty; \alpha &gt; 0; \beta &gt; 0 \\\<br>\;\;\quad 0, &amp; others<br>\end{cases}<br>$$</p>
<h3 id="Beta函数－第一类欧拉积分">Beta函数－第一类欧拉积分</h3><p>在@rickjin的《LDA-math》系列中详细地解释了Beta和Dirichlet分布的由来和推导过程，堪称经典。原文见下面链接：</p>
<ul>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%831" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(1)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%832" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(2)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%833" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(3)</a> <br></li>
</ul>
<p>@rickjin在文中从一个<strong>魔鬼的游戏</strong>开始引入。魔鬼撒旦抓走一人，撒旦说：“你们人类很聪明，而我是很仁慈的，和你玩一个游戏，赢了就可以走，否则把灵魂出卖给我。”</p>
<blockquote>
<p>游戏规则：<br><br>我有一个魔盒，上面有一个按钮，你每按一下按钮，就均匀的输出一个[0,1]之间的随机数，我现在按10下，我手上有10个数，你猜第7大的数是什么，偏离不超过0.01就算对。</p>
</blockquote>
<h4 id="数学抽象与推导">数学抽象与推导</h4><p>上述游戏实际在说随机变量\(X_1, X_2, \cdots, X_{10} \sim U(0,1)\), 把这\(n\)个随机变量排序后得到顺序统计量\(X_{(1)}, X_{(2)}, \cdots, X_{(10)}\)，然后问\(X_{(k)}\)的分布是什么？</p>
<blockquote>
<p>因为如果知道随机变量\(X_{(k)}\)分布的概率密度，用概率密度的极值点作为猜测值是最好的策略。</p>
</blockquote>
<p>对于上述游戏而言，\(n=10,k=7\)，\(X_{(k)}\)的分布如何计算？@rickjin在文中列举了通过尝试计算\(X_{(k)}\)落在区间\([x,x+\Delta x]\)的概率，也就是求如下概率值</p>
<p>$$<br>P(x \leq X_{(k)} \leq x + \Delta x) = \;?<br>$$</p>
<p>如果把[0,1]区间分为三个子区间，即\([0,x)、[x,x+\Delta x]和(x+\Delta x,1]\)。考虑简单的情形，假设\(n\)个数中只有一个落在了区间\([x,x+\Delta x]\)内，这个区间内的数\(X_{(k)}\)是第\(k\)大的，则区间\([0,x)\)中应该有\(k-1\)个数，区间\((x+\Delta x,1]\)中应该有\(n-k\)个数。那么，符合上述要求的事件\(E\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E = \\{<br>&amp; X_1 \in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x)\quad (i=2,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<blockquote>
<p>注：</p>
<ol>
<li>符号<code>*</code>在markdown中有特殊含义，若要当作latex环境中的语法用，需要转义，即<code>\ *</code>.</li>
<li>在Mathjax下，<code>$$E = \{x, y\}$$</code>无法正常输出<code>{}</code>, 因为Latex要想输出<code>{}</code>，需要使用<code>\{, \}</code>。而<code>\</code>在markdown中要想正常表法其自身意义也需要再次转义，代码为: <code>$$E = \\{x,y\\}$$</code>。</li>
</ol>
<p>$$E = \\{ x,y\\}$$</p>
</blockquote>
<p>那么事件\(E\)发生的概率，有：</p>
<p>$$<br>\begin{align*}<br>P(E) &amp; = \prod_{i=1}^nP(X_i) \\\<br>&amp; = x^{k-1}(1-x-\Delta x)^{n-k}\Delta x \\\<br>&amp; = x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>\(o(\Delta x)\)表示\(\Delta x\)的高阶无穷小。显然，由于<strong>不同的排列组合</strong>，即\(n\)个数中有一个落在\([x, x+ \Delta x]\)区间的有\(n\)中取法，余下\(n-1\)个数中有\(k-1\)个落在\([0, x)\)区间有\(\binom{n-1}{k-1}\)种组合，所以<strong>与事件E等概率的事件一共有\(n \binom{n-1}{k-1}\)个（当只有1个数落在\([x, x+ \Delta x]\)时）</strong>。</p>
<p>继续考虑复杂一些的情形，假设\(n\)个数中<strong>两个数</strong>落在了区间\([x, x+ \Delta x]\)，此时事件\(E^{‘}\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E’ = \\{<br>&amp; X_1,X_2\in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x) \quad (i=3,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<p>此时，事件\(E^{‘}\)发生的概率为：</p>
<p>$$<br>P(E^{‘}) = x^{k-2}(1-x-\Delta x)^{n-k}(\Delta x)^2 = o(\Delta x)<br>$$</p>
<p>很容易看出，只要落在\([x, x+ \Delta x]\)内的数字<strong>大于1个</strong>，则对应事件的概率就是\(\Delta x\)。于是随机变量\(X_{(k)}\)落在\([x, x+ \Delta x]\)区间的概率：</p>
<p>$$<br>\begin{align*}<br>&amp; P( x \le X_{(k)} \le x+\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}P(E) + o(\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>进一步可得\(X_{(k)}\)的概率密度函数为</p>
<p>$$<br>\begin{align*}<br>f(x) &amp; = \lim_{\Delta x \to 0} \frac {P(x \leq X_{(x)} \leq x + \Delta x)}{\Delta x} \\\<br>&amp; = n \binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\\\<br>&amp; = \frac {n!}{(k-1)! (n-k)!}x^{k-1} (1-x)^{n-k} \quad x \in [0,1]<br>\end{align*}<br>$$</p>
<p>利用上述介绍的Gamma函数，可以把\(f(x)\)表示为</p>
<p>$$<br>f(x) = \frac {\Gamma(n+1)}{\Gamma(k) \Gamma(n-k+1)} x^{k-1} (1-x)^{n-k}<br>$$</p>
<p><strong>\(\Gamma\)函数起源于人们希望把数学计算从整数集合拓展至实数集合</strong>。这里另\(\alpha=k, \beta=n-k+1\)，于是可以得到</p>
<p>$$<br>Beta(x|\alpha,\beta)=f(x)=\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}<br>$$</p>
<p>上式即为一般意义上的Beta分布！</p>
<h4 id="\(\beta\)型概率分布"><br><strong>\(\beta\)型概率分布</strong></h4><ul>
<li>\(\beta\)型随机变量\(X\)的概率密度函数：</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}, &amp; 0 \leq x \leq 1;\;\alpha &gt; 0, \beta &gt; 0 \\\<br>0, &amp; others<br>\end{cases}<br>\quad (**)<br>$$</p>
<blockquote>
<p>关于<strong>Beta函数详细推导</strong><br></p>
<p>$$<br>\begin{align*}<br>f(x;\alpha,\beta) &amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt} \\\<br>&amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{Beta(\alpha,\beta)}<br>\end{align*}<br>$$</p>
<p>其中，</p>
<p>$$<br>Beta(\alpha,\beta)= \int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}<br>$$</p>
<p>$$<br>\Gamma(\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt<br>$$<br>且当\(\alpha\)是正整数时，\(\Gamma(\alpha)=(\alpha-1)!\)</p>
</blockquote>
<ul>
<li>\(\beta\)型随机变量的期望与方差分别是：</li>
</ul>
<p>$$\mu=\frac{\alpha}{\alpha+\beta} \;\qquad\qquad\qquad (期望)$$</p>
<p>$$\sigma^2=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \quad (方差)$$</p>
<ul>
<li>\(\beta\)型分布的期望公式详细推导</li>
</ul>
<p>如果\(p \sim Beta(t|\alpha,\beta)\)，那么\(p\)的期望表示如下：</p>
<p>$$<br>\begin{align*}<br>E(p)<br>&amp; =\int_{0}^{1} t \ast Beta(t|\alpha,\beta)\;dt \\\<br>&amp; = \int_{0}^{1} t \ast \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}t^{\alpha-1}(1-t)^{\beta-1}\;dt \\\<br>&amp; =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt<br>\end{align*}<br>$$</p>
<p>上式右边的积分\(\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt\)恰好对应到概率分布\(Beta(t|\alpha+1,\beta)\)。对于此分布，根据Beta型随机变量的密度函数在\([0,1]\)之间的累积分布等于1，可得:</p>
<p>$$<br>\int_{0}^{1} \frac{\Gamma(\alpha+\beta+1)}{\Gamma(\alpha+1)\Gamma(\beta)} t^\alpha (1-t)^{\beta-1}\;dt=1<br>$$</p>
<p>把上式带入\(E(p)\)计算式，得到 </p>
<p>$$<br>\begin{align*}<br>E(p) &amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \cdot<br>\frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \\\<br>&amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+1)}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \notag \\\<br>&amp; = \frac{\alpha}{\alpha+\beta}<br>\end{align*}<br>$$</p>
<p>这说明，<strong>对于Beta分布的随机变量，其均值可以用\(\frac{\alpha}{\alpha+\beta}\)来估计</strong>。</p>
<ul>
<li>\(\beta\)型概率分布曲线</li>
</ul>
<p><strong>_(此处暂略~)_</strong></p>
<p>回到<strong>魔鬼的游戏</strong>中，\(n=10, k=7\)时，按照密度分布的峰值去猜测是最好的策略。</p>
<p>$$<br>f(x)=\frac{10!}{(6)!(3)!} x^6 (1-x)^3 \quad x \in [0,1]<br>$$</p>
<p>即便按照密度函数分布的峰值作为猜测结果，第一次猜中的概率也不高。<br><br></p>
<p><strong>游戏继续\(\cdots\)</strong></p>
<p>很遗憾，根据上述最好的策略算出来的值竟然有偏差，没猜中，魔鬼微笑着说：“我再仁慈一点，再给你一个机会，你按5下这个机器，你就得到了5个\([0,1]\)之间的随机数，然后我可以告诉你这5个数中的每一个，和我的第7个数相比，谁大谁小，然后你继续猜我手头上的第7大的数是多少。” 这时候该如何猜测呢？</p>
<h3 id="Beta-Binomial共轭"><br><strong>Beta-Binomial共轭</strong></h3><p>魔鬼的两个问题，数学抽象一下，就是：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \in Uniform(0,1)\)，对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\)，我们要猜测\(p=X_{(k)}\)；（第1个问题）</li>
<li>\(Y_1,Y_2, \cdots, Y_m \in Uniform(0,1)\)，\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大；（第2个问题）</li>
</ul>
<p>最后的问题：<strong>\(P(p|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</strong></p>
<p><strong>问题分析</strong></p>
<p>由于\(p=X_(k)\)在\(X_1,X_2, \cdots, X_n\)中是第\(k\)大的，利用\(Y_i\)的信息，可以很容易的推理得到\(p=X_{(k)}\)在\(X_1,X_2, \cdots, X_n,\) \(Y_1, Y_2, \cdots, Y_m \in Uniform(0,1)\)这\((m+n)\)个独立随机变量中是第\(k+m_1\)大的。那么按照<strong>Beta分布（贝塔分布）</strong>小节的推理，此时\(p=X_{(k)}\)的概率密度函数是\(Beta(p|\,k+m_1, n-k+1+m_2)\)。</p>
<p>根据<strong>贝叶斯推理</strong>的逻辑，整理上述过程如下：</p>
<ul>
<li>\(p=X_{(k)}\)是我们需要猜测的参数，并且推导出\(p\)的分布为\(f(p)=Beta(p|k,n-k+1)\)，称为\(p\)的<strong>先验分布</strong>；</li>
<li>数据\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大，\(Y_i\)相当于做了\(m\)次<strong>贝努利试验</strong>，所以\(m_1\)服从二项分布\(B(m,p)\)；</li>
<li>在给定了来自数据提供的\((m_1, m_2)\)的知识后，\(p\)的<strong>后验分布</strong>变为:</li>
</ul>
<p>$$f(p|\,m_1,m_2)=Beta(p|\,k+m_1, n-k+1+m_2)$$</p>
<blockquote>
<p>关于<strong>贝叶斯参数估计</strong>的基本过程：</p>
<p>$$先验分布 ＋ 数据知识 ＝ 后验分布$$</p>
</blockquote>
<p>在这里，贝叶斯分析过程的简单直观的表述就是：</p>
<p>$$Beta(p|\,k, n-k+1) + Count(m_1, m_2) = Beta(p|\,k+m_1,n-k+1+m_2)$$</p>
<p>其中\(m_1,m_2\)对应的是二项分布\(B(m_1+m_2, p)\)的计数。更为一般地数学表述：对于非负实数\(\alpha,\beta\)，存在如下关系</p>
<p>$$<br>Beta(p|\,\alpha, \beta) + Count(m_1, m_2) = Beta(p|\,\alpha+m_1, \beta+m_2) \quad (**)<br>$$</p>
<p>该式描述的就是<strong>Beta-Binomial共轭</strong>。</p>
<blockquote>
<p>关于<strong>Beta-Binomial共轭</strong> <br><br>数据符合<strong>二项分布</strong>的时候，参数的先验分布和后验分布都能保持<strong>Beta分布</strong>的形式，这种形式不变的好处是：我们能够在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续至后验分布中进行解释，同时从先验变换到后验的过程中从数据中补充的知识也容易给出物理上的解释。</p>
</blockquote>
<p>推导过程中可以看到，\(Beta\)分布中的参数\(\alpha,\beta\)都可以理解为物理计数，这两个参数经常被称为<strong>伪计数（pseudo-count）</strong>。\(Beta(p|\,\alpha, \beta)\)可以写成如下表达式：</p>
<p>$$Beta(p|\,1,1) + Count(\alpha-1, \beta-1) = Beta(p|\,\alpha,\beta)$$</p>
<p>其中的<strong>\(Beta(p|\,1,1)\)恰好就是均匀分布\(Uniform(0,1)\)</strong>。</p>
<blockquote>
<p>关于<strong>Beta分布与均匀分布</strong>的关系：</p>
<p>$$Beta(p|\,\alpha=1,\beta=1)= Uniform(a=0,b=1)$$</p>
</blockquote>
<p>对于\((**)\)共轭公式，其实可纯粹从贝叶斯的角度来进行推导。理解过程：</p>
<ul>
<li>假设有一个不均匀的硬币抛出正面的概率为\(p\)，抛\(m\)次后出现正面和反面的次数分别是\(m_1,m_2\)，那么按照传统的<strong>频率学派</strong>观点，\(p\)的估计值应该是\(\hat p = \frac{m_1}{m}\)。<br></li>
<li>而从<strong>贝叶斯学派</strong>的观点来看，开始对硬币不均匀性一无所知，所以应该假设\(p\ \in Uniform(0,1)\)，于是二项分布的计数\(m_1,m_2\)之后，按照贝叶斯公式计算\(p\)的后验分布：</li>
</ul>
<p>$$<br>\begin{align*}<br>P(p|\,m_1,m_2) = &amp; \frac {P(p) \cdot P(m_1,m_2\,|p)}{P(m_1,m_2)} \\\<br>= &amp; \frac {1 \cdot P(m_1,m_2 |p)}{\int_{0}^{1} P(m_1,m_2|t)dt} \\\<br>= &amp; \frac {\binom{m}{m_1}p^{m_1}(1-p)^{m_2}}{\int_{0}^{1} \binom{m}{m_1}t^{m_1}(1-t)^{m_2}dt} \\\<br>= &amp; \frac {p^{m_1}(1-p)^{m_2}}{\int_{0}^{1}t^{m_1}(1-t)^{m_2}dt}<br>\end{align*}<br>$$</p>
<p>得到的后验分布正好是\(Beta(p|\;m_1 + 1, m_2 + 1)\)。</p>
<p>回到<strong>魔鬼的游戏</strong>，如果按出的5个随机数字中，魔鬼告诉你有2个（即\(m_1=2\)）小于他手中第7大的数，那么应该按照如下概率分布的<strong>峰值</strong>做猜测是最好的(\(\alpha+2=9,\beta+3=7\))：</p>
<p>$$<br>Beta(x|9,7)=\frac{15!}{(8)!(6)!}x^8(1-x)^6  \quad x \in [0,1]<br>$$</p>
<p>很幸运，这次猜中了，但是魔鬼开始耍赖，游戏不得不继续 \(\cdots\)</p>
<blockquote>
<p>游戏3新规则：<br><br>魔说道：“这个游戏对你来说太简单了，我要加大点难度，我们重新来一次，<strong>我按20下生成20个随机数，你要同时给我猜第7大和第13大的数是什么？</strong>”</p>
</blockquote>
<p>此时又该如何猜测呢？<br></p>
<h3 id="Dirichlet分布"><strong>Dirichlet分布</strong></h3><p>对于魔鬼变本加厉的新的游戏规则，数学抽象如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\),</li>
<li>排序后对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\),</li>
<li><strong>问: \((X_{(k_1)},X_{(k_1 + k_2)})\)的联合分布是什么？</strong></li>
</ul>
<p>游戏3完全类似游戏1的推导过程，可进行如下的概率计算：</p>
<blockquote>
<p>注：为了公式的简洁性，这里取\(x_3=1-x_1-x_2\),但只有\(x_1,x_2\)是变量。说明符号：<br></p>
<ol>
<li>n: 总的样本数<br></li>
<li>\(X_{(k_1)}\)和\(X_{(k_1 + k_2)}\)将n分割为3段，长度分别为\(k_1, k_2和k_3\)，即<br></li>
</ol>
<p>$$<br>分段结果：<br>\begin{cases}<br>[X_{(1)}, X_{(k-1)}], &amp; 有(k_1-1)个取值，统一记为x_1 \\\<br>[X_{(k_1+1)}, X_{(k_1+k_2)}], &amp; 有(k_2-1)个取值，统一记为x_2 \\\<br>[X_{(k_1+k_2+1)}, X_{(n)}] &amp; 有(n-k_1-k_2)个取值，统一记为x_3<br>\end{cases}<br>$$</p>
</blockquote>
<p>完全类似于游戏1的推导过程，\(X_{(k_1)},X_{(k_1+k_2)}\)的<strong>联合概率</strong>计算如下：</p>
<p>$$<br>\begin{align*}<br>&amp; P\Bigl(X_{(k_1)} \in (x_1, x_1+\Delta x), X_{(k_1+k_2)} \in (x_2, x_2+\Delta x)\Bigr) \\\<br>&amp; \quad = n(n-1)\binom{n-2}{k_1-1,k_2-1}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2 \\\<br>&amp; \quad = \frac{n!}{(k_1-1)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2<br>\end{align*}<br>$$</p>
<blockquote>
<p>式中的\(n(n-1)\binom{n-2}{k_1-1,k_2-1}\)是一个排列组合问题，比较容易理解。</p>
</blockquote>
<p>于是得到\(X_{(k_1)},X_{(k_1+k_2)}\)的联合分布是：</p>
<p>$$<br>\begin{align*}<br>f(x_1,x_2,x_3) &amp; =\frac{n!}{(k_1-)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2} \\\<br>&amp; = \frac{\Gamma(n+1)}{\Gamma(k_1)\Gamma(k_2)\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}<br>\end{align*} \quad (**)<br>$$</p>
<p>而公式\((**)\)则是3维形式的Dirichlet分布，即\(Dir(x_1,x_2,x_3\;|\;k_1,k_2,n-k_1-k_2+1)\)。这里令\(\alpha_1=k_1,\alpha_2=k_2,\alpha_3=n-k_1-k_2+1\)，于是分布密度函数可以写为：</p>
<p>$$<br>f(x_1, x_2, x_3) = \frac{\Gamma(\alpha_1+\alpha_2+\alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}x_2^{\alpha_2-1}x_3^{\alpha_3-1} \qquad (0)<br>$$</p>
<p>公式\((0)\)即为一般形式的3维Dirichlet分布。即使\(\vec{\alpha}=(\alpha_1,\alpha_2,\alpha_3)\)<strong>延伸至非负实数集合</strong>，上述概率分布也是成立的。</p>
<p>在游戏3的基础上还可以往更高的维度上推，譬如\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)中的4、5、…等更多个数，可以得到更高维度的Dirichlet分布和Dirichlet-Multinomial共轭。</p>
<ul>
<li>Dirichlet分布</li>
</ul>
<p>如果\(\vec{p} \sim Dir(\vec{t}|\vec{\alpha})\),那么随机变量\(\vec{p}\)的概率密度函数为</p>
<p>$$<br>\begin{equation}<br>\displaystyle Dir(\overrightarrow{p}|\overrightarrow{\alpha}) =<br>\displaystyle \frac{\Gamma(\sum_{k=1}^{K} \alpha_k)}<br>{\prod_{k=1}^K \Gamma(\alpha_k)} \prod_{k=1}^{K} p_k^{\alpha_k -1}<br>\end{equation}    \qquad (1)<br>$$</p>
<p>对于给定的\(\overrightarrow{p}\)和\(N\)，多项式分布定义为：</p>
<p>$$<br>Multi(\overrightarrow{n}|\overrightarrow{p},N)=<br>\binom{N}{\overrightarrow{n}} \prod_{k=1}^K p_k^{n_k} \qquad (2)<br>$$</p>
<p>而\(Multi(\overrightarrow{n}|\overrightarrow{p},N)\)和\(Dir((\overrightarrow{p}|\overrightarrow{\alpha})\)这两个分布是共轭关系。</p>
<ul>
<li>Dirichlet分布均值</li>
</ul>
<p>$$<br>E(\vec{p}) = \Bigl(\frac{\alpha_1}{\sum_{i=1}^{K}\alpha_i},\frac{\alpha_2}{\sum_{i=1}^{K}\alpha_i}, \cdots, \frac{\alpha_K}{\sum_{i=1}^{K}\alpha_i}\Bigr)<br>$$</p>
<blockquote>
<p>关于<strong>Dirichlet分布与Beta分布</strong>：<br><br>Dirichlet分布是Beta分布在高维度上的推广。当Dirichlet分布维度趋向无限时，便成为Dirichlet过程。<br><br></p>
</blockquote>
<h3 id="Dirichlet-Multinomial共轭"><strong>Dirichlet-Multinomial共轭</strong></h3><p>类似于魔鬼的游戏2，调整一下游戏3，可得游戏4的规则如下：</p>
<blockquote>
<p>游戏4: <br><br>从魔盒中生成m个随机数\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)，魔鬼告诉我们\(Y_i\)和\(X_{(k_1)},X_{(k_1+k_2)}\)相比谁大谁小，然后再次猜测第7大和第13大的数是多少？</p>
</blockquote>
<p>同样，对游戏4进行数学抽象，表示如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\)，排序后对应的顺序统计量为\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)；</li>
<li>令\(p_1=X_{(k_1)},p_2=X_{(k_1+k_2)},p_3=1-p_1-p_2(p_3是为了简洁的数学表达)\)，我们要猜测\(\vec{p}=(p_1,p_2,p_3)\)；</li>
<li>\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\),\(Y_i\)中落到\([0,p_1), [p_1,p_2),[p_2,1]\)三个区间的个数分别是\(m_1,m_2,m_3\),其中\(m=m_1+m_2+m_3\)；</li>
<li>问后验分布\(P(\vec{p}|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</li>
</ul>
<p>为了计算方便，记：</p>
<p>$$<br>\vec{m}=(m_1, m_2, m_3), \quad \vec{k}=(k_1,k_2,n-k_1-k_2+1)<br>$$</p>
<p>从游戏中的信息可以得知：\(p_1,p_2\)在\(X_1,X_2, \cdots, X_n,Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)这\(m+n\)个数中分别成为了第\(k_1+m_1, k_2+m_2\)大的数，于是后验分布\(P(\vec{p}|Y_1,Y_2,\cdots, Y_m)\)应该是\(Dir(\vec{p}|k_1+m_1,k_2+m_2, n-k_1-k_2+1+m_3)\)，即\(Dir(\vec{p}|\vec{k}+\vec{m})\)。按照贝叶斯推理的逻辑，同样可以把上述过程整理如下：</p>
<ul>
<li>首先，根据游戏3需要猜测参数\(\vec{p}=(p_1,p_2,p_3)\)，其先验分布为\(Dir(\vec{p}|\vec{k})\)；</li>
<li>其次，数据\(Y_i\)落到\([0,p_1),[p_1,p_2),[p_2,1]\)三个区间的个数分别为\(m_1,m_2,m_3\)，所以\(\vec{m}=(m_1, m_2, m_3)\)服从多项式分布\(Multi(\vec{m}|\vec{p})\)<strong>（理解这一点很重要）</strong>；</li>
<li>在给定了来自数据提供的知识\(\vec{m}\)后，\(\vec{p}\)的后验分布变为\(Dir(\vec{p}|\vec{k}+\vec{m})\)。</li>
</ul>
<p>以上贝叶斯分析过程的最简单直接的表述：</p>
<p>$$<br>Dir(\overrightarrow{p}|\overrightarrow{k})+Multi(\vec{m})=Dir(\vec{p}|\vec{k}+\vec{m})<br>$$</p>
<p>令\(\overrightarrow{\alpha}=\vec{k}\)，把\(\vec{\alpha}\)从整数集合延拓至实数集合，可以证明如下关系仍然成立：</p>
<p>$$<br>Dir(\vec{p}|\vec{\alpha})+Multi(\vec{m})=Dir(\vec{p}|\vec{\alpha}+\vec{m}) \quad (11)<br>$$</p>
<p>公式\((11)\)描述就是<strong>Dirichlet-Multinomial共轭</strong>。同时，我们可以把Dirichlet分布中的\(\alpha\)都可以理解为物理计数。那么，类似于Beta分布，\(Dir(\vec{p}|\vec{\alpha})\)可做如下分解：</p>
<p>$$<br>Dir(\vec{p}|\vec{1})+Multi(\vec{m}-\vec{1}) = Dir(\vec{p}|\vec{\alpha})<br>$$</p>
<p>这里\(\vec{1}=(1,1,\cdots,1)\),上式同样可以类似的用纯粹贝叶斯的观点推导和解释。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/概率与统计/">概率与统计</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Gamma-Beta-Dirichlet/">Gamma,Beta,Dirichlet</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/stats/prob-stats-chapter3-continuous-random-variable/" title="概率与统计-chapter3-连续随机变量" itemprop="url">概率与统计-chapter3-连续随机变量</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-06-20T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-06-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="连续随机变量（continuous_random_variable）">连续随机变量（continuous random variable）</h2><ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-06-29 22:34:13</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>目录</strong></p>
<ul>
<li>均匀分布</li>
<li>高斯分布</li>
<li>Gamma分布</li>
<li>Erlang分布</li>
<li>指数分布</li>
<li>Beta分布</li>
<li>Dirichlet分布</li>
<li>other<br><br></li>
</ul>
<h3 id="说明">说明</h3><p>可以取连续值的随机变量称为<strong>连续随机变量</strong>（continuous random variable）。</p>
<p>由于是取连续值，那么在任意区间都可以有无穷多个结果。比如高度，可以有1米、2米，也可以在两者之间：1.1米、1.11米，1.688米等无穷多个结果。如此，每个结果取值的可能性都是无穷小。</p>
<p>因此，连续随机变量区别于离散随机变量重要一点：在连续随机变量中，讨论的是时间在<strong>_某个区间_</strong>内发生的概率，即\(P(a &lt; X &lt; b)\)，而不是具体某一取值的概率\(P(X)\)。因为在这种情况下，分到各个结果的概率都无限趋于0。显然，无法用离散随机变量中的<strong>概率质量函数</strong>来描述随机变量的分布。</p>
<p>针对连续随机变量的分布，我们可用以下指标来描述具体的分布：</p>
<ul>
<li><p>累积分布函数（Cumulative Distribution Function，简称CDF）</p>
<p>  累积分布函数本身表示随机变量在一个区间上的概率，所以可直接用于连续随机变量。即：</p>
</li>
</ul>
<p>$$F(x)=P(X \leq x),\quad -\infty &lt; x &lt; \infty$$</p>
<ul>
<li><p>概率密度函数（Probability Density Function，简称PDF）</p>
<p>  根据<code>无穷小</code>的概念，可以得到概率密度函数。在随机变量\(X＝x\)的附近去一个无穷小段，该小段的区间长度为\(dx\)，这无穷小段对应的概率为\(dF\)，那么该点的概率密度为\(dF/dx\)。</p>
<p>  因此，概率密度函数可以代替累积分布函数，表示一个连续随机变量的概率分布。</p>
<p>  $$f(x)=\frac {dF(x)}{dx}$$</p>
<p>  $$F(x)=\int_{-\infty}^{x}f(t)dt$$ </p>
<blockquote>
<p>CDF与PDF之间的关系：PDF是CDF的微分，CDF是PDF在区间\([-\infty, x]\)上的积分。</p>
</blockquote>
</li>
<li><p>均值（Expectation，又称均值Mean）</p>
</li>
<li>方差（Variance）</li>
</ul>
<blockquote>
<p>这里的连续随机变量主要包括：均匀概率分布、高斯分布、Gamma分布等.<br></p>
</blockquote>
<h3 id="均匀分布（Uniform_Distribution）"><strong>均匀分布（Uniform Distribution）</strong></h3><p>假设有一个随机数生成器，产生\([a, b]\)之间的实数\(X)\)（\(a&lt;b\)），每个实数值出现的概率相等，这样的分布被称为均匀分布。那么，随机变量\(X\)服从区间\([a,b]\)上的均匀分布，记作\(X \sim U(a,b)\)。该类型的随机变量称作<strong>均匀随机变量</strong>。</p>
<blockquote>
<p>随机变量\(X\)的密度函数用图可表示为一个长方形，长方形的高是\(1\,/\,(b-a)\)，以保证长方形的面积等于1.</p>
</blockquote>
<ul>
<li>均匀随机变量X的<strong>概率密度函数（PDF）</strong>为：</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {1}{b-a}, &amp; a \leq x \leq b \\\<br>\quad0, &amp; others<br>\end{cases}<br>$$</p>
<ul>
<li>均匀随机变量X的<strong>累积分布函数（CDF）</strong>本身就表示随机变量在\([-\infty, x]\)区间上的概率，是概率密度函数在\([-\infty, x]\)区间上的积分，公式如下：</li>
</ul>
<p>$$<br>F(x)=P(X \leq x)=\int_{-\infty}^{x}f(t)dt=<br>\begin{cases}<br>\quad0, &amp; -\infty &lt; x &lt; a \\\<br>\displaystyle\frac {x-a}{b-a}, &amp; a \leq x \leq b \\\<br>\quad1, &amp; b &lt; x &lt; \infty<br>\end{cases}<br>$$</p>
<ul>
<li>均匀随机变量X的<strong>期望</strong>：</li>
</ul>
<p>$$<br>E(X)=\frac {a+b}{2}<br>$$</p>
<ul>
<li>均匀随机变量X的<strong>方差</strong>：</li>
</ul>
<p>$$<br>D(X)=E(X^2)-E^2(X)=\frac {(b-a)^2}{12}<br>$$</p>
<blockquote>
<p>注：在概率统计学中，几乎所以重要的概率分布都可以从均匀分布\(Uniform(0,1)\)中生成，尤其是在做统计模拟试验中，所有统计分布的随机样本都是通过均匀分布产生的。<br></p>
</blockquote>
<h3 id="高斯分布（Gaussian_Distribution）">高斯分布（Gaussian Distribution）</h3><ul>
<li>概率密度函数</li>
</ul>
<p>$$<br>f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2 \sigma^2} \right)<br>$$</p>
<p><br></p>
<h3 id="Gamma分布（伽马分布，又称\(\Gamma\)型概率分布）">Gamma分布（伽马分布，又称\(\Gamma\)型概率分布）</h3><p>在认识Gamma分布之前，我们先熟悉一下Gamma函数，在@52nlp的<a href="http://www.52nlp.cn/lda-math-%E7%A5%9E%E5%A5%87%E7%9A%84gamma%E5%87%BD%E6%95%B01" target="_blank" rel="external">神奇的Gamma函数</a>一文和<a href="http://baike.baidu.com/link?url=l_mNJgJzx9XbJKf4QBI2zBr3SrEzd6pqx1X-smju8t5vFmzB8xx8ynWew0-2dIrOf9iSuIl5wdaJnTlKFGZbs_" target="_blank" rel="external">百度百科－Gamma函数</a>中，讲的非常清楚。这里仅列出背景、核心公式和说明。</p>
<p><br></p>
<h4 id="Gamma函数"><strong>Gamma函数</strong></h4><ul>
<li><strong>历史由来</strong></li>
</ul>
<p>1728年，<a href="http://baike.baidu.com/view/7386144.htm?fromtitle=%E5%93%A5%E5%BE%B7%E5%B7%B4%E8%B5%AB&amp;fromid=1632207&amp;type=syn" target="_blank" rel="external">哥德巴赫</a>（Goldbach C. 德国数学家，1690-1764）在考虑<strong>数列插值</strong>的问题，通俗的说就是把数列的通项公式定义<strong>从整数集合延伸至实数集合</strong>，例如数列1,4,9,16…..可以用通项公式\(n^2\)自然的表达，即便\(n\)为实数的时候，这个通项公式也可以良好定义。</p>
<p>直观地说，就是可以找到一条平滑的曲线\(f(x)=x^2\)通过所有的整数点\([x, x^2]\)，从而可以把定义在整数集上的公式延伸至实数集合。一天哥德巴赫开始处理<strong>阶乘序列</strong>\(1,2,6,24,120,720,\cdots\)，我们可以计算\(2!,3!\),是否可以计算\(2.5!\)呢？</p>
<p>遗憾的是，哥德巴赫当时无法解决<strong>阶乘的计算从整数集合延伸至实数集合上</strong>的问题，于是写信请教尼古拉斯.伯努利和其弟丹尼尔.伯努利，由于欧拉当时与丹尼尔.伯努利在一起，因此也得知了该问题。而欧拉与1729年完美的解决了这个问题，由此导致Gamma函数的诞生，当时的欧拉只有22岁。</p>
<ul>
<li><strong>Gamma函数发现之旅</strong></li>
</ul>
<p>最早发现\(n!\)差值计算的是丹尼尔.伯努利。他发现：如果\(m,n\)都是正整数，且\(m\to\infty\)，有下述公式成立：</p>
<p>$$<br>\frac {1 \cdot 2 \cdot 3 \cdots m} {(1+n)(2+n) \cdots (m-1+n)} \left(m+ \frac{n}{2}\right)^{n-1} \to n!<br>$$</p>
<p>用此<strong>无穷乘积</strong>的方式可以把\(n!\)定义延伸至实数集合。如\(n=2.5,m\)足够大时，上式基本可近似计算出\(2.5!\)的值。</p>
<p>欧拉偶然地发现\(n!\)可用如下无穷乘积表达：</p>
<p>$$<br>\left[\left(\frac{2}{1}\right)^n \frac{1}{n+1}\right] \left[\left(\frac{3}{2}\right)^n \frac{2}{n+2}\right]<br>\left[\left(\frac{4}{3}\right)^n \frac{3}{n+3}\right]<br>\cdots = n!<br>$$</p>
<p>于是他用一些简单的例子做一些计算，寻找其规律。当\(n=1/2\)时，整理上式可得：</p>
<p>$$<br>\left(\frac{1}{2}\right)!=\sqrt{\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots}<br>$$</p>
<p>此式恰好与著名的<a href="http://baike.baidu.com/link?url=FWbPfOA5BqiQxrCJ4KJnedzJRf6blsxbh2qn1XyGlPvUbGXufoVXf4xie6UMnWNUUAH2ewNaziQug2YzHQ9jpK" target="_blank" rel="external">Wallis公式</a>有关。</p>
<p>注：\( \lim\limits_{k\to\infty} \left(\frac{2^{2k}(k!)^2}{2k!}\right)^2 \frac{1}{2k+1} = \frac{\pi}{2} \qquad（Wallis公式）\) </p>
<p>Wallis公式是关于圆周率的无穷乘积的公式，Wallis在1665年使用<strong>插值方法</strong>计算半圆曲线\(y=\sqrt{x(1-x)}\)下的面积（即直径为1的半圆面积）时，发现\(\pi/4\)等于下式：</p>
<p>$$<br>\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots = \frac{\pi}{4}<br>$$</p>
<p>于是欧拉根据Wallis公式得到如下的结果：</p>
<p>$$<br>\left(\frac{1}{2}\right)! = \frac{\sqrt{\pi}}{2}<br>$$</p>
<p>欧拉发现\((\frac{1}{2})!\)中含有\(\pi\)，而\(\pi\)与圆相关的积分有关系。欧拉猜测\(n!\)一定可以用积分形式表示。</p>
<p>Wallis时代微积分还没问世，当时是使用插值方式做推导计算的，但Wallis公式的推导过程基本上是在处理积分\(\int_{0}^{1} x^\frac{1}{2} (1-x)^\frac{1}{2} dx\)。受Wallis启发，欧拉开始考虑如下一般形式的积分：</p>
<p>$$<br>J(e,n)=\int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>这里，\(n\)为正整数，\(e\)为正实数。利用<strong>分部积分</strong>法，可得：</p>
<p>$$<br>J(e,n)=\frac{n}{e+1}J(e+1, n-1)<br>$$</p>
<p>重复迭代上式，可得：</p>
<p>$$<br>J(e,n) = \frac {1\cdot2\cdot3\cdots n}{(e+1)(e+2)\cdots (e+n+1)}<br>$$</p>
<p>于是欧拉得到如下重要的式子：</p>
<p>$$<br>n!=(e+1)(e+2)\cdots (e+n+1) \int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>接下来，欧拉使用一些计算技巧，即取\(e=f/g\) 且 \(f\to1, g\to0\)，然后对上式右边计算极限，得到如下简洁的结果：</p>
<p>$$<br>n! = \int_{0}^{1}(-\log{t})^n dt<br>$$</p>
<p>到此，欧拉成功地把\(n!\)表达为了积分形式！令\(t=e^{-u}\)，可得常见的Gamma函数形式：</p>
<p>$$<br>n! = \int_{0}^{\infty} u^n e^{-u} du<br>$$</p>
<p>注意，此时的\(n\)仍然为正整数，利用上式把<strong>阶乘延伸至实数集</strong>上，就得到Gamma函数的一般形式：</p>
<p>$$<br>\Gamma(x)=\int_{0}^{1} (-\log t)^{x-1}dt = \int_{0}^{\infty} t^{x-1} e^{-t} dt<br>$$</p>
<p>.</p>
<ul>
<li><strong>为什么</strong>\(\Gamma(n)=(n-1)!\)<strong>，而不是</strong>\(\Gamma(n)=n!\)</li>
</ul>
<p>（未完继续…）</p>
<ul>
<li><p>数学家的成就</p>
<ul>
<li><p>哥德巴赫</p>
<blockquote>
<p>哥德巴赫 于1690年3月18日出生于德国（当时是普鲁士）哥尼斯堡的一个富裕家庭。早年在英国牛津大学学习法学。由于喜欢到处旅游，在欧洲各国访问期间结交了伯努利家族，由此对数学产生兴趣。后来又结交了像欧拉等很多著名数学家，并与他们通信交流问题。他在数学上的研究以<strong>数论</strong>为主。</p>
<ol>
<li><p>1728年，提出了实数集上的数列差值问题，该问题在1929年由欧拉解决，并诞生了著名的Gamma函数。</p>
</li>
<li><p>1742年6月7日，在与好友欧拉的一封信中陈述了著名的<strong>哥德巴赫猜想</strong>：任一大于2的偶数都可以写成两个质数之和。</p>
</li>
</ol>
</blockquote>
</li>
<li><p>伯努利</p>
</li>
<li><p>欧拉</p>
<blockquote>
<p>莱昂哈德·欧拉（Leonhard Euler ，1707年4月15日～1783年9月18日），瑞士数学家、自然科学家。1707年出生于瑞士－巴塞尔的牧师家庭，15岁从巴塞尔大学毕业，翌年获得硕士学位。</p>
<p>欧拉是18世纪数学界最杰出的人物之一，不仅在数学界做出伟大贡献，而且把数学应用到了几乎整个物理领域。简述其成就：</p>
<ol>
<li><p><strong>_微分方程_</strong>: 18世纪中叶，欧拉与其它数学家在解决物理问题过程中，创立了<strong>微分方程</strong>学科。偏微分方程的纯数学研究的第一篇论文是欧拉写的《方程的积分法研究》。此外，还提出了<strong>函数用三角级数表示</strong>的方法和<strong>解微分方程的级数法</strong>等。</p>
</li>
<li><p><strong>_微分几何学_</strong>：引入了空间曲线的参数方程，给出了空间曲线曲率半径的解析表达式；1766年出版了《关于曲面上曲线的研究》，建立了<strong>曲面理论</strong>，是微分几何发展史上的里程碑。</p>
</li>
<li><p><strong>_分析学_</strong>：1729年引入了<strong>Gamma函数和Beta函数</strong>，证明了椭圆积分的加法定理，最早引入了<strong>二重积分</strong>。</p>
</li>
<li><p><strong>_数论_</strong>：欧拉的一系列成果奠定了该数学分支。</p>
</li>
</ol>
<p>后人这样评价欧拉：</p>
<p>著名数学家拉普拉斯（Laplace）说：“读读欧拉，他是所有人的老师”。</p>
<p>数学史上公认的4名最伟大的数学家分别是：<strong>阿基米德、牛顿、欧拉和高斯</strong>。阿基米德有“翘起地球”的豪言壮语，牛顿因为苹果闻名世界，高斯少年时就显露出计算天赋，唯独欧拉没有戏剧性的故事让人印象深刻。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="Gamma分布">Gamma分布</h4><ul>
<li><p>概率密度函数:<br>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {x^{\alpha-1} e^{x/\beta}} {\beta^\alpha \Gamma(\alpha)}, &amp; 0 \leq x \leq \infty; \alpha &gt; 0; \beta &gt; 0 \\\<br>\;\;\quad 0, &amp; others<br>\end{cases}<br>$$</p>
</li>
<li><p>概率密度函数</p>
</li>
</ul>
<h3 id="Beta分布（贝塔分布）">Beta分布（贝塔分布）</h3><p>在@rickjin的《LDA-math》系列中详细地解释了Beta和Dirichlet分布的由来和推导过程，堪称经典。拜读后，受益颇深。原文见下面链接：</p>
<ul>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%831" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(1)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%832" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(2)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%833" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(3)</a> <br></li>
</ul>
<p>@rickjin在文中从一个<strong>魔鬼的游戏</strong>开始引入。魔鬼撒旦抓走一人，撒旦说：“你们人类很聪明，而我是很仁慈的，和你玩一个游戏，赢了就可以走，否则把灵魂出卖给我。”</p>
<blockquote>
<p>游戏规则：<br><br>我有一个魔盒，上面有一个按钮，你每按一下按钮，就均匀的输出一个[0,1]之间的随机数，我现在按10下，我手上有10个数，你猜第7大的数是什么，偏离不超过0.01就算对。</p>
</blockquote>
<h4 id="数学抽象与推导">数学抽象与推导</h4><p>上述游戏实际在说随机变量\(X_1, X_2, \cdots, X_{10} \sim U(0,1)\), 把这\(n\)个随机变量排序后得到顺序统计量\(X_{(1)}, X_{(2)}, \cdots, X_{(10)}\)，然后问\(X_{(k)}\)的分布是什么？</p>
<blockquote>
<p>因为如果知道随机变量\(X_{(k)}\)分布的概率密度，用概率密度的极值点作为猜测值是最好的策略。</p>
</blockquote>
<p>对于上述游戏而言，\(n=10,k=7\)，\(X_{(k)}\)的分布如何计算？@rickjin在文中列举了通过尝试计算\(X_{(k)}\)落在区间\([x,x+\Delta x]\)的概率，也就是求如下概率值</p>
<p>$$<br>P(x \leq X_{(k)} \leq x + \Delta x) = \;?<br>$$</p>
<p>如果把[0,1]区间分为三个子区间，即\([0,x)、[x,x+\Delta x]和(x+\Delta x,1]\)。考虑简单的情形，假设\(n\)个数中只有一个落在了区间\([x,x+\Delta x]\)内，这个区间内的数\(X_{(k)}\)是第\(k\)大的，则区间\([0,x)\)中应该有\(k-1\)个数，区间\((x+\Delta x,1]\)中应该有\(n-k\)个数。那么，符合上述要求的事件\(E\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E = \\{<br>&amp; X_1 \in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x)\quad (i=2,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<blockquote>
<p>注：</p>
<ol>
<li>符号<code>*</code>在markdown中有特殊含义，若要当作latex环境中的语法用，需要转义，即<code>\ *</code>.</li>
<li>在Mathjax下，<code>$$E = \{x, y\}$$</code>无法正常输出<code>{}</code>, 因为Latex要想输出<code>{}</code>，需要使用<code>\{, \}</code>。而<code>\</code>在markdown中要想正常表法其自身意义也需要再次转义，代码为: <code>$$E = \\{x,y\\}$$</code>。</li>
</ol>
<p>$$E = \\{ x,y\\}$$</p>
</blockquote>
<p>那么事件\(E\)发生的概率，有：</p>
<p>$$<br>\begin{align*}<br>P(E) &amp; = \prod_{i=1}^nP(X_i) \\\<br>&amp; = x^{k-1}(1-x-\Delta x)^{n-k}\Delta x \\\<br>&amp; = x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>\(o(\Delta x)\)表示\(\Delta x\)的高阶无穷小。显然，由于<strong>不同的排列组合</strong>，即\(n\)个数中有一个落在\([x, x+ \Delta x]\)区间的有\(n\)中取法，余下\(n-1\)个数中有\(k-1\)个落在\([0, x)\)区间有\(\binom{n-1}{k-1}\)种组合，所以<strong>与事件E等概率的事件一共有\(n \binom{n-1}{k-1}\)个（当只有1个数落在\([x, x+ \Delta x]\)时）</strong>。</p>
<p>继续考虑复杂一些的情形，假设\(n\)个数中<strong>两个数</strong>落在了区间\([x, x+ \Delta x]\)，此时事件\(E^{‘}\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E’ = \\{<br>&amp; X_1,X_2\in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x) \quad (i=3,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<p>此时，事件\(E^{‘}\)发生的概率为：</p>
<p>$$<br>P(E^{‘}) = x^{k-2}(1-x-\Delta x)^{n-k}(\Delta x)^2 = o(\Delta x)<br>$$</p>
<p>很容易看出，只要落在\([x, x+ \Delta x]\)内的数字<strong>大于1个</strong>，则对应事件的概率就是\(\Delta x\)。于是随机变量\(X_{(k)}\)落在\([x, x+ \Delta x]\)区间的概率：</p>
<p>$$<br>\begin{align*}<br>&amp; P( x \le X_{(k)} \le x+\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}P(E) + o(\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>进一步可得\(X_{(k)}\)的概率密度函数为</p>
<p>$$<br>\begin{align*}<br>f(x) &amp; = \lim_{\Delta x \to 0} \frac {P(x \leq X_{(x)} \leq x + \Delta x)}{\Delta x} \\\<br>&amp; = n \binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\\\<br>&amp; = \frac {n!}{(k-1)! (n-k)!}x^{k-1} (1-x)^{n-k} \quad x \in [0,1]<br>\end{align*}<br>$$</p>
<p>根据<a href="">神奇的Gamma函数、Beta函数系列</a>可知，利用Gamma函数，可以把上述\(f(x)\)表示为</p>
<p>$$<br>f(x) = \frac {\Gamma(n+1)}{\Gamma(k) \Gamma(n-k+1)} x^{k-1} (1-x)^{n-k}<br>$$</p>
<p><strong>\(\Gamma\)函数起源于人们希望把数学计算从整数集合拓展至实数集合</strong>。这里另\(\alpha=k, \beta=n-k+1\)，于是可以得到</p>
<p>$$<br>Beta(x|\alpha,\beta)=f(x)=\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}<br>$$</p>
<p>上式即为一般意义上的Beta分布！</p>
<h4 id="\(\beta\)型概率分布"><br><strong>\(\beta\)型概率分布</strong></h4><ul>
<li>\(\beta\)型随机变量\(X\)的概率密度函数：</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}, &amp; 0 \leq x \leq 1;\;\alpha &gt; 0, \beta &gt; 0 \\\<br>0, &amp; others<br>\end{cases}<br>\quad (**)<br>$$</p>
<blockquote>
<p>关于<strong>Beta函数详细推导</strong><br></p>
<p>$$<br>\begin{align*}<br>f(x;\alpha,\beta) &amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt} \\\<br>&amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{Beta(\alpha,\beta)}<br>\end{align*}<br>$$</p>
<p>其中，</p>
<p>$$<br>Beta(\alpha,\beta)= \int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}<br>$$</p>
</blockquote>
<ul>
<li>\(\beta\)型随机变量的期望与方差分别是：</li>
</ul>
<p>$$\mu=\frac{\alpha}{\alpha+\beta} \;\qquad\qquad\qquad (期望)$$</p>
<p>$$\sigma^2=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \quad (方差)$$</p>
<blockquote>
<p>关于<strong>Gamma函数</strong></p>
<p>$$<br>\Gamma(\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt<br>$$</p>
<p>且当\(\alpha\)是正整数时，\(\Gamma(\alpha)=(\alpha-1)!\)</p>
</blockquote>
<ul>
<li>\(\beta\)型分布的期望公式详细推导</li>
</ul>
<p>如果\(p \sim Beta(t|\alpha,\beta)\)，那么\(p\)的期望表示如下：</p>
<p>$$<br>\begin{align*}<br>E(p)<br>&amp; =\int_{0}^{1} t \ast Beta(t|\alpha,\beta)\;dt \\\<br>&amp; = \int_{0}^{1} t \ast \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}t^{\alpha-1}(1-t)^{\beta-1}\;dt \\\<br>&amp; =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt<br>\end{align*}<br>$$</p>
<p>上式右边的积分\(\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt\)恰好对应到概率分布\(Beta(t|\alpha+1,\beta)\)。对于此分布，根据Beta函数在\([0,1]\)之间的累积分布函数等于1，可得:</p>
<p>$$<br>\int_{0}^{1} \frac{\Gamma(\alpha+\beta+1)}{\Gamma(\alpha+1)\Gamma(\beta)} t^\alpha (1-t)^{\beta-1}\;dt=1<br>$$</p>
<p>把上式带入\(E(p)\)计算式，得到 </p>
<p>$$<br>\begin{align*}<br>E(p) &amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \cdot<br>\frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \\\<br>&amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+1)}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \notag \\\<br>&amp; = \frac{\alpha}{\alpha+\beta}<br>\end{align*}<br>$$</p>
<p>这说明，<strong>对于Beta分布的随机变量，其均值可以用\(\frac{\alpha}{\alpha+\beta}\)来估计</strong>。</p>
<ul>
<li>\(\beta\)型概率分布曲线</li>
</ul>
<p><strong>_(此处暂略~)_</strong></p>
<p>回到<strong>魔鬼的游戏</strong>中，\(n=10, k=7\)时，按照密度分布的峰值去猜测是最好的策略。</p>
<p>$$<br>f(x)=\frac{10!}{(6)!(3)!} x^6 (1-x)^3 \quad x \in [0,1]<br>$$</p>
<p>即便按照密度函数分布的峰值作为猜测结果，第一次猜中的概率也不高。<br><br></p>
<p><strong>游戏继续\(\cdots\)</strong></p>
<p>很遗憾，根据上述最好的策略算出来的值竟然有偏差，没猜中，魔鬼微笑着说：“我再仁慈一点，再给你一个机会，你按5下这个机器，你就得到了5个\([0,1]\)之间的随机数，然后我可以告诉你这5个数中的每一个，和我的第7个数相比，谁大谁小，然后你继续猜我手头上的第7大的数是多少。” 这时候该如何猜测呢？</p>
<h3 id="Beta-Binomial共轭"><br><strong>Beta-Binomial共轭</strong></h3><p>魔鬼的两个问题，数学抽象一下，就是：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \in Uniform(0,1)\)，对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\)，我们要猜测\(p=X_{(k)}\)；（第1个问题）</li>
<li>\(Y_1,Y_2, \cdots, Y_m \in Uniform(0,1)\)，\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大；（第2个问题）</li>
</ul>
<p>最后的问题：<strong>\(P(p|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</strong></p>
<p><strong>问题分析</strong></p>
<p>由于\(p=X_(k)\)在\(X_1,X_2, \cdots, X_n\)中是第\(k\)大的，利用\(Y_i\)的信息，可以很容易的推理得到\(p=X_{(k)}\)在\(X_1,X_2, \cdots, X_n,\) \(Y_1, Y_2, \cdots, Y_m \in Uniform(0,1)\)这\((m+n)\)个独立随机变量中是第\(k+m_1\)大的。那么按照<strong>Beta分布（贝塔分布）</strong>小节的推理，此时\(p=X_{(k)}\)的概率密度函数是\(Beta(p|\,k+m_1, n-k+1+m_2)\)。</p>
<p>根据<strong>贝叶斯推理</strong>的逻辑，整理上述过程如下：</p>
<ul>
<li>\(p=X_{(k)}\)是我们需要猜测的参数，并且推导出\(p\)的分布为\(f(p)=Beta(p|k,n-k+1)\)，称为\(p\)的<strong>先验分布</strong>；</li>
<li>数据\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大，\(Y_i\)相当于做了\(m\)次<strong>贝努利试验</strong>，所以\(m_1\)服从二项分布\(B(m,p)\)；</li>
<li>在给定了来自数据提供的\((m_1, m_2)\)的知识后，\(p\)的<strong>后验分布</strong>变为:</li>
</ul>
<p>$$f(p|\,m_1,m_2)=Beta(p|\,k+m_1, n-k+1+m_2)$$</p>
<blockquote>
<p>关于<strong>贝叶斯参数估计</strong>的基本过程：</p>
<p>$$先验分布 ＋ 数据知识 ＝ 后验分布$$</p>
</blockquote>
<p>在这里，贝叶斯分析过程的简单直观的表述就是：</p>
<p>$$Beta(p|\,k, n-k+1) + Count(m_1, m_2) = Beta(p|\,k+m_1,n-k+1+m_2)$$</p>
<p>其中\(m_1,m_2\)对应的是二项分布\(B(m_1+m_2, p)\)的计数。更为一般地数学表述：对于非负实数\(\alpha,\beta\)，存在如下关系</p>
<p>$$<br>Beta(p|\,\alpha, \beta) + Count(m_1, m_2) = Beta(p|\,\alpha+m_1, \beta+m_2) \quad (**)<br>$$</p>
<p>该式描述的就是<strong>Beta-Binomial共轭</strong>。</p>
<blockquote>
<p>关于<strong>Beta-Binomial共轭</strong> <br><br>数据符合<strong>二项分布</strong>的时候，参数的先验分布和后验分布都能保持<strong>Beta分布</strong>的形式，这种形式不变的好处是：我们能够在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续至后验分布中进行解释，同时从先验变换到后验的过程中从数据中补充的知识也容易给出物理上的解释。</p>
</blockquote>
<p>推导过程中可以看到，\(Beta\)分布中的参数\(\alpha,\beta\)都可以理解为物理计数，这两个参数经常被称为<strong>伪计数（pseudo-count）</strong>。\(Beta(p|\,\alpha, \beta)\)可以写成如下表达式：</p>
<p>$$Beta(p|\,1,1) + Count(\alpha-1, \beta-1) = Beta(p|\,\alpha,\beta)$$</p>
<p>其中的<strong>\(Beta(p|\,1,1)\)恰好就是均匀分布\(Uniform(0,1)\)</strong>。</p>
<blockquote>
<p>关于<strong>Beta分布与均匀分布</strong>的关系：</p>
<p>$$Beta(p|\,\alpha=1,\beta=1)= Uniform(a=0,b=1)$$</p>
</blockquote>
<p>对于\((**)\)共轭公式，其实可纯粹从贝叶斯的角度来进行推导。理解过程：</p>
<ul>
<li>假设有一个不均匀的硬币抛出正面的概率为\(p\)，抛\(m\)次后出现正面和反面的次数分别是\(m_1,m_2\)，那么按照传统的<strong>频率学派</strong>观点，\(p\)的估计值应该是\(\hat p = \frac{m_1}{m}\)。<br></li>
<li>而从<strong>贝叶斯学派</strong>的观点来看，开始对硬币不均匀性一无所知，所以应该假设\(p\ \in Uniform(0,1)\)，于是二项分布的计数\(m_1,m_2\)之后，按照贝叶斯公式计算\(p\)的后验分布：</li>
</ul>
<p>$$<br>\begin{align*}<br>P(p|\,m_1,m_2) = &amp; \frac {P(p) \cdot P(m_1,m_2\,|p)}{P(m_1,m_2)} \\\<br>= &amp; \frac {1 \cdot P(m_1,m_2 |p)}{\int_{0}^{1} P(m_1,m_2|t)dt} \\\<br>= &amp; \frac {\binom{m}{m_1}p^{m_1}(1-p)^{m_2}}{\int_{0}^{1} \binom{m}{m_1}t^{m_1}(1-t)^{m_2}dt} \\\<br>= &amp; \frac {p^{m_1}(1-p)^{m_2}}{\int_{0}^{1}t^{m_1}(1-t)^{m_2}dt}<br>\end{align*}<br>$$</p>
<p>得到的后验分布正好是\(Beta(p|\;m_1 + 1, m_2 + 1)\)。<strong>_这里真心没看太懂 …_</strong></p>
<p>回到<strong>魔鬼的游戏</strong>，如果按出的5个随机数字中，魔鬼告诉你有2个（即\(m_1=2\)）小于他手中第7大的数，那么应该按照如下概率分布的<strong>峰值</strong>做猜测是最好的(\(\alpha+2=9,\beta+3=7\))：</p>
<p>$$<br>Beta(x|9,7)=\frac{15!}{(8)!(6)!}x^8(1-x)^6  \quad x \in [0,1]<br>$$</p>
<p>很幸运，这次猜中了，但是魔鬼开始耍赖，游戏不得不继续 \(\cdots\)</p>
<blockquote>
<p>游戏3新规则：<br><br>魔说道：“这个游戏对你来说太简单了，我要加大点难度，我们重新来一次，<strong>我按20下生成20个随机数，你要同时给我猜第7大和第13大的数是什么？</strong>”</p>
</blockquote>
<p>这时候又该如何猜测呢？<br></p>
<h3 id="Dirichlet分布（狄利克雷分布）"><strong>Dirichlet分布（狄利克雷分布）</strong></h3><p>对于魔鬼变本加厉的新的游戏规则，数学抽象如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\),</li>
<li>排序后对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\),</li>
<li><strong>问: \((X_{(k_1)},X_{(k_1 + k_2)})\)的联合分布是什么？</strong></li>
</ul>
<p>游戏3完全类似游戏1的推导过程，可进行如下的概率计算：</p>
<blockquote>
<p>注：为了公式的简洁性，这里取\(x_3=1-x_1-x_2\),但只有\(x_1,x_2\)是变量。说明符号：<br></p>
<ol>
<li>n: 总的样本数<br></li>
<li>\(X_{(k_1)}\)和\(X_{(k_1 + k_2)}\)将n分割为3段，长度分别为\(k_1, k_2和k_3\)，即<br></li>
</ol>
<p>$$<br>分段结果：<br>\begin{cases}<br>[X_{(1)}, X_{(k-1)}], &amp; 有(k_1-1)个取值，统一记为x_1 \\\<br>[X_{(k_1+1)}, X_{(k_1+k_2)}], &amp; 有(k_2-1)个取值，统一记为x_2 \\\<br>[X_{(k_1+k_2+1)}, X_{(n)}] &amp; 有(n-k_1-k_2)个取值，统一记为x_3<br>\end{cases}<br>$$</p>
</blockquote>
<p>完全类似于游戏1的推导过程，\(X_{(k_1)},X_{(k_1+k_2)}\)的<strong>联合概率</strong>计算如下：</p>
<p>$$<br>\begin{align*}<br>&amp; P\Bigl(X_{(k_1)} \in (x_1, x_1+\Delta x), X_{(k_1+k_2)} \in (x_2, x_2+\Delta x)\Bigr) \\\<br>&amp; \quad = n(n-1)\binom{n-2}{k_1-1,k_2-1}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2 \\\<br>&amp; \quad = \frac{n!}{(k_1-1)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2<br>\end{align*}<br>$$</p>
<blockquote>
<p>式中的\(n(n-1)\binom{n-2}{k_1-1,k_2-1}\)是一个排列组合问题，比较容易理解。</p>
</blockquote>
<p>于是得到\(X_{(k_1)},X_{(k_1+k_2)}\)的联合分布是：</p>
<p>$$<br>\begin{align*}<br>f(x_1,x_2,x_3) &amp; =\frac{n!}{(k_1-)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2} \\\<br>&amp; = \frac{\Gamma(n+1)}{\Gamma(k_1)\Gamma(k_2)\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}<br>\end{align*} \quad (**)<br>$$</p>
<p>而公式\((**)\)则是3维形式的Dirichlet分布，即\(Dir(x_1,x_2,x_3\;|\;k_1,k_2,n-k_1-k_2+1)\)。这里令\(\alpha_1=k_1,\alpha_2=k_2,\alpha_3=n-k_1-k_2+1\)，于是分布密度函数可以写为：</p>
<p>$$<br>f(x_1, x_2, x_3) = \frac{\Gamma(\alpha_1+\alpha_2+\alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}x_2^{\alpha_2-1}x_3^{\alpha_3-1} \qquad (0)<br>$$</p>
<p>公式\((0)\)即为一般形式的3维Dirichlet分布。即使\(\vec{\alpha}=(\alpha_1,\alpha_2,\alpha_3)\)<strong>延伸至非负实数集合</strong>，上述概率分布也是成立的。</p>
<p>在游戏3的基础上还可以往更高的维度上推，譬如\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)中的4、5、…等更多个数，可以得到更高维度的Dirichlet分布和Dirichlet-Multinomial共轭。</p>
<ul>
<li>Dirichlet分布</li>
</ul>
<p>如果\(\vec{p} \sim Dir(\vec{t}|\vec{\alpha})\),那么随机变量\(\vec{p}\)的概率密度函数为</p>
<p>$$<br>\begin{equation}<br>\displaystyle Dir(\overrightarrow{p}|\overrightarrow{\alpha}) =<br>\displaystyle \frac{\Gamma(\sum_{k=1}^{K} \alpha_k)}<br>{\prod_{k=1}^K \Gamma(\alpha_k)} \prod_{k=1}^{K} p_k^{\alpha_k -1}<br>\end{equation}    \qquad (1)<br>$$</p>
<p>对于给定的\(\overrightarrow{p}\)和\(N\)，多项式分布定义为：</p>
<p>$$<br>Multi(\overrightarrow{n}|\overrightarrow{p},N)=<br>\binom{N}{\overrightarrow{n}} \prod_{k=1}^K p_k^{n_k} \qquad (2)<br>$$</p>
<p>而\(Multi(\overrightarrow{n}|\overrightarrow{p},N)\)和\(Dir((\overrightarrow{p}|\overrightarrow{\alpha})\)这两个分布是共轭关系。</p>
<ul>
<li>Dirichlet分布均值</li>
</ul>
<p>$$<br>E(\vec{p}) = \Bigl(\frac{\alpha_1}{\sum_{i=1}^{K}\alpha_i},\frac{\alpha_2}{\sum_{i=1}^{K}\alpha_i}, \cdots, \frac{\alpha_K}{\sum_{i=1}^{K}\alpha_i}\Bigr)<br>$$</p>
<blockquote>
<p>关于<strong>Dirichlet分布与Beta分布</strong>：<br><br>Dirichlet分布是Beta分布在高维度上的推广。当Dirichlet分布维度趋向无限时，便成为Dirichlet过程。<br><br></p>
</blockquote>
<h3 id="Dirichlet-Multinomial共轭"><strong>Dirichlet-Multinomial共轭</strong></h3><p>类似于魔鬼的游戏2，调整一下游戏3，可得游戏4的规则如下：</p>
<blockquote>
<p>游戏4: <br><br>从魔盒中生成m个随机数\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)，魔鬼告诉我们\(Y_i\)和\(X_{(k_1)},X_{(k_1+k_2)}\)相比谁大谁小，然后再次猜测第7大和第13大的数是多少？</p>
</blockquote>
<p>同样，对游戏4进行数学抽象，表示如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\)，排序后对应的顺序统计量为\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)；</li>
<li>令\(p_1=X_{(k_1)},p_2=X_{(k_1+k_2)},p_3=1-p_1-p_2(p_3是为了简洁的数学表达)\)，我们要猜测\(\vec{p}=(p_1,p_2,p_3)\)；</li>
<li>\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\),\(Y_i\)中落到\([0,p_1), [p_1,p_2),[p_2,1]\)三个区间的个数分别是\(m_1,m_2,m_3\),其中\(m=m_1+m_2+m_3\)；</li>
<li>问后验分布\(P(\vec{p}|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</li>
</ul>
<p>为了计算方便，记：</p>
<p>$$<br>\vec{m}=(m_1, m_2, m_3), \quad \vec{k}=(k_1,k_2,n-k_1-k_2+1)<br>$$</p>
<p>从游戏中的信息可以得知：\(p_1,p_2\)在\(X_1,X_2, \cdots, X_n,Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)这\(m+n\)个数中分别成为了第\(k_1+m_1, k_2+m_2\)大的数，于是后验分布\(P(\vec{p}|Y_1,Y_2,\cdots, Y_m)\)应该是\(Dir(\vec{p}|k_1+m_1,k_2+m_2, n-k_1-k_2+1+m_3)\)，即\(Dir(\vec{p}|\vec{k}+\vec{m})\)。按照贝叶斯推理的逻辑，同样可以把上述过程整理如下：</p>
<ul>
<li>首先，根据游戏3需要猜测参数\(\vec{p}=(p_1,p_2,p_3)\)，其先验分布为\(Dir(\vec{p}|\vec{k})\)；</li>
<li>其次，数据\(Y_i\)落到\([0,p_1),[p_1,p_2),[p_2,1]\)三个区间的个数分别为\(m_1,m_2,m_3\)，所以\(\vec{m}=(m_1, m_2, m_3)\)服从多项式分布\(Multi(\vec{m}|\vec{p})\)<strong>（理解这一点很重要）</strong>；</li>
<li>在给定了来自数据提供的知识\(\vec{m}\)后，\(\vec{p}\)的后验分布变为\(Dir(\vec{p}|\vec{k}+\vec{m})\)。</li>
</ul>
<p>以上贝叶斯分析过程的最简单直接的表述：</p>
<p>$$<br>Dir(\overrightarrow{p}|\overrightarrow{k})+Multi(\vec{m})=Dir(\vec{p}|\vec{k}+\vec{m})<br>$$</p>
<p>令\(\overrightarrow{\alpha}=\vec{k}\)，把\(\vec{\alpha}\)从整数集合延拓至实数集合，可以证明如下关系仍然成立：</p>
<p>$$<br>Dir(\vec{p}|\vec{\alpha})+Multi(\vec{m})=Dir(\vec{p}|\vec{\alpha}+\vec{m}) \quad (11)<br>$$</p>
<p>公式\((11)\)描述就是<strong>Dirichlet-Multinomial共轭</strong>。同时，我们可以把Dirichlet分布中的\(\alpha\)都可以理解为物理计数。那么，类似于Beta分布，\(Dir(\vec{p}|\vec{\alpha})\)可做如下分解：</p>
<p>$$<br>Dir(\vec{p}|\vec{1})+Multi(\vec{m}-\vec{1}) = Dir(\vec{p}|\vec{\alpha})<br>$$</p>
<p>这里\(\vec{1}=(1,1,\cdots,1)\),上式同样可以类似的用纯粹贝叶斯的观点推导和解释。</p>
<p>数学</p>
<hr>
<p>表格(改日修改语法)</p>
<p>$$<br>\begin{array}{c|lcr}<br>n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\<br>\hline<br>1 &amp; 0.24 &amp; 1 &amp; 125 \\<br>2 &amp; -1 &amp; 189 &amp; -8 \\<br>3 &amp; -20 &amp; 2000 &amp; 1+10i \\<br>\end{array}<br>$$</p>
<p>$$<br>f(x)=<br>\begin{cases}<br>n/2, &amp; \text{if \(n\) is even} \\<br>3n+1, &amp; \text{if \(n\) is odd}<br>\end{cases}<br>$$</p>
<p>f(x)=<br>\begin{cases}<br>n/2, &amp; \text{if \(n\) is even} \\\<br>3n+1, &amp; \text{if \(n\) is odd}<br>\end{cases}</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/概率与统计/">概率与统计</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/连续随机变量/">连续随机变量</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/computational_ads/ca_roadmap/" title="计算广告学-RoadMap" itemprop="url">计算广告学-RoadMap</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-05-09T14:31:24.000Z" itemprop="datePublished"> 发表于 2015-05-09</time>
    
  </p>
</header>
    <div class="article-content">
        
        <blockquote>
<p>Author: zhouyongsdzh@foxmail.com </p>
<p>Date: 2015-05-09</p>
</blockquote>
<p>说明</p>
<p>这里阐述了在线广告介绍以及产品形态、用户画像、大规模约束优化、CTR预估、流量预估、竞价机制设计、广告拍卖等计算广告学核心概念和技术进行总结与梳理</p>
<h2 id="第一部分：广告与计算">第一部分：广告与计算</h2><ul>
<li>第01章：在线广告介绍</li>
<li>第02章：计算广告基础知识</li>
<li>第03章：受众定向</li>
</ul>
<h2 id="第二部分：合约广告系统">第二部分：合约广告系统</h2><ul>
<li>第04章：流量预估</li>
<li>第05章：在线分配</li>
</ul>
<h2 id="第三部分：竞价广告系统">第三部分：竞价广告系统</h2><ul>
<li>第06章：匹配与相关性计算</li>
<li>第07章：广告检索</li>
<li>第08章：点击率预估</li>
<li>第09章：竞价机制设计</li>
</ul>
<h2 id="第四部分：广告交易系统">第四部分：广告交易系统</h2><ul>
<li>第10章：交易与拍卖理论</li>
<li>第11章：在线广告生态系统</li>
<li>第12章：需求方平台</li>
<li>第13章：供给方平台</li>
<li>第14章：数据管理平台</li>
</ul>
<h2 id="第五部分：广告智能推荐">第五部分：广告智能推荐</h2><ul>
<li>第15章：推荐与广告</li>
<li>第16章：推荐系统与算法</li>
</ul>
<h2 id="第六部分：广告相关技术">第六部分：广告相关技术</h2><ul>
<li>第17章：智能频次控制</li>
<li>第18章：反作弊技术</li>
<li>第19章：效果监控与可视化</li>
<li>第20章：隐私保护与数据安全</li>
</ul>
<h2 id="第七部分：大数据与机器学习">第七部分：大数据与机器学习</h2><ul>
<li>第21章：大数据框架在广告系统中的应用</li>
<li>第22章：机器学习在广告系统中的应用</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/计算广告学/">计算广告学</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/计算广告学/">计算广告学</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/computational_ads/ca-chapter1.1-introduction/" title="Chapter1.1 网络广告的前世今生" itemprop="url">Chapter1.1 网络广告的前世今生</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-04-20T14:54:37.000Z" itemprop="datePublished"> 发表于 2015-04-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>Author: zhouyongsdzh@foxmail.com </li>
<li>Date: 2015-04-20</li>
</ul>
<h3 id="写在前面"><strong>写在前面</strong></h3><p>计算广告学从字面上理解主要包括两个方面，一个是广告，另一个是计算。如果在20年甚至10年之前，人们也许想不到计算与广告也会发生关系。然而在互联网时代快速变迁的今天，以大数据和人工智能为核心的互联网时代，不仅加剧对传统行业的改造，同时也改变了互联网自身的商业变现模式与规则。</p>
<p>我们先来看看广告是什么？有人说是市场营销的一种手段，是商业推广最给力的工具之一等等。 一言以蔽之，广告的背后是商业及其内在的规模。</p>
<p>广告里的计算又是什么呢？包括哪些内容呢？这里的计算不仅包括大规模的数据处理、数据分析、数据存储、数据挖掘、机器学习、海量数据计算（离线计算、流式计算、内存计算），同时也涉及到博弈、竞价、拍卖、交易等经济学范畴的综合体。</p>
<p>下面，我们分别介绍这两部分包含的内容。</p>
<h3 id="1-1_网络广告分类">1.1 网络广告分类</h3><p>按计费方式、广告创意、投放方式、广告产品形态等分类</p>
<h3 id="1-2_网络广告发展史">1.2 网络广告发展史</h3><p>1994年开始 -&gt; 展示、搜索 -&gt; 技术驱动 -&gt; 当前广告生态圈</p>
<p>世界广告的发展史可以追溯至几千年前。而网络广告的媒介是互联网，互联网的发展从1969年至今还未到半个世纪，因此网络广告的历史就更短了。</p>
<p>第一个网络广告何时、何地出现的呢？追本溯源，网络广告起源于美国。1994年10月27日是网络广告发展史上的里程碑，当时著名的商业网络杂志Hotwired推出了网络版的Hotwired（注：美国科技新闻评论网，现更名为Wired，2006年7月11日被Conde Nast Publishing收购），并首次在网站上推出了网络广告，在当时立即吸引了AT&amp;T等14个客户在其主页上发布广告，10月27日，AT&amp;T的一个468*60的Banner广告出现在页面上时，标志着网络广告正式诞生。更值得一提的是，该广告的点击率达到40%。</p>
<p>像AT&amp;T这种在网站的页面上展示banner广告创意的产品形式，这种投放方式称为<strong>展示广告（Display Advertising）</strong>。虽然是在网络这个新媒介上展示，但基本延续了传统广告投放的那一套思路和逻辑。大致过程如下：4A广告代理公司（如奥美等）把网站上的HTML页面当作杂志的版面，在里面设置广告位，然后把广告的创意（图片方式）嵌入到广告位中。网站在广告代理商眼里就相当于一本本的杂志，采买的思路和逻辑与线下的报刊、杂志基本无异。</p>
<p>还有一个非常重要的关键点，就是广告的售卖模式。想一下，广告主与传统的广告媒介（如电视、报刊、杂志等）是如何谈费用的？双方谈判的其中一个焦点就是广告投放位置、广告投放时间、每个时间单位（按天、周、月）的费用、广告投放的页面中是否可包括竞品的推广信息等，一切细节等谈妥后以合约的方式确定下来。像这种采用合同约定的方式确定某一广告位在某段时间内为某特定广告主所独占，确定广告创意允许范围以及同一页面上的广告排它策略等，这类售卖模式称为<strong>合约式广告（Agreement-based Advertising）</strong>。展示广告的发展在很长一段时间内都在采用传统广告的这一套售卖模式，至今为止这种售卖模式仍旧存在。</p>
<p>该阶段的广告虽然是按照时间单位计费，但是不同流量规模的网站在相同的时间单位内的费用自然也不同。一个每天拥有10亿流量的网站和另一个每天只有1亿流量的网站，如果广告主分别在这两个网站都投放1个月的广告，前者应该支付更高的费用。那个到底该如何去计费呢？业内普遍采用一个指标：千人成本。即1千个人看到展示的广告后，广告主应该向网站支付的广告成本，称为<strong>CPM计费（Cost Per Mille）</strong>。计算公式如下：</p>
<p>CPM =（广告费用/到达人数）×1000</p>
<p>在网络广告发展初期，当时比较大的在线媒体，如AOL（美国在线）、雅虎等网站，已经有了不错的流量规模，它们采用展示广告＋合约售卖的方式，给自身带来了相当可观的营收和利润。那么，随着流量的增长和品牌认知度的提高，通过提高广告位报价的方式，可以保证这些互联网公司业绩的持续增长。但是当网站的流量规模和品牌认知度趋于稳定时，广告主很难再接受广告位的提价，那这些互联网企业又该如何保证业绩的增长呢？</p>
<p>事情发展到此时，貌似出现了瓶颈。但是不久之后，互联网广告的运营者们经过探索，发现了在线广告不同于传统媒体广告的一个本质特点：<strong>同一个广告位可以对不同的受众（Audience）呈现不同的广告创意</strong>。</p>
<blockquote>
<p>举例：两个广告A和B，A广告的定位人群是男性，B广告的定位人群是女性。广告位P每天touch的流量有100W，其中男女比例1:1，各50万。</p>
<p>按照之前的方式，广告位P只能展示一种广告（非A即B），给广告主带来的收益为1W。但是不管P展示的A还是B，对于广告主而言只能得到50W的有效流量，另外50W对广告主没有意义。因为任何一个产品都有自己的受众人群，非受众人群对该产品很少带来转化。</p>
<p>现在同一个广告位可以针对不同的人群展示不同的广告创意，判断如果该流量对应的用户是男性，给其展示A广告；如果是女性，展示B广告。</p>
<p>此时对于A、B广告主而言，都获得了50W的有效流量。对于任意一个广告主而言，之前100W流量需要1W的费用，现在的流量虽然是50W，但有效流量的比重在提升，费用应该在(5K, 1W]之间，假设是6K。那么网站从广告主那里拿到的总费用为1.2W。</p>
<p>对于网站而言，流量没有变化，但是通过计算用户性别（属于受众定向范畴，第3章详细介绍）的方式可以让收益从1W增加到1.2W。</p>
</blockquote>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/计算广告学/">计算广告学</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/计算广告学/">计算广告学</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/OpenMIT/" title="OpenMIT">OpenMIT<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/分布式机器学习/" title="分布式机器学习">分布式机器学习<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/强化学习与智能决策/" title="强化学习与智能决策">强化学习与智能决策<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/概率与统计/" title="概率与统计">概率与统计<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/深入浅出机器学习/" title="深入浅出机器学习">深入浅出机器学习<sup>9</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习/" title="深度学习">深度学习<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Agent/" title="Agent">Agent<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/参数服务器/" title="参数服务器">参数服务器<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/反向传播/" title="反向传播">反向传播<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Softmax/" title="Softmax">Softmax<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Pooling/" title="Pooling">Pooling<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Attention/" title="Attention">Attention<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Iteration/" title="Policy Iteration">Policy Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Value-Iteration/" title="Value Iteration">Value Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Greedy-Policy/" title="Greedy Policy">Greedy Policy<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/连续随机变量/" title="连续随机变量">连续随机变量<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/MDP/" title="MDP">MDP<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Markov-Decision-Process/" title="Markov Decision Process">Markov Decision Process<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/RL/" title="RL">RL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Environments/" title="Environments">Environments<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/GD/" title="GD">GD<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/FTRL/" title="FTRL">FTRL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaGrad/" title="AdaGrad">AdaGrad<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaDelta/" title="AdaDelta">AdaDelta<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Adam/" title="Adam">Adam<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello, Welcome to CAML technology sharing platform.  <br/>
			I&#39;m Zhou Yong, engaged in algorithms work on computational advertising and machine learning.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1707438033" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/ComputationalAds" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:zhouyongsdzh@foxmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2019 
		
		<a href="/about" target="_blank" title="ZhouYong">ZhouYong</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
 </html>
