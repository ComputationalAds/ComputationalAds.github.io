
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>计算广告与机器学习</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="ZhouYong">
    

    
    <meta name="description" content="广告是流量变现的主要手段！鄙站主要梳理在线广告的主要产品形态，并就不同产品形态中的涉及的核心技术，尤其是大规模机器学习、投放推荐策略、基础数据挖掘以及博弈论（机制、均衡、拍卖）等方面，分享一些解决方案、经验教训和走过的坑儿！">
<meta property="og:type" content="article">
<meta property="og:title" content="计算广告与机器学习">
<meta property="og:url" content="http://www.52caml.com/page/2/index.html">
<meta property="og:site_name" content="计算广告与机器学习">
<meta property="og:description" content="广告是流量变现的主要手段！鄙站主要梳理在线广告的主要产品形态，并就不同产品形态中的涉及的核心技术，尤其是大规模机器学习、投放推荐策略、基础数据挖掘以及博弈论（机制、均衡、拍卖）等方面，分享一些解决方案、经验教训和走过的坑儿！">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="计算广告与机器学习">
<meta name="twitter:description" content="广告是流量变现的主要手段！鄙站主要梳理在线广告的主要产品形态，并就不同产品形态中的涉及的核心技术，尤其是大规模机器学习、投放推荐策略、基础数据挖掘以及博弈论（机制、均衡、拍卖）等方面，分享一些解决方案、经验教训和走过的坑儿！">

    
    <link rel="alternative" href="/atom.xml" title="计算广告与机器学习" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="计算广告与机器学习" title="计算广告与机器学习"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="计算广告与机器学习">计算广告与机器学习</a></h1>
				<h2 class="blog-motto">Computational Advertising and Machine Learning</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/home">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:www.52caml.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/deep_learning/dl-chapter5-word-embedding/" title="第05章：Word Embedding" itemprop="url">第05章：Word Embedding</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2017-06-22T13:43:20.000Z" itemprop="datePublished"> 发表于 2017-06-22</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2017-06-08</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p>目录</p>
<ul>
<li><a href="#5.0.写在前面">写在前面</a></li>
<li><a href="#5.1.词向量">词向量</a></li>
</ul>
<h2 id="5.0.写在前面">写在前面</h2>

<p>在第四章：循环神经网络一章中讲到了神经语言模型，神经语言模型第一层是embedding映射，然后是全连接操作。embedding映射就是将一个词（CTR预估模型中经常是ID型特征）映射为一个$k$维的实数值向量。</p>
<p>在NLP任务中，我们将自然语言交给机器学习（或深度学习）算法来处理，但机器无法直接理解这些数据或语言，因此首先要做的是<strong>将语言数学化</strong>，如何对自然语言进行数学化呢？词向量是一种很好的方式。</p>
<h2 id="5.1.词向量">词向量</h2>

<p>通常词向量有两种表示方式，一种是One-hot Representation，另一种是Distributed Representation。</p>
<p>One-hot比较好理解，不过多赘述，它有一些缺点，如容易受维数灾难的困扰，尤其是将其用于深度学习场景时；在NLP场景中，它还不能刻画词与词之间的相似性。</p>
<p>Distributed Representation最早是Hinton于1986年提出的，它可以克服one hot表示的缺点，其基本思想：<strong>通过训练将某种语言中的每一个词映射成一个固定长度的向量</strong>，所有的向量构成词向量空间，而每一向量可视为该空间中的一个点，在这个空间上引入“距离”，就可以根据词之间的距离来判断它们之间相似性了。</p>
<blockquote>
<p>为什么叫Distributed Representation（分布式表示）呢？一个理解是：对于one-hot presentation，向量中只有一个非零值，非常集中；而对于Distributed Representation表示的向量有大量非零值，相对分散，把词的信息分布到各个分量中去了。</p>
<p>注意：词向量只是针对“词”来提的，事实上，我们也可以针对更细粒度或更粗粒度进行推广，如字向量（charactor-level），句子向量或文档向量等。</p>
</blockquote>
<p>目前，<strong>词向量学习方法中比较主流有word2vec，glove，fasttext和elmo，还有基于char-level的Bi-LSTM方法、CNN＋Highway NN + LSTM等方法</strong>。我们先从word2vec开始介绍。</p>
<p>word2vec有两种不同的建模方式，分别称为CBOW模型和Skip-gram模型。</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/deep_learning/dl_chapter5_cbow_skip_gram.png" width="600" height="400" alt="CBOW和Skip-gram"></p>
<p>由图可见，两个模型都包含3层结构：输入层、映射层和输出层。cbow模型是在已知当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的前提下预测当前词$w_t$；skip-gram模型恰恰相反，是在遗址当前词$w_t$的前提下，预测其上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$。</p>
<p>对于这两个模型结构，word2vec论文中提供了两套学习框架，分别是Herarchical Softmax和Negative Sampling来进行学习。</p>
<p>小结：<strong>两种建模方式对应着两种不同的条件概率，学习框架具体地表示了条件概率函数</strong>。</p>
<h2 id="5.2.word2vec模型">word2vec模型</h2>

<h3 id="5.2.1.基于Hierarchical Softmax算法的word2vec模型">基于Hierarchical Softmax算法的word2vec模型</h3>

<p>在第4章中我们提到，基于神经网络的语言模型的目标函数通常优化其<strong>对数似然函数</strong>：</p>
<p>$$<br>\mathcal{L} = \sum_{w \in C} \log p(w | context(w))<br>$$</p>
<p>其中的关键是<strong>条件概率函数$p(w | context(w))$的构造</strong>，n-gram模型中给出了一种构造方法。对于word2vec中基于分层Softmax的cbow模型，优化的目标函数也如上式；而对于基于分层softmax的Skip-gram模型，优化的目标函数则形如：</p>
<p>$$<br>\mathcal{L} = \sum_{w \in C} \log p(context(w) | w)<br>$$</p>
<p>因此，下面我们重点关注两个条件概率模型是如何构造的？</p>
<p><strong>cbow模型结构</strong></p>
<p>基于HS的cbow模型的网络结构示意图如下：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/deep_learning/dl_chapter5_hs_cbow1.png" width="500" height="450" alt="基于HS的CBOW模型"></p>
<ul>
<li>输入层</li>
</ul>
<p>包含context(w)中2c个词的词向量$v(context(w)_1),v(context(w)_2),\cdots,v(context(w)_{2c}) \in R^k$，这里$k$表示词向量的长度。</p>
<ul>
<li>映射层</li>
</ul>
<p>cbow投影层比较简单，将输入层的2c个词向量做求和累加，即$x_w = \sum_{i=1}^{2c} v(context(w)_i) \in R^k$</p>
<ul>
<li>输出层</li>
</ul>
<p>输出层对应一颗二叉树，它是<strong>以语料中出现过的词当叶子节点，以各词在语料中出现的次数当权重构造出来的Huffman树</strong>。这棵Huffman树中，叶子节点共有N个，分别对应词典$\mathcal{D}$中的词，非叶子节点N-1个。</p>
<p>相对于第4章的神经网络语言模型结构，基于cbow模型的结构有3点不同：1.输入层到映射层的操作，前者是concat，后者是sum；2. 前者有隐藏层，后者无隐藏层；3. 输出层方面，前者是线性模型，后者是树形结构（Huffmans树）。</p>
<p><strong>cbow模型表达</strong></p>
<p>Herarchical Softmax是提高word2vec模型训练性能的重要技术，为了描述方便，先定义一些符号，假设huffman树中的某个叶子节点对应词典$\mathcal{D}$中的词$w$，定义：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>$p^{w}$</td>
<td>从根节点出发到达$w$叶子节点的对应路径</td>
</tr>
<tr>
<td>$l^w$</td>
<td>路径$p^w$中包含节点的个数</td>
</tr>
<tr>
<td>$p_1^w,p_2^w,\cdots,p_{l^w}^{w}$</td>
<td>路径$p^w$中的$l^w$个节点，其中$p_1^w$表示根节点，$p_{l^w}^w$表示词$w$对应的节点</td>
</tr>
<tr>
<td>$d_2^w,d_3^w,\cdots,d_{l^w}^w \in \{0,1\}$</td>
<td>词$w$的huffman编码，它由$l^w-1$位编码构成，$d_j^w$表示路径$p^w$中第$j$个节点对应的编码（根节点不对应编码）</td>
</tr>
<tr>
<td>$\theta_1^w,\theta_2^w,\cdots,\theta_{l^w - 1}^w \in R^k$</td>
<td>路径$p^w$中<strong>非叶子节点</strong>对应的向量，$\theta_j^w$表示路径$p^w$中第$j$个非叶子节点对应的向量。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>word2vec模型最后得到的是词典$\mathcal{D}$中每个词的向量，这里为什么还要为huffman树非叶子节点也定义个向量呢？这里它只是辅助向量，相当于中间变量。</p>
</blockquote>
<p>// TODO 补充</p>
<p>基于HS的cbow模型的条件概率函数为： </p>
<p>$$<br>p(w|context(w)) = \prod_{j=2}^{l^w} p(d_j^w | x_w, \theta_{j-1}^w)<br>$$</p>
<p>其中</p>
<p>$$<br>p(d_j^w | x_w, \theta_{j-1}^w) =<br>\begin{cases} \sigma(x_w^T \theta_{j-1}^w), \quad &amp; d_j^w = 0, \\\<br>1-\sigma(x_w^T \theta_{j-1}^w) \quad &amp; d_j^w=1<br>\end{cases}<br>$$</p>
<p>写成整体的表达式为:</p>
<p>$$<br>p(w|context(w)) = \prod_{j=2}^{l^w} \left[ \sigma(x_w^T \theta_{j-1}^w) \right]^{1-d_j^w} \cdot \left[ 1-\sigma(x_w^T \theta_{j-1}^w) \right]^{d_j^w}<br>$$</p>
<blockquote>
<p>注意：这里是基于HS的cbow模型表达式，笔者经常用该题目考察候选人对word2vec的理解程度，但仅有十之一二能给出正确答案。</p>
</blockquote>
<p><strong>梯度计算</strong></p>
<p>那么<strong>基于HS的cbow模型的目标函数（似然函数的对数形式）</strong>整理为：</p>
<p>$$<br>\begin{align}<br>\mathcal{L} &amp;= \sum_{w \in \mathcal{C}} \log p(w|context(w)) \\\<br>&amp;= \sum_{w \in \mathcal{C}} \sum_{j=2}^{l^w} \left\{ (1 - d_j^w) \log\left(\sigma(x_w^T \theta_{j-1}^w)\right) + d_j^w \log \left(1-\sigma(x_w^T \theta_{j-1}^w) \right) \right\}<br>\end{align}<br>$$</p>
<p>优化目标中的参数有$x_w, \theta_{j-1}^w$。如何最大化目标函数，word2vec工具使用的是随机梯度提升法，前提先算法出优化目标对参数的梯度表达式。为公式推导方便，即花括号里的公式为$\mathcal{L}(w, j)$，分别对参数$x_w, \theta_{j-1}^w$求导：</p>
<p>$$<br>\begin{align}<br>\frac{\partial \mathcal{L}(w,j)}{\partial \theta_{j-1}^w}<br>&amp;= \frac{\partial}{\partial \theta_{j-1}^w} \left\{ (1 - d_j^w) \log\left(\sigma(x_w^T \theta_{j-1}^w)\right) + d_j^w \log \left(1-\sigma(x_w^T \theta_{j-1}^w) \right) \right\} \\\<br>&amp;= (1-d_j^w) \left(1-\sigma(x_w^T \theta_{j-1}^w) \right)x_w - d_j^w \sigma(x_w^T \theta_{j-1}^w)x_w \\\<br>&amp;= \left(1 - \sigma(x_w^T \theta_{j-1}^w) - d_j^w \right) x_w<br>\end{align}<br>$$</p>
<p>于是，参数$\theta_{j-1}^w$的更新公式为：</p>
<p>$$<br>\theta_{j-1}^w \leftarrow \theta_{j-1}^w + \eta \left(1 - \sigma(x_w^T \theta_{j-1}^w) - d_j^w \right) x_w<br>$$</p>
<p>对于参数$x_w$的梯度，可以根据变量的对称性（两者可交换位置）得到。即相应的梯度$\frac{\partial \mathcal{L}(w,j)}{\partial x_w}$只需在梯度$\frac{\partial \mathcal{L}(w,j)}{\partial \theta_{j-1}^w}$的基础上对两个参数交换位置就可以了，即</p>
<p>$$<br>\frac{\partial \mathcal{L}(w,j)}{\partial x_w} = \left(1 - \sigma(x_w^T \theta_{j-1}^w) - d_j^w \right) \theta_{j-1}^w<br>$$</p>
<p>参数$x_w$的更新公式类似$\theta_{j-1}^w$的公式，不再赘述。那么，对每个词的向量$v(\hat{w}), \hat{w} \in context(w)$，其更新公式为：</p>
<p>$$<br>v(\hat{w}) \leftarrow v(\hat{w}) + \eta \sum_{j=2}^{l^w} \left(1 - \sigma(x_w^T \theta_{j-1}^w) - d_j^w \right) \theta_{j-1}^w<br>$$</p>
<h3 id="5.2.1.2.基于Hierarchical Softmax的skip-gram模型">基于Hierarchical Softmax的skip-gram模型</h3>


<p><strong>skip-gram模型结构</strong></p>
<p>与cbow模型一样，基于hs的skip-gram输出层也是Huffman树。</p>
<p>$$<br>\begin{align}<br>p(context(w)|w)<br>&amp;= \prod_{u \in context(w)} p(u|w) \\\<br>&amp;= \prod_{u \in context(w)} \prod_{j=2}^{l^w} p(d_j^u | v_w, \theta_{j-1}^u) \\\<br>&amp;= \prod_{u \in context(w)} \prod_{j=2}^{l^w} \left[\sigma(v_w \theta_{j-1}^u)\right]^{1-d_j^u} \cdot \left[1 - \sigma(v_w \theta_{j-1}^u)\right]^{d_j^u}<br>\end{align}<br>$$</p>
<p>目标函数：</p>
<p>$$<br>\begin{align}<br>\mathcal{L}<br>&amp;= \sum_{w \in C} \log p(context(w)|w) \\\<br>&amp;= \sum_{w \in C} \sum_{u \in context(w)} \sum_{j=2}^{l^u} \left\{ (1 - d_j^u)  \log \left(\sigma(v_w\theta_{j-1}^u)\right) + d_j^u \log \left(1 - \sigma(v_w\theta_{j-1}^u)\right) \right\}<br>\end{align}<br>$$</p>
<p>为了梯度公式推导方便，我们把三重花括号里的公式记为$\mathcal{L}(w,u,j)$，公式为：</p>
<p>$$<br>\mathcal{L}(w,u,j) = (1 - d_j^u)  \log \left(\sigma(v_w\theta_{j-1}^u)\right) + d_j^u \log \left(1 - \sigma(v_w\theta_{j-1}^u)\right)<br>$$</p>
<p>以上就是基于HS的skip-gram模型的优化目标。下面分别对参数$v_w, \theta_{j-1}^u$求导，</p>
<p>$$<br>\frac{\partial \mathcal{L}(w,u,j)} {\partial v_w} = \left(1 - d_j^u - \sigma(v_w \theta_{j-1}^u) \right) \theta_{j-1}^u<br>$$</p>
<p>因此，可得参数$v_w$的更新公式为：</p>
<p>$$<br>v_w \leftarrow v_w + \eta \sum_{u \in context(w)} \sum_{j=2}^{l^u} \left(1 -d_j^u - \sigma(v_w \theta_{j-1}^u) \right) \theta_{j-1}^u<br>$$</p>
<p>同样的，根据参数对称性，可得参数$\theta_{j-1}^u$的更新公式为：</p>
<p>$$<br>\theta_{j-1}^u \leftarrow \theta_{j-1}^u + \eta \left(1 - d_j^u - \sigma(v_w \theta_{j-1}^u) \right) v_w<br>$$</p>
<p>在Google word2vec源码对于$v_w$的更新并不是在两层for循环外才更新，而是每处理一个词后更新$v_w$参数。</p>
<h3 id="5.2.2.基于Negative Sample算法的word2vec模型">基于Negative Sample算法的word2vec模型</h3>


<p>cbow模型</p>
<p>cbow模型是已知上下文$context(w)$预测中心词$w$。因此，对于给定的$context(w)$，词$w$就是一个<strong>正样本</strong>，其它词就是<strong>负样本</strong>了。</p>
<blockquote>
<p>负样本如此多，该如何选取呢？该问题相对独立，后面会单独介绍。</p>
</blockquote>
<p>假定现在已经选好了一个关于词$w$的负样本子集$NEG(w) \neq \emptyset$，且对于$\forall \hat{w} \in \mathcal{D}$，定义：</p>
<p>$$<br>L^w(\hat{w}) = \begin{cases}<br>1, \quad \hat{w} = w; \\\<br>0, \quad \hat{w} \neq w<br>\end{cases}<br>$$</p>
<p>$L^w(\hat{w})$表示词的标签，正样本标签为1，负样本标签为0.</p>
<p>对于一个给定的正样本，我们希望最大化：</p>
<p>$$<br>g(w) = \prod_{u \in {w} \cup NEG(w)} p(u | context(w))<br>$$</p>
<p>概率公式为： </p>
<p>$$<br>p(u|context(w)) =<br>\begin{cases}<br>\sigma(x_w^T \theta^u)  &amp; \quad L^w(u) = 1, \\\<br>1 - \sigma(x_w^T \theta^u)  &amp; \quad L^w(u) \neq 1<br>\end{cases}<br>$$</p>
<p>这里$x_w$仍然表示上下文向量之和，<strong>$\theta^u \in R^m$是词u对应的一个辅助向量，待训练参数</strong>。把上式整理为一个完整的表达式为：</p>
<p>$$<br>g(w) = \prod_{u \in w \cup NEG(w)} \left[\sigma(x_w^T \theta^u) \right]^{L^w(u)} \cdot \left[1 - \sigma(x_w^T \theta^u) \right]^{1 - L^w(u)}<br>$$</p>
<p>那么，为什么要最大化$g(w)$呢？我们再整理一下上式：</p>
<p>$$<br>g(w) = \sigma(x_w^T \theta^w) \prod_{u \in NEG(w)} \left[1- \sigma(x_w^T \theta^u) \right]<br>$$</p>
<p>$\sigma(x_w^T \theta^w)$表示当上下文为context(w)时，预测中心词为w的概率。$\sigma(x_w^T \theta^u), u \in NEG(w)$表示当上下文为context(w)时，预测中心词为u的概率。<strong>从形式上看，最大化$g(w)$等价于最大化$\sigma(x_w^T \theta^w)$同时最小化所有的$\sigma(x_w^T \theta^u)$</strong>，这正是我们所希望的：<strong>最大化正样本的概率同时最小化负样本的概率</strong>。</p>
<p>于是，对于给定的一个语料库$C$，优化目标为： </p>
<p>$$<br>G = \prod_{w \in C} g(w)<br>$$</p>
<h3 id="5.2.2.基于Negative Sample算法的word2vec模型">基于Negative Sample算法的word2vec模型</h3>






        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深度学习/">深度学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/CBow/">CBow</a><a href="/tags/ELMo/">ELMo</a><a href="/tags/FastText/">FastText</a><a href="/tags/GloVe/">GloVe</a><a href="/tags/Hierarchical-Softmax/">Hierarchical Softmax</a><a href="/tags/Negative-Sampling/">Negative Sampling</a><a href="/tags/Skip-Gram/">Skip Gram</a><a href="/tags/Word2Vec/">Word2Vec</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/openmit/openmit-chapter3-admm/" title="第03章：OpenMIT_算法框架" itemprop="url">第03章：OpenMIT_算法框架</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2017-05-20T03:30:31.000Z" itemprop="datePublished"> 发表于 2017-05-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2017-04-30</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<p><strong>内容列表</strong></p>
<ul>
<li><a href="#0.写在前面">0. 写在前面</a></li>
<li><a href="#1.大规模机器学习的本质">1. 大规模机器学习的本质</a></li>
<li><a href="#2.约束优化问题">2. 约束优化问题</a><ul>
<li><a href="#2.1.对偶提升">2.1. 对偶提升</a></li>
<li><a href="#2.2.对偶分解">2.2. 对偶分解</a></li>
<li><a href="#2.3.增强拉格朗日乘子法">2.3. 增强拉格朗日乘子法</a></li>
</ul>
</li>
<li><a href="#3.交替方向乘子法">3. 交替方向乘子法</a><ul>
<li><a href="#3.1.算法框架">3.1. 算法框架</a></li>
<li><a href="#3.2.算法性质与评价">3.2. 算法性质与评价</a></li>
</ul>
</li>
<li><a href="#4.一致性优化与均分优化">4. 一致性优化与均分优化</a><ul>
<li><a href="#4.1.全局变量一致性优化（consensus）">4.1. 全局变量一致性优化（consensus）</a></li>
<li><a href="#4.2.均分优化（sharing）">4.2. 均分优化（sharing）</a></li>
</ul>
</li>
<li><a href="#5.ADMM在优化领域的具体应用">5. ADMM在优化领域的具体应用</a><ul>
<li><a href="#5.1.$\ell_1\text{norm}$问题">5.1. $\ell_1－\text{norm}$问题</a></li>
<li><a href="#5.2.等式约束的凸优化问题">5.2. 等式约束的凸优化问题</a></li>
</ul>
</li>
<li><a href="#6.ADMM与统计机器学习">6. ADMM与统计机器学习</a></li>
<li><a href="#7.ADMM与机器通信">7. ADMM与机器通信</a></li>
<li><a href="#8.ADMM应用场景">8. ADMM应用场景</a></li>
<li><a href="#9.基于MPI的ADMM框架实现">9. 基于MPI的ADMM框架实现</a></li>
<li><a href="#参考资料">参考资料</a> </li>
</ul>
<h3 id="0.写在前面">0. 写在前面</h3>

<p>分布式优化算法是分布式机器学习算法方向的核心，本章主要想介绍一个算法框架－ADMM算法－来实现这一功能。ADMM算法并不是一个很新的算法，他只是整合许多不少经典优化思路，然后结合现代统计学习所遇到的问题，提出了一个比较一般的比较好实施的<strong>分布式算法框架</strong>。</p>
<p>ADMM算法结构天然地适用于分布式环境下具体任务的求解。在详细介绍ADMM分布式算法之前，我们先了解狭下一个大学习问题的如何在分布式环境下拆分成多个子任务学习问题的。然后通过《约束优化问题一般的解决方案》来阐述ADMM算法的演化过程，过渡到ADMM。最后详细阐述ADMM算法结构、理论可行性证明、分布式环境下如何保证一致性以及信息共享，适用的学习问题。</p>
<h3 id="1.大规模机器学习的本质">1. 大规模机器学习的本质</h3>

<p>大规模机器学习问题（large-scale machine learning）必须要解决“三大”问题：1. 大数据学习能力；2. 大模型学习能力；3. 产生更大的效果。</p>
<p>毫无疑问，大规模机器学习对比单机版机器学习任务，它要有更大的数据吞吐、处理和学习能力（远超出单机版能处理的数据范围）；由于存储和计算资源的增加，它必然得具体单机学习不了的复杂模型的能力，比如10亿级别以上的LR模型，更复杂的树模型等；在具备前两项能力的同时，必须要产生更大更好的效果（improve performance），否则大规模机器学习的存储毫无疑义，</p>
<blockquote>
<p>业内其他的叫法，比如distributed machine learning，cloud machine learning，要表达的意思差不多.</p>
</blockquote>
<p>因为是基于海量数据的学习任务，因此大规模机器学习解决方案基本上都需要将一个大的学习任务拆解成多个子任务并行（或分布式）进行：海量数据切分为若干块，每个子任务学习仅用一块数据进行训练，最后整合学习结果，得到最终参数解。公式化表示如下：</p>
<p>$$<br>\min_{w} \; \sum_{(x, y) \in \mathcal{D}} L(w^T x, y) + \lambda {\Vert w \Vert}_1<br>\; \overset{\text{任务分解}}{\Longrightarrow}<br>\min_{w_1, \cdots, w_T; w} \sum_{t=1}^{T} \left( \sum_{(x,y) \in \mathcal{D}_t} L(w_t^T x, y) \right) + \lambda {\Vert w \Vert}_1 \qquad(diml.2.5.0)<br>$$</p>
<p>$$<br>\min_{w} \; \sum_{(x, y) \in \mathcal{D}} L(w^T x, y) + \lambda {\Vert w \Vert}_1<br>\xrightarrow[\text{ADMM结构}]{任务分解}<br>\begin{align}<br>\min_{w_1, w_2, \cdots, w_T} &amp; \; \sum_{t=1}^{T} \left( \sum_{(x,y) \in \mathcal{D}_t} L(w_t^T x, y) \right) + \lambda{\Vert \theta \Vert}_1  \\\<br>s.b. \quad &amp; \; w_t=\theta \;(t=1,\cdots,T)<br>\end{align}<br>$$</p>
<p>公式$(diml.2.5.0)$是一个带正则项的模型优化目标（<code>&quot;损失函数＋正则项&quot;</code>），损失函数用\(L(w^T x, y)\)表示。其中，$\mathcal{D}$表示训练集，把$\mathcal{D}$切分为$T$个子数据集，每一块用$\mathcal{D_t}$表示；模型参数$w \in R^n，n$为特征维度，$\lambda$为正则项系数。箭头右边是改写为多任务形式的优化目标，其中参数$w$起到了连接不同子任务的作用，$\lambda$控制了多个子任务连接的强度，$\lambda$越大说明连接强度越强。当$\lambda = 0$时，等价于$T$个子任务独立学习，子任务之间没有关联。</p>
<p>毫无例外地，任何一个大规模机器学习的任务都会用到“分而治之”的思想，即把大的机器学习任务拆分成多个子任务（大规模机器学习之间的差异主要在于拆分手段的不同）。不管如何拆分最后都会转化到一个本质问题上来，即多任务的联合学习。因此，可以把<strong>大规模机器学习问题的本质是多任务的联合学习</strong>。那么，接下来我们要思考的是如何求解该问题。</p>
<p><strong>问题1: 如何在分布式环境下求解多任务联合学习问题？</strong> </p>
<p>本章要讲述的ADMM算法就是解决这类学习问题不错的解决方案。严格意义上来讲，ADMM不同于梯度法、共轭梯度法、牛顿法、拟牛顿法等具体的参数学习算法，，把它称为<strong>分布式计算框架</strong>更合理。</p>
<p>在介绍ADMM算法之前，我们先来看下它的前身（precursors）是谁？是如何演化过来的？</p>
<h3 id="2.约束优化问题">2. 约束优化问题</h3>

<p>说到ADMM算法的演化历程，要从等式约束优化问题说起。一个典型的等式约束优化问题，形式化表示如下：</p>
<p>$$<br>\begin{align}<br>&amp; \; \min_{x} \;\; f(x) \\\<br>&amp; s.t. \; Ax = b<br>\end{align}     \qquad\qquad(diml.2.5.1)<br>$$</p>
<p>其中，<strong>目标函数</strong>\(f(x): R^n \rightarrow R\)，\(Ax=b\)为<strong>约束条件</strong>，参数\(x \in R^n, A \in R^{m \times n}, b \in R^m\)。\(s.t\)是英文<code>subject to</code>的缩写。如何求解等式约束优化问题？</p>
<h4 id="2.1.对偶提升">2.1. 对偶提升</h4>

<p>等式约束优化问题一般是要用到拉格朗日乘子法。通过引入拉格朗日乘子，构造拉格朗日函数，分别对原参数和乘子求偏导（偏导数等于0），通过<strong>交替优化</strong>，使其最终收敛到最优解。其中，拉格朗日乘子又称算子、对偶变量。</p>
<p>公式\((diml.2.5.1)\)引入对偶变量（用\(\beta \in R^m \)表示），得到拉格朗日函数\(\mathcal{L}: R^{m \times n} \rightarrow R \) 为</p>
<p>$$<br>\begin{array}{lc}<br>&amp; \; \min_{x} \;\; f(x) \\\<br>&amp; s.t. \; Ax = b<br>\end{array}<br>\overset{拉格朗日函数}{\Longrightarrow}<br>\mathcal{L}(x, \beta) = f(x) + \beta^T (Ax-b)    \qquad\qquad(diml.2.5.2)<br>$$</p>
<p>把原问题的对偶问题表示为\(\max \; g(\beta)\)。对偶提升法的思想：在<strong>强对偶性</strong>假设下，通过最大化对偶函数，使得原函数和对偶函数会同时达到最优。求得最优解的过程是对参数的交替更新，参数更新的迭代公式如下：</p>
<p>$$<br>\begin{array}{lc}<br>&amp; \; \min_{x} \;\; f(x) \\\<br>&amp; s.t. \; Ax = b<br>\end{array}<br>\overset{拉格朗日函数}{\Longrightarrow}<br>\mathcal{L}(x, \beta) = f(x) + \beta^T (Ax-b)<br>\overset{对偶提升法}{\Longrightarrow}<br>\begin{align}<br>x^{k+1} &amp; := \arg \min_{x} \mathcal{L}(x, \beta^{k})  \qquad\;(\text{step1}) \\\<br>\beta^{k+1} &amp; := \beta^k + \alpha^{k}(A x^{k+1} - b) \\quad(\text{step2})<br>\end{align} \qquad(diml.2.5.3)<br>$$</p>
<p>收敛后得到原参数的最优解：</p>
<p>$$<br>x^{_} = \arg \min_{x} \mathcal{L}(x, \beta^{_})   \qquad\qquad\quad(diml.2.5.4)<br>$$</p>
<blockquote>
<p>公式\((diml.2.5.3)\)解释：</p>
<p>step1: 求拉格朗日函数极小化时对应的参数\(x\)，具体实现时，需要对参数\(x\)求偏导，令\(\frac{\partial}{\partial{x}} L(x, \beta^k) = 0\).</p>
<p>step2: 公式\((diml.2.5.3)\)对\(\beta\)求偏导得到梯度，使用梯度提升法得到对偶变量\(\beta\)的迭代公式，\(\alpha^k\)为迭代步长。这一步称为<strong>对偶提升(Dual Ascent)</strong>。</p>
</blockquote>
<p><strong>要想用对偶提升法求得最优解有一个前提假设：原目标函数必须是强凸函数</strong>。否则得不到最优解，这也是对偶提升法的局限之处。</p>
<p><strong>问题2: 为什么对偶提升法对目标函数有强约束？</strong></p>
<p>参考<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier" target="_blank" rel="external">维基百科</a>Figure2. 假设目标函数\(y=f(x), x\)是向量。\(y\)取不同的值，在\(x\)构成的平面（或曲面）上构成等高线。约束条件假设为\(g(x) = c, x\)是向量，在\(x\)构成的平面或去面上是一条曲线。</p>
<p>假设\(g(x)\)与等高线相交，交点就是同时满足等式约束条件和目标函数的可行域的值。但可以确定交点不是最优值，因为相交意味着肯定还存在其它的等高线在该条等高线的内侧或外侧，使得新的等高线与目标函数的交点的值更大或更小。只有当等高线与目标函数的曲线相切的时候，即目标函数的梯度方向与约束条件的梯度方向平行时缺德最优值，此时必须满足：\(\nabla_{x} f(x) = \alpha \cdot \nabla_{x} g(x) \)。而该式即为Lagrange函数\(\mathcal{L}(x, \beta)\)对参数\(x\)求偏导后的结果。</p>
<p>最优化领域的一些核心概念，如凸函数、对偶函数、共轭函数、强对偶性等，这里简单的说明如下：</p>
<blockquote>
<p>(1). 强凸函数</p>
<p>在<a href="http://www.52caml.com/head_first_ml/ml-chapter10-clustering-family/" target="_blank" rel="external">《深入浅出机器学习》系列 第10章：深入浅出ML之cluster家族</a> 中的EM算法有提到.<br>强凸函数需满足：\(E[f(x)] &gt; f(E(x)) \)</p>
<p>函数\(f: I \rightarrow R\)成为强凸的，若\(\exists\alpha &gt; 0\)，使\(\forall(x, y) \in I \times I, \forall t \in [0, 1]\)，恒有：</p>
<p>$$<br>f[tx+(1-t)y] \le tf(x) + (1-t) f(y) - t(1-t) \alpha (x-y)^2 \qquad(n.diml.2.5.1)<br>$$</p>
<p>(2). 对偶函数</p>
<p>以公式$(diml.2.5.1)$等式约束优化问题为例，定义对偶函数\(g(\beta): R^m \rightarrow R \) 为Lagrange函数关于 \(x\)取得的最小值，即对\(\beta \in R^m \)有<br>$$<br>\begin{align}<br>g(\beta) &amp; = \inf_{x} \mathcal{L}(x, \beta) = \inf_{x} \left(f(x) + \beta^T(Ax-b) \right) \\\<br>&amp; = -\beta^T b + \inf_{x} \left(f(x) + \beta^TAx \right) = -\beta^T b - f^{_}(-A^T\beta)<br>\end{align} \qquad\\qquad(n.diml.2.5.2)<br>$$<br>其中\(f^{_}\)是\(f\)的<a href="参考wiki">共轭函数</a>。</p>
<p>(3). 共轭函数</p>
<p>设函数\(f: R^n \rightarrow R\)，定义函数\(f^{_}: R^n \rightarrow R\)为<br>$<br>f^{_} = \inf_{x \in dom \, f} \left(f(x) - y^T x \right)<br>$<br>。此函数称为函数\(f\)的共轭函数。（参考《Convex Optimization》3.3节 共轭函数定义.）</p>
<p>(4). 强对偶性</p>
<p>在约束优化问题中，如果最小化<strong>原凸函数</strong>等价于最大化<strong>对偶函数</strong>时，称为强对偶性。即\(\min f(x) = \max g(\beta)\)</p>
<p>对偶提升法在满足强对偶性假设下可以证明公式\((diml.2.5.3)\)能达到收敛。即要求目标函数\(f(x)\)为强凸函数。</p>
</blockquote>
<p>对偶提升法虽然对目标函数有严格的约束，使得很多优化目标不能用其求解。但是它具体一个很好的性质，详细的见下面要提到的对偶分解。</p>
<h4 id="2.2对偶分解">2.2. 对偶分解</h4>

<p>对偶提升法虽然对目标函数有严格的要求，但是它还有一个非常好的性质：</p>
<p><strong>如果目标函数\(f(x)\)是可分的，整个优化问题可以拆分成多个子优化问题，分块优化后得到局部分数，然后汇集起来 整体更新全局参数，有利于问题的并行化处理。这个过程称为对偶分解（Dual Decomposition）。</strong></p>
<p>如果目标函数\(f(x)\)和约束条件是可分解的，那么原问题\((diml.2.5.1)\)对应的分解形式为：</p>
<p>$$<br>\begin{array}{lc}<br>&amp; \; \min_{x} \;\; f(x) \\\<br>&amp; s.t. \; Ax = b<br>\end{array}<br>\overset{分解原问题}{\Longrightarrow}<br>\begin{align}<br>&amp; \min_{x} \; f(x) = \sum_{t=1}^{T} f_t(x_t) \\\<br>&amp; s.t. \; Ax = \sum_{t=1}^{T} A_t x_t = b<br>\end{align}   \qquad\qquad(diml.2.5.5)<br>$$</p>
<p>拉格朗日函数 与 参数更新公式：</p>
<p>$$<br>\mathcal{L}(x, \beta) = \sum_{t=1}^{T} \mathcal{L_t}(x_t, \beta) = \sum_{t=1}^{T} \left(f_t(x_t) + \beta^T A_t x_t - \frac{1}{T}\beta^T b \right)<br>\Longrightarrow<br>\begin{array}{lc}<br>x_{t}^{k+1} := \arg \min_{x} \mathcal{L}_t(x_t,\beta^{k}) \qquad\qquad\qquad\quad(\text{1})\\\<br>\beta^{k+1} := \beta^{k} + \alpha^k \nabla g(\beta) = y^k + \alpha^k(A x^{k+1} -b)  \;\;\,(\text{2})<br>\end{array} \;(diml.2.5.7)<br>$$</p>
<blockquote>
<p>公式$(diml.2.5.7)$解读：</p>
<p>step1: 并行化求解多个子目标函数</p>
<p>step2: 汇总\(x_{i}^{k+1}\)（论证是否求平均可否？），更新对偶变量.</p>
</blockquote>
<p>如何在目标函数不满足强凸函数约束时，求解对应的优化问题呢？下面要提到的增广拉格朗日乘子法可以解决。</p>
<h4 id="2.3.增强拉格朗日乘子法">2.3. 增强拉格朗日乘子法</h4>

<p>前面提到，对偶提升方法求解优化问题时，目标函数必须满足强凸函数的条件，限制过于严格。为了增加对偶提升法的鲁棒性和放松对目标函数\(f\)强凸的约束条件，人们提出了<a href="https://en.wikipedia.org/wiki/Augmented_Lagrangian_method" target="_blank" rel="external">增广拉格朗日乘子法</a>（Augmented Lagrangians）用于解决这类问题。</p>
<p>Argumented Lagrangians方法的思想：<strong>在目标函数基础上引入惩罚函数项（二次项），放松对目标函数\(f(x)\)严格凸的限制，同时使得算法更加稳健。</strong></p>
<p>最优化问题从\((diml.2.5.1)\)变为：</p>
<p>$$<br>\begin{array}{lc}<br>&amp; \; \min_{x} \;\; f(x) \\\<br>&amp; s.t. \; Ax = b<br>\end{array}<br>\overset{惩罚函数项}{\Longrightarrow}<br>\begin{align}<br>&amp; \; \min_{x} \;\; f(x) + \frac{\rho}{2} {\Vert Ax-b \Vert}_2^2 \\\<br>&amp; \quad s.t. \; Ax = b<br>\end{align}     \qquad\qquad(diml.2.5.8)<br>$$</p>
<p>在约束条件\(Ax-b=0\)下，目标函数定义为\(f(x) + \frac{\rho}{2} {\Vert Ax-b \Vert}_2^2\) 与\(f(x)\)是等价的，因为最优解在满足约束条件的前提下优化目标的后一项等于0，这一项称为<strong>惩罚函数项</strong>（或二次惩罚项）。</p>
<p>对应的拉格朗日函数：</p>
<p>$$<br>\mathcal{L}_{\rho}(x, \beta) = f(x) + \frac{\rho}{2} {\Vert Ax-b \Vert}_2^2 + \beta^T (Ax-b)    \qquad\quad(diml.2.5.9)<br>$$</p>
<blockquote>
<p>公式解读：</p>
<p>$$<br>\mathcal{L}_{\rho}(x, \beta) = \overbrace{f(x)}^{原优化目标} + \overbrace{\underbrace{\frac{\rho}{2} {\Vert Ax-b \Vert}_2^2}_{二次惩罚项} + \underbrace{\beta^T(Ax-b)}_{拉格朗日乘子项} }^{增广拉格朗日乘子项}  \qquad (n.diml.2.5.3)<br>$$</p>
</blockquote>
<p><strong>问题3：为什么添加二次惩罚项就可以“解除对目标函数\(f(x)\)强凸性质”的限制, 进而可得最优解了呢？</strong></p>
<p>原因在于如果<strong>原优化目标\(f(x)\)是非凸的，那意味着拉格朗日函数在对\(x\)求偏导时 结果可能不可导，无法进行参数交替迭代优化。添加了二次项惩罚项之后，可以保证即便\(f(x)\)不满足强凸条件，在对x求偏导时 仍然可导，因为二次惩罚项部分可以保证这一点</strong>。</p>
<p>因此，添加二次惩罚项的好处是可以保证增广的对偶函数一定是可微的。迭代公式如下：</p>
<p>$$<br>\mathcal{L}_{\rho}(x, \beta) = f(x) + \frac{\rho}{2} {\Vert Ax-b \Vert}_2^2 + \beta^T (Ax-b)<br>\Longrightarrow<br>\begin{align}<br>x^{k+1} &amp; := \arg \min_{x} \mathcal{L}_{\rho} (x, \beta^k) \qquad (step1) \\\<br>\beta^{k+1} &amp; := \beta^k + \rho(Ax^{k+1}-b)  \quad\;\;(step2)<br>\end{align}  \qquad(diml.2.5.10)<br>$$</p>
<p>上式即为增广拉格朗日乘子法的交替迭代公式。与标准的对偶提升法类似，但有两点不同：</p>
<ol>
<li>$\text{step1}$. \(\mathcal{L}(x, \beta)\)对\(x\)求偏导得到最小值时用的是增广拉格朗日函数；</li>
<li>$\text{step2}$. 惩罚项参数\(\rho\)在这里作为对偶变量更新时的步长（step size）。</li>
</ol>
<p><strong>问题4：为什么选择\(\rho\)作为对偶变量更新的迭代的step size？</strong></p>
<p>为了推导方便，这里假设\(f\)是可微的（尽管在Augmented拉格朗日法中不是必须的），\((x^{\star}, \beta^{\star})\)是优化问题的最优解。此时应该满足以下条件：</p>
<p>$$<br>Ax^{\star} - b = 0,\; \nabla f(x^{\star}) + A^T\beta^{\star} = 0 \qquad (diml.2.5.11)<br>$$</p>
<p>根据参数交替迭代公式的\(step1\)，第\(k+1\)的迭代结果\(x^{k+1}\)是由最小化函数\(\mathcal{L}_{\rho}(x, \beta^k)\)得来（具体做法：对参数\(x\)求偏导，令偏导数等于0），所以\(x^{k+1}\)满足下式成立</p>
<p>$$<br>\begin{align}<br>0 &amp; = \nabla_{x} \mathcal{L}_{\rho}(x^{k+1}, \beta^k) = \nabla_{x} f(x^{k+1}) + \beta^k A + \rho( A^T Ax - A^T b) \\\<br>&amp; = \nabla_{x}f(x^{k+1}) + A^T \left(\underline {\beta^k + \rho(Ax^{k+1} - b) } \right) \\\<br>&amp; = \nabla_{x}f(x^{k+1}) + A^T \beta^{k+1}<br>\end{align} \qquad(diml.2.5.12)<br>$$</p>
<p>因此可以得到，\(\rho\)为对偶变量参数更新的步长，并得到了第\(k+1\)次迭代的最优解\((x^{k+1}, \beta^{k+1})\)。随着迭代的进行，原函数的残差\(Ax^{k+1}-b\)逐步收敛到0，得到最优解。</p>
<p>相比对偶提升法，增广拉格朗日乘子法有更好的收敛性质，并且拥有对目标\(f(x)\)不做强凸条件的限制，但是这些好处总是要付出一定的代价的：<strong>如果目标函数是\(f(x)\)是可分的，此时增广拉格朗日函数\(\mathcal{L}_{\rho}(x, \beta)\)是不可分的（因为惩罚函数项部分涉及到矩阵相乘计算，无法用分块形式进行并行化求解），因此公式\((diml.2.5.10)\)的\(step1\)没有办法在分布式环境下并行优化</strong>。</p>
<p>如果能结合Dual Ascent的并行求解的优势 和 Augmented Lagrangians Methods of Multipliers鲁棒性和不错的收敛性质，那么就可以使大多数目标函数求解都能在分布式环境下并行实现，岂不美哉！！！</p>
<p>果然在2010年由Stephen Boyd大师等人系统性的整理了交替方向乘子法（ADMM算法），可以解决上述的问题。为此它们长篇论述了ADMM算法的演化过程，参考论文：<a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf" target="_blank" rel="external">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</a> 该文被《Foundations and Trends in Machine Learning》录入。</p>
<h3 id="3.交替方向乘子法">3. 交替方向乘子法</h3>

<p>交替方向乘子法（Alternating Direction Method of Multipliers，简称ADMM）<strong>适用于大规模统计学习在分布式环境下的优化求解问题</strong>。可以理解为增广拉格朗日乘子法的变种，旨在整合对偶提升法的可分解性和增广拉格朗日乘子法优秀的收敛性质，进一步提出的新算法。</p>
<h4 id="3.1.算法框架">3.1. 算法框架</h4>

<p>我们修改下公式\((diml.2.5.1)\)，这样更符合统计学习目标函数的形式. 重新定义的优化问题和Lagrange函数：</p>
<p>$$<br>\begin{array}{lc}<br>\min_{x} \;\; f(x) \\\<br>s.t. \; Ax = b<br>\end{array}<br>\overset{目标函数分解}{\Longrightarrow}<br>\begin{array}{lc}<br>\min &amp; f(x) + g(z) \\\<br>s.t. &amp; Ax + Bz = C<br>\end{array}  \qquad\quad \quad (diml.2.5.13)<br>$$</p>
<p>其中\(x \in R^n, z \in R^m; A \in R^{p \times n}, B \in R^{p \times m}, C \in R^p\)。</p>
<p>拉格朗日函数：</p>
<p>$$<br>\mathcal{L}_{\rho}(x, z, \beta) = f(x) + g(z) +\underline{ \frac{\rho}{2} {\Vert Ax+Bz-C \Vert}_2^2 + \beta^T(Ax+Bz-C) } \quad (diml.2.5.14)<br>$$</p>
<p>按照乘子法的思路，参数交替更新迭代：</p>
<p>$$<br>\begin{align}<br>(x^{k+1}, z^{k+1}) &amp; := \arg \min_{x,z} \mathcal{L}_{\rho}(x,z,\beta^k) \qquad\qquad(\text{step1}) \\\<br>\beta^{k+1} &amp; := \beta^k + \rho (Ax^{k+1} + Bz^{k+1} - C) \quad(\text{step2})<br>\end{align}  \qquad(diml.2.5.15)<br>$$</p>
<p>上式中的\(step1\)要求对两个原始变量联合最小化，也就是说\(x,z\)是融合在一起优化的，暂且不说联合优化是否容易求解，可以确定的这一步优化不可分解。</p>
<p>ADMM采用了拆分思想，最初就把\(x\)和\(z\)分别看作两个不同的变量，约束条件也是如此。采用交替方式迭代（序贯式迭代），称为<strong>交替方向</strong>(alternating direction)。</p>
<p>$$<br>\begin{align}<br>x_t^{k+1} &amp; := \arg \min_x \mathcal{L}_{\rho}(x, z^{k}, \beta^k) \qquad\qquad (\text{step1, 局部更新}) \\\<br>z^{k+1} &amp; := \arg \min_z \mathcal{L}_{\rho}(x^{k+1}, z, \beta^k) \qquad\quad\; (\text{step2, 全局更新}) \\\<br>\beta_t^{k+1} &amp; := \beta^{k} + \rho(Ax^{k+1} + Bz^{k+1} - C) \quad\;\; (\text{step3, 局部更新})<br>\end{align}  \qquad\qquad(diml.2.5.16)<br>$$</p>
<p>ADMM算法拆分参数\(x\)和\(z\)两步迭代最大的好处是：<strong>当\(f\)可分时，参数可以并行求解。</strong></p>
<p>在<a href=""><code>chapter6_DiML_算法框架_学习器</code></a>中可以看到，ADMM这种参数和目标函数的拆分非常适合机器学习中的\(\ell_1 \text{-norm}\)优化问题，即：<code>loss function + regularization</code>目标函数的分布式求解。</p>
<h4 id="3.2.算法性质与评价">3.2. 算法性质与评价</h4>

<p><strong>1). 收敛性</strong></p>
<p>论文中对收敛性的证明，提到了两个假设条件：</p>
<ul>
<li>$f(x)$和$g(z)$分别是扩展的实质函数：\(R^{n}(R^{m}) \rightarrow R \; \cup {+\infty} \), 并且是closed、proper和convex的；</li>
<li>增广拉格朗日函数\(\mathcal{L}_0\)有一个鞍点（saddle point）；对于约束中的矩阵$A,B$都不需要满秩。</li>
</ul>
<p>满足两个假设条件下，可以保证残差、目标函数、对偶变量的收敛性。（详细证明过程参考paper Appendix A）.</p>
<blockquote>
<p>实际应用表明，ADMM算法收敛速度是很慢的，类似于共轭梯度法。迭代数十次可以得到一个可接受的结果，与快速的高精度算法（牛顿法、拟牛顿法、内点法等）相比收敛就满多了。因此实际应用中ADMM会与其它高精度算法结合其俩，这样从一个可接受的结果变得在预期时间内可以达到较高的收敛精度。</p>
<p>在大规模问题求解中，高精度的参数解对于预测的泛化效果没有很大的提高。因此实际应用中，预期时间内得到的一个可接受的结果就可以直接应用预测了。</p>
</blockquote>
<p><strong>2). 最优条件和停止准则</strong></p>
<p>最优条件先省略。放在公式(diml.2.5.17)中</p>
<p>从最优条件中可以得到初始残差（primal residuals）和对偶残差（dual residuals）的表达式：</p>
<p>$$<br>\begin{align}<br>r^{k+1} &amp; := Ax^{k+1} + Bz^{k+1} - C \qquad(初始残差) \\\<br>s^{k+1} &amp; := \rho A^T B(z^{k+1} - z^{k}) \qquad\quad(对偶残差)<br>\end{align}  \qquad\qquad(diml.2.5.18)<br>$$</p>
<p>迭代停止准则比较难以把握，因为受收敛速度问题，要想获得一个不错的参数解，判断迭代停止条件还是比较重要的。实际应用中，一般都根据初始残差和对偶残差足够小来停止迭代。阈值包含了绝对容忍度（absolute tolerance）和相对容忍度（relative tolerance），阈值设置难以把握，具体形式如下：</p>
<p>$$<br>\begin{align}<br>{\Vert s^k \Vert} \leq \epsilon^{\text{dual}} &amp; = \sqrt{n} \epsilon^{\text{abs}} + \epsilon^{\text{rel}} {\Vert A^T y^k \Vert}_2 \\\<br>{\Vert r^k \Vert}_2 \leq &amp; = \sqrt{p} \epsilon^{\text{abs}} + \epsilon^{\text{rel}} \; \text{max} \{ {\Vert Ax^k \Vert}_2, {\Vert Bz^k \Vert} _2, {\Vert C \Vert}_2 \}<br>\end{align}  \qquad\quad (diml.2.5.19)<br>$$</p>
<p>上面的$\sqrt{p}$和$\sqrt{n}$分别是维度和样本量。一般而言，相对停止阈值$\epsilon^{\text{rel}} = 10^{-1}$或$10^{-4}$，绝对阈值的选取要根据变量的取值范围来选取。</p>
<p>此外，在对偶变量更新的惩罚参数\(\rho\)原来是不变的。有一些文章做了可变的惩罚参数，目的是为了降低惩罚参数对初始值的依赖。而证明变动的$\rho$给ADMM的收敛性证明比较困难，因此实际中开设经过一系列迭代后$\rho$也稳定，直接用固定的惩罚参数$\rho$了。</p>
<h3 id="4.一致性优化与均分优化">4. 一致性优化与均分优化</h3>

<p>分布式优化有两个很重要的问题，即一致性优化问题和共享优化问题，这也是ADMM算法通往并行和分布式计算的途径。下面以机器学习问题的参数优化为例解释这两个重要的概念。</p>
<p><strong>全局变量一致性优化（global consensus）</strong></p>
<p>所谓全局变量一致性优化问题，说白了就是<strong>切割大样本数据，并行化计算</strong>。即整体优化求解任务根据目标函数分解成$T$个子任务，每个子任务和子数据都可以获得一个参数解$x_i$，但是整体优化任务的解只有一个$z$。机器学习中的优化目标通常是“损失函数＋正则项”的结构形式，问题就转化为<strong>带正则项的全局一致性问题</strong>，优化问题可以写成如下形式：</p>
<p>$$<br>\begin{array}{lc}<br>\min \quad \sum_{i=1}^{T} \; f_i(x_i) + g(z)\\\<br>s.t. \quad x_i = z, \; x_i \in R^n, i=1,\cdots,T<br>\end{array}<br>\overset{参数迭代}{\Longrightarrow }<br>\begin{align}<br>x_i^{k+1} &amp; := \arg \min_{x_i} \left(f_i(x_i) + (\beta_i^{k})^T (x_i - z^k) + \frac{\rho}{2} {\Vert x_i - z \Vert}_2^2 \right) \\\<br>z^{k+1} &amp; := \arg \min_{z} \left(g(z) - \sum_{t=1}^{T} (\beta_t^k)^T \theta + \frac{\rho}{2} {\Vert x_i - z \Vert}_2^2 \right) \\\<br>\beta_i^{k+1} &amp; := \beta_i^k + \rho (x_i^{k+1} - z^{k+1})<br>\end{align}<br>$$</p>
<p>注意，此时$f_i : R^n \rightarrow R \, \cup {+\infty}$仍然是凸函数，而局部参数$x_i$并不是对参数空间进行划分，而是把数据集合$\mathcal{D}$划分为$T$个子集，对数据划分。下文提到的ADMM在$\ell_{1}\text{-norm}$问题中具体应用会详细推导这部分。</p>
<p><strong>均分优化问题（sharing）</strong></p>
<p>在统计学习优化问题中，所谓均分优化问题说白了就是<strong>切分特征维度，并行化求解</strong>。它更侧重于高维数据并行化求解的应用场景，通过切分特征维度得到并行化处理。同样假设数据矩阵$A \in R^{m \times n}$和$b \in R^m$，此时满足$n \gg m$，也就是说样本特征维度远大于样本数。shareing的做法是对数据矩阵$A$按照特征空间切分（竖着切分），</p>
<p>$$<br>\begin{array}{lc}<br>A = [A_1, A_2, \cdots, A_T], A_i \in R^{m \times n_i} \\\<br>x = (x_1, x_2, \cdots, x_T), x_i \in R^{n_i}<br>\end{array}<br>\overset{数据和特征按列切分}{\Longrightarrow}<br>\begin{array}{lc}<br>Ax = \sum_{i=1}^{T} A_i x_i ; \\\<br>g(x) = \sum_{i=1}^{T} g_i(x_i)<br>\end{array}<br>\Longrightarrow<br>\min \; L \left(\sum_{i=1}^{T} A_i x_i - b \right) + \sum_{i=1}^{T} g_i(x_i)  \quad(diml.2.5.21)<br>$$</p>
<p>我们把公式$(diml.2.5.21)$改成ADMM sharing形式：</p>
<p>$$<br>\begin{array}{lc}<br>\min \; L(\sum_{i=1}^{T} z_i - b) + \sum_{i=1}^{T} g_i(x_i) \\\<br>s.t. \quad A_i x_i - z_i = 0, \; z_i \in R^m<br>\end{array}<br>\Longrightarrow<br>\begin{align}<br>x_i^{k+1} &amp; := \arg \min_{x_i} \left(g_i(x_i) + \frac{\rho}{2} {\Vert A_i x_i - A_i x_i^k + \overline{Ax}^k - \overline{z}^k + \mu^k \Vert}_2^2 \right) \\\<br>z^{k+1} &amp; := \arg \min_{z} \left(L(N\overline{z} - b) + T \frac{\rho}{2} {\Vert \overline{z} - \overline{Ax}^{k+1} - \mu^k \Vert}_2^2 \right) \\\<br>\mu_{k+1} &amp; := \mu_i^k + \overline{Ax}^{k+1} - \overline{z}^{k+1}<br>\end{align}<br>$$</p>
<h3 id="5.ADMM在优化领域的具体应用">5. ADMM在优化领域的具体应用</h3>

<h4 id="5.1.$\ell_1\text{norm}$问题">5.1. l1-norm问题</h4>  

<p> 这里的$\ell_{1}\text{-norm}$问题不仅仅是指称为[Lasso](<a href="https://en.wikipedia.org/wiki/Lasso" target="_blank" rel="external">https://en.wikipedia.org/wiki/Lasso</a>_(statistics))问题，而是包含了多种$\ell_{1}\text{-norm}$类型问题。 它的初衷是通过特征选择（自变量选择）来提高模型的预测精度和解释性，具体做法是在优化目标上添加$\ell_{1}$正则项。</p>
<blockquote>
<p>Lasso 的基本思想是在回归系数的绝对值之和小于一个常数的约束条件下，使残差平方和最小化，从而能够产生某些严格等于0 的回归系数，得到可以解释的模型。</p>
</blockquote>
<p>如果要大规模部署$\ell_{1}\text{-norm}$问题的解决方案，ADMM算法可能是首选。它非常适合机器学习和统计学习的优化问题，因为机器学习问题的优化目标大部分都是<strong>“损失函数＋正则项”</strong>形式. 这种表达形式切好饿意套用ADMM算法框架$f(x) + g(z)$。因此机器学习问题结合ADMM算法框架基本可以在分布式环境下求解很多已由的问题。</p>
<p>本系列我们用ADMM算法框架主要解决分布式机器学习任务的参数优化问题，所以我们直接关注<strong>一般化的损失函数＋$\ell_{1}正则项问题$</strong>。这类问题通用框架：</p>
<p>以平方损失函数为例，定义损失函数\(L(y^{(i)}, w^Tx^{(i)}) = \left(y^{(i)} - w^Tx^{(i)} \right)^2\). 优化目标：</p>
<p>$$<br>\begin{array}{lc}<br>\min_{w} \quad \frac{1}{m} \sum_{i=1}^{m} \left(y^{(i)} - w^Tx^{(i)} \right)^2 + \lambda {\Vert w \Vert}_1<br>\end{array} \qquad\qquad\qquad\qquad\quad (整体优化目标)\\\ . \\\<br>\overset{\text{ADMM}形式} {\Longrightarrow } \quad<br>\begin{array}{lc}<br>\min_{w_1, w_2, \cdots, w_T} \quad \sum_{t=1}^{T} \left( \frac{1}{m_t} \sum_{i=1}^{m_t} \left(y_t^{(i)} - w_t^T x_t^{(i)} \right)^2 \right) + \lambda{\Vert \theta \Vert}_1  \\\<br>s.b \qquad\; w_t=\theta \;(t=1,\cdots,T)<br>\end{array} \quad (分布式优化目标表达式)<br>$$</p>
<p>令\(\frac{1}{m_t} \sum_{i=1}^{m_t} \left(y_t^{(i)} - w_t^T x_t^{(i)} \right)^2 = L(\mathbf{y}_t, \mathbf{x}_t w_t), \; \)，增广拉格朗日函数：</p>
<p>$$<br>\mathcal{L}(w_t, \theta, \beta_t) = \sum_{t=1}^{T} \; L(\mathbf{y}_t, \mathbf{x}_t w_t) + \lambda {\Vert \theta \Vert}_1 + \sum_{t=1}^{T} \beta_t^T(w_t - \theta) + \frac{\rho}{2} \sum_{t=1}^{T} {\Vert w_t - \theta \Vert}_2^2<br>$$</p>
<p><strong>参数迭代过程</strong></p>
<ul>
<li>局部参数更新（worker节点）</li>
</ul>
<p>$$<br>\begin{align}<br>w_t^{k+1}  \longleftarrow \; &amp; \arg\min_{w_t} \; \frac{1}{m_t}\; L(\mathbf{y}_t, \mathbf{x}_t w_t) + (\beta_t^k)^T w_t + \frac{\rho}{2} {\Vert w_t - \theta^k \Vert}_2^2 \qquad\qquad\quad(1) \\\<br>\; &amp; \arg\min_{w_t} \; \frac{1}{m_t}\; L(\mathbf{y}_t, \mathbf{x}_t w_t) + \frac{\rho}{2} {\Vert w_t - \theta^k + \frac{1}{\rho} \beta_t^k \Vert}_2^2<br>\end{align}<br>$$</p>
<ul>
<li><p>全局参数更新（master节点）</p>
<p>  $$<br>\theta^{k+1} \longleftarrow \arg\min_{\theta} \; \lambda {\Vert \theta \Vert}_1 - \sum_{t=1}^{T} (\beta_t^k)^T \theta + \frac{\rho}{2} \sum_{t=1}^{T} {\Vert w_t^{k+1} - \theta \Vert}_2^2 \qquad\qquad(2)<br>  $$</p>
</li>
<li><p>对偶变量更新（worker节点）</p>
<p>  $$<br>\begin{align}<br>\beta_t^{k+1} \longleftarrow &amp; \beta_t^k + \rho(w_t^{k+1} - \theta^{k+1}) \qquad\qquad\qquad\qquad\qquad\qquad\qquad\;\,(3)<br>\end{align}<br>$$</p>
</li>
</ul>
<p>迭代公式说明：</p>
<ul>
<li>第1步：局部参数更新。目标函数可以看作是<strong><code>损失函数+L2正则项</code></strong>(\({\Vert w_t - const \Vert}_2^2\))，涉及参数：$(\theta, \beta_t, w_t)$；</li>
<li>第2步：全局参数更新。需要详细推到，涉及到软阈值. 涉及参数：\((w_1,\cdots,w_T, \rho, \beta_1,\cdots,\beta_T,\lambda)\)</li>
<li>第3步：局部对偶变量更新。涉及参数：\((\theta, \lambda, w_1,\cdots,w_T, \rho)\)</li>
</ul>
<p>这里涉及到一个worker节点参数与master节点参数通信问题，以及<strong>master节点是如何利用worker局部参数更新全局参数的？</strong>我们先看master节点参数更新推导过程。</p>
<p>拉格朗日函数，对\(\theta\)求偏导：</p>
<p>$$<br>\begin{align}<br>\frac{\partial \, \mathcal{L}(w_t, \theta, \beta_t)} {\partial{\theta}} &amp; ＝ \frac{\partial \; \left({\lambda {\vert \theta \vert}_1} - \sum_{t=1}^{T}(\beta_t)^T \theta + \frac{\rho}{2} \sum_{t=1}^{T} {\Vert w_t - \theta \Vert}_2^2 \right)} {\partial {\theta}} \\\<br>&amp; = \mathbf{sign}(\theta) \cdot \lambda - \sum_{t=1}^{T} \beta_t + {\rho} \sum_{t=1}^{T} \left(\theta - w_t \right) \\\<br>&amp; = \mathbf{sign}({\vert \theta \vert}_1) \cdot \frac{\lambda}{\rho} - \sum_{t=1}^{T} \left( \frac{\beta_t}{\rho} + w_t\right) + \sum_{t=1}^{T} \theta = 0 \\\<br>\end{align}<br>$$</p>
<p>$$<br>全局参数\theta更新公式整理得到：\underline{\color{blue}{ \theta } = \frac{1}{T} \left(\sum_{t=1}^{T}<br>\left(\color{red} { \frac{\beta_t}{\rho} + w_t } \right)  - sign({\vert \theta \vert}_1) \cdot \frac{\lambda}{\rho} \right) }<br>$$</p>
<p>说明：全局参数更新时，$\ell_{1}\text{-norm}$项需要求导。虽然在0处不可导，但仍有解析解，这里使用<strong><a href="http://blog.csdn.net/jbb0523/article/details/52103257" target="_blank" rel="external">软阈值</a></strong>的方法得到解析解：</p>
<p>$$<br>\color{blue}{\theta} =<br>\begin{cases}<br>\frac{1}{T} \left( \sum_{t=1}^{T} \left( \color{red}{\frac{\beta_t}{\rho} + w_t}\right) + \frac{\lambda}{\rho}  \right) &amp; \qquad \text{if} \; {\vert \theta \vert}_1 &lt; 0, 若：\sum_{t=1}^{T} \left( \frac{\beta_t}{\rho} + w_t\right) + \frac{\lambda}{\rho} &lt; 0 ; \\\<br>\frac{1}{T}\left(\sum_{t=1}^{T}\left(\color{red}{\frac{\beta_t}{\rho} + w_t} \right) - \frac{\lambda}{\rho} \right) &amp; \qquad \text{if} \; {\vert \theta \vert}_1 &gt; 0, 若：\sum_{t=1}^{T} \left( \frac{\beta_t}{\rho} + w_t\right) - \frac{\lambda}{\rho} &gt; 0 ; \\\<br>\; 0  &amp; \qquad \text{otherwise}.<br>\end{cases}<br>$$</p>
<p>参数：\(\lambda\) 为$L_1$正则项系数；\(\beta_t\)对偶变量（拉格朗日乘子）; \(\theta\)全局参数; \(w_t\)局部参数;</p>
<p>公式中的红色区域是worker向master传递的传递参数，这一步称为merge过程（在MPI对应allreduce操作，Hadoop对应reduce过程）；蓝色区域（全局参数）是master向所有</p>
<blockquote>
<p><a href="">软阈值（Soft-Thresholding）</a>又称压缩算子（shrinkage operator）</p>
</blockquote>
<h4 id="5.2.等式约束的凸优化问题">5.2. 等式约束的凸优化问题</h4>

<h3 id="6.ADMM与统计机器学习">6. ADMM与统计机器学习</h3>

<p>generalized lasso, group lasso, 高斯图模型，Tensor型图模型等问题的求解，都可以在ADMM算法框架上直接应用和实施，这正是ADMM算法的一个优势所在，便于大规模分布式部署。</p>
<table>
<thead>
<tr>
<th>算法框架</th>
<th style="text-align:left">模型</th>
<th>参数学习方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>admm</td>
<td style="text-align:left">Lasso (Group Lasso) <br> Logistic Regression <br> Factorization Machine <br> Filed-awared Factorization Machine</td>
<td>sgd <br> adaptive sgd <br> ftrl <br> lbfgs <br> mcmc <br> …</td>
</tr>
</tbody>
</table>
<h3 id="8-_ADMM应用场景"><h3 id="8.ADMM应用场景">8. ADMM应用场景</h3></h3><ul>
<li>Big-Data Learning </li>
<li>Multi-Task Learning </li>
</ul>
<h3 id="9-_基于MPI的ADMM框架实现"><h3 id="9.基于MPI的ADMM框架实现">9. 基于MPI的ADMM框架实现</h3></h3><h4 id="数据节点">数据节点</h4><h4 id="模型节点">模型节点</h4><h3 id="参考资料">参考资料</h3>

<ul>
<li>论文：Boyd S, Parikh N, Chu E, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers[J]. Foundations and Trends® in Machine Learning, 2011, 3(1): 1-122.</li>
<li><a href="http://web.stanford.edu/~boyd/admm.html" target="_blank" rel="external">斯坦福大学－ADMM网址</a></li>
<li><a href="http://joegaotao.github.io/cn/2014/02/admm/" target="_blank" rel="external">分布式计算、统计学习与ADMM算法</a></li>
</ul>
<p>非常赞：优化算法总结文档：<a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">http://sebastianruder.com/optimizing-gradient-descent/</a><br>优化算法：<a href="http://www.cnblogs.com/neopenx/p/4768388.html" target="_blank" rel="external">http://www.cnblogs.com/neopenx/p/4768388.html</a><br>优化算法；Coupled Group Lasso<br>点击率预估模型经典简介：<a href="http://blog.csdn.net/starzhou/article/details/51769561" target="_blank" rel="external">http://blog.csdn.net/starzhou/article/details/51769561</a><br>论文： Coupled group lasso for web-scale ctr prediction in display advertising</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/分布式机器学习/">分布式机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/ADMM/">ADMM</a><a href="/tags/交替方向乘子法/">交替方向乘子法</a><a href="/tags/受限约束优化/">受限约束优化</a><a href="/tags/梯度提升法/">梯度提升法</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/deep_learning/dl-chapter2-dnn/" title="第02章：深度神经网络" itemprop="url">第02章：深度神经网络</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2017-05-01T12:23:42.000Z" itemprop="datePublished"> 发表于 2017-05-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2017-04-12</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<ul>
<li><a href="#2.0.DNN符号定义">DNN符号定义</a></li>
<li><a href="#2.1神经网络的前向与反向">前向传播与反向传播</a></li>
<li><a href="#2.2.softmax">Softmax函数</a><ul>
<li><a href="#2.2.1.softmax函数由来">softmax函数由来</a></li>
<li><a href="#2.2.2.softmax函数概率模型">softmax函数概率模型</a></li>
<li><a href="#2.2.3.softmax表达式与概率的关系">softmax表达式与概率的关系</a></li>
<li><a href="#2.2.4.SoftmaxOp和SoftmaxCrossEntropyWithLogitsOp">SoftmaxOp和SoftmaxCrossEntropyWithLogitsOp</a></li>
</ul>
</li>
<li><a href="#2.3.激活函数">激活函数</a></li>
<li><a href="#2.4.Normalization">Normalization（规范化）</a><ul>
<li><a href="#2.4.1.为什么需要Normalization">为什么需要Normalization</a> </li>
<li><a href="#2.4.2.Normalization的通用框架">Normalization的通用框架</a></li>
<li><a href="#2.4.3.Batch_Normalization">Batch Normalization</a></li>
<li><a href="#2.4.4.Layer_Normalization">Layer Normalization</a></li>
<li><a href="#2.4.5.Weight_Normalization">Weight Normalization</a></li>
<li><a href="#2.4.6.Cosine_Normalization">Cosine Normalization</a></li>
<li><a href="#2.4.7.Normalization为什么会有效">Normalization为什么会有效</a></li>
</ul>
</li>
</ul>
<p>本章关键词：多层感知机、全连接层、Softmax、激活函数、Normalization、Pooling、Embedding、注意力机制、Transformer等</p>
<h2 id="2.0.DNN符号定义">DNN符号定义</h2>


<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:left">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n^{[0]}$</td>
<td style="text-align:left">表示NN输入层的神经元个数，为了符号统一，规定输入层为第0层</td>
</tr>
<tr>
<td style="text-align:center">$n^{[l]}$</td>
<td style="text-align:left">第$l$层神经元个数</td>
</tr>
<tr>
<td style="text-align:center">$a^{[l-1]}$</td>
<td style="text-align:left">表示NN第$l$层输入信息，一条样本的维度为$(1, n^{[l]})$</td>
</tr>
<tr>
<td style="text-align:center">$W^{[l]}$</td>
<td style="text-align:left">表示第$l$层与$l-1$层之间的连接矩阵，维度：($n^{[l]}$, $n^{[l-1]}$)，即：$W^{[1]} \in R^{n^{[1]} \times n^{[0]}}$</td>
</tr>
<tr>
<td style="text-align:center">$b^{[l]}$</td>
<td style="text-align:left">表示第$l$层的偏置项参数，维度：($n^{[l]}$, $1$)</td>
</tr>
<tr>
<td style="text-align:center">$z^{[l]}$</td>
<td style="text-align:left">表示第$l$层的线性表达$z^{[l]} = W^{[l]} \cdot a^{[l-1]} + b^{[l]}$, 维度：($n^{[l]}$, $1$)</td>
</tr>
<tr>
<td style="text-align:center">$m$</td>
<td style="text-align:left">表示Batch Size</td>
</tr>
</tbody>
</table>
<h2 id="2.1.神经网络的前向与反向">神经网络的前向与反向</h2>

<p>以NN全连接层操作为例：</p>
<p>前向计算</p>
<p>$$<br>z^{[1]} =  a^{[0]} \cdot {W^{[1]}}^T+ b^{[1]},  \\<br>a^{[1]} = \sigma(z^{[1]}), \\\<br>z^{[2]} =  a^{[1]} \cdot {W^{[2]}}^T+ b^{[2]},  \\<br>a^{[2]} = \sigma(z^{[2]}), \\\<br>z^{[3]} =  a^{[2]} \cdot {W^{[3]}}^T+ b^{[3]},  \\<br>a^{[3]} = \sigma(z^{[3]}), \\\<br>\hat{y} = sigmoid(a^{[3]})<br>$$</p>
<p>反向计算</p>
<p>$$<br>\mathcal{L}(a^{[3]};W^{[1]},\; \cdots,\;b^{[3]}) = - y \cdot \log(\hat{y}) - (1 - y) \cdot \log(1-\hat{y}) \\\<br>\mathbf{d}a^{[3]} = \frac{\partial{\mathcal{L}}}{\partial a^{[3]}} = \frac{\partial{\mathcal{L}}}{\partial{\sigma(z^{[3]})}}  = (\hat{y} - y) \\\<br>\mathbf{d}z^{[3]} = \mathbf{d}a^{[3]} \cdot \frac{\partial{a^{[3]}}} {\partial z^{[3]}}, \\\<br>\quad  注：\frac{\partial{a^{[3]}}} {\partial z^{[3]}}是关于z^{[3]}的梯度公式，也是一个关于输入z^{[3]}和输出a^{[3]}的表达式。这里的z^{[3]}，a^{[3]}也是“变量” \\\<br>\mathbf{d}W^{[3]} = \frac{\partial{\mathcal{L}}}{\partial{W^{[3]}}} = \frac{\partial{\mathcal{L}}}{\partial a^{[3]}} \cdot \frac{\partial{a^{[3]}} }{\partial z^{[3]}} \cdot \frac{\partial{z^{[3]}} }{\partial W^{[3]}} = \mathbf{d}z^{[3]} \cdot \frac{\partial{z^{[3]}} } {\partial W^{[3]}} = \mathbf{d}z^{[3]} \cdot a^{[2]} \\\<br>\qquad 注：可以看到，这里是矩阵相乘（前向的输入 * 反向的输入），写代码时⚠矩阵转秩问题 \\\<br>\mathbf{d}a^{[2]} = \frac{\partial{\mathcal{L}}} {\partial{a^{[2]}}} = \mathbf{d}z^{[3]} \cdot \frac{\partial{z^{[3]}}} {\partial{a^{[2]}}} = \mathbf{d}z^{[3]} \cdot W^{[3]} \\\<br>注：这里a^{[2]}也是中间变量，也存在自身前向信息（data)和反向信息(grad) \\\<br>\mathbf{d}b^{[3]} = \mathbf{d}z^{[3]} \cdot \frac{\partial{z^{[3]}}} {\partial{b^{[3]}}} = \mathbf{d}z^{[3]} \\\<br>注：注意维度，这里需要按cols求和 \\\<br>…..<br>$$</p>
<p>由此，总结NN前向计算和反向传播（梯度计算）的通用公式。变量如下：$a^{[l-1]} \in R^{k \times n^{[l-1]}}$, $\mathbf{d}a^{[l-1]} \in R^{k \times n^{l-1}}$, $W^{[l]} \in R^{n^{[l]} \times n^{[l-1]}}$, $b^{[l]} \in R^{1 \times n^{[l]}}$, $z^{[l]} \in R^{k \times n^{[l]}}$, $a^{[l]} \in R^{k \times n^{[l]}}$, $k$表示样本数，如此下去….</p>
<p><strong>FC前向传播通用公式</strong></p>
<p>$$<br>z^{[l]} = a^{[l-1]} \cdot {W^{[l]}}^{\mathrm{T}} + b^{[l]} \\\<br>a^{[l]} = \sigma(z^{[l]}) \\\<br>z^{[l+1]} = a^{[l]} \cdot {W^{[l+1]}}^{\mathrm{T}} + b^{[l+1]} \\\<br>…<br>$$</p>
<p>伪代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y.noalias() = (X * W.transpose()).rowwise() + b.row(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里<code>b.row(0)</code>的维度是(1,n[l])，使用了类似python中的广播机制 维度变为(k,n[l])</p>
</blockquote>
<p><strong>FC反向传播通用公式</strong></p>
<p>$$<br>\mathbf{d}z^{[l]} = \mathbf{d}a^{[l]} \cdot \frac{\partial{a^{[l]}}}{\partial{z^{[l]}}} \\\<br>\mathbf{d}W^{[l]} = \mathbf{d}z^{[l]} \cdot \frac{\partial{z^{[l]}}}{\partial{W^{[l]}}} = {\mathbf{d}z^{[l]}}^{\mathrm{T}} \cdot a^{[l-1]} \\\<br>\mathbf{d}a^{[l-1]} = \mathbf{d}z^{[l]} \cdot W^{[l]}  \\\<br>\mathbf{d}b^{[l]} = \mathbf{d}z^{[l]}   \qquad // 注：需要按cols求和<br>$$</p>
<p>针对NN的全连接$z^{[l]} = {a^{[l-1]}} \cdot {W^{[l]}}^{\mathrm{T}} + b^{[l]}$，反向伪代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dW.noalias() += dY.transpose() * X;</span><br><span class="line">dX.noalias() += dY * W;</span><br><span class="line">db.noalias() += dY.colwise().sum();</span><br></pre></td></tr></table></figure>
<p>深度MLP 小结：</p>
<ol>
<li>前向传播由多层的矩阵运算、连接函数共同组成，分别对应两个Graph Op算子；</li>
<li>反向传播从Loss出发，分别由多层的连接函数、矩阵运算的梯度计算组成；</li>
<li>参与Op算子的变量均要保存当前变量数据（data）和对应的梯度信息（grad），二者shape一致；</li>
</ol>
<h2 id="2.2.softmax">Softmax</h2>

<p>softmax是深度学习模型中重要的操作之一，在分类监督学习、Attention机制中均离不开该操作。 本节的目标是对softmax函数的来龙去脉以及在分类场景的应用讲清楚，安排如下：</p>
<ul>
<li>首先，从数学和概率的角度理解soft（软化的）max的由来；</li>
<li>然后，讲清楚softmax函数为什么可以用在多分类问题上；</li>
<li>最后，给出它在开源深度学习框架tensorflow、openmi（Open-sourced Machine Intelligence）上的实现。</li>
</ul>
<h3 id="2.2.1.softmax函数由来">softmax函数由来</h3>

<p>softmax函数表达式我想大家已经很清楚了，在此赘述：</p>
<p>$$<br>\text{softmax}(a_i) = \frac {\exp(a_i)} {\sum_{k=1}^{K} \exp(a_k)}<br>$$</p>
<p>下面这个函数与上式“貌似”不相同，但是它们又及其相关：</p>
<p>$$<br>g(x, y) = \text{log}_e(e^x + e^y)<br>$$</p>
<p>两个表达式的关联性先不谈，我们先要解释清楚$g(x,y)$函数的由来，就要从hardmax说起。</p>
<p><strong>hardmax函数</strong></p>
<p>softmax函数其实是从hardmax演变而来的，hardmax其实是我们生活中很常见的函数，表达式为：</p>
<p>$$<br>\text{hardmax} = max\{x, y\}<br>$$</p>
<p>物理含义很清楚，就是取x和y中较大的哪个值。如果y=0，可得表达式为$max\{x, 0\}$。画图可知，该函数在$x=0$处是连续不可导的。函数可导可以帮助我们做很多事情，那么有没有办法对其变形，找到一个连续可导的近似函数呢？此时就有了softmax函数$g(x,y) = \text{log}_e(e^x + e^y)$。</p>
<blockquote>
<p>在基于梯度法求解机器学习模型最优解的过程中，函数可导是一个必要的前提条件。</p>
</blockquote>
<p>我们先来看$g(x,y)$函数的数学特性。指数函数有一个特点，就是变化率非常快。当$x &gt; y$时，通过指数的放大作用，使得二者的差距进一步变大，即$e^x \gg e^y$。所以有$e^x + e^y \approx e^x$，那么此时$g(x,y) = \text{log}_e(e^x + e^y) \approx \text{log}_e(e^x) = x$。根据推导过程，可以知道：<strong>$g(x,y)$约等于$x,y$中较大的值，即$g(x,y) \approx \text{max}(x,y)$</strong>。</p>
<p>所以我们得出一个结论：$g(x,y)$是$max(x,y)$的近似函数，两个函数有相似的数学特性。两个函数的图形如下：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/deep_learning/dl_chapter2_softmax_and_hardmax.png" ;="" width="400" height="300" alt="softmax与hardmax"></p>
<p>可以看出，当$x,y$的差别越大时，$g(x,y)$和hardmax函数吻合度越高。同时也可以看出，softmax是一个连续且处处可导的函数，这时一个非常重要的特性：<strong>$g(x,y)$即具有与$\text{max}(x,y)$的相似性，又避免了$\text{max}(x,y)$函数会出现连续不可导的情况</strong>。</p>
<h3 id="2.2.2.softmax函数概率模型">softmax函数概率模型</h3>

<p>本节我们回答一个问题：上面提到的hardmax近似函数$g(x,y)$与神经网络分类层的softmax函数是什么关系？</p>
<p>假设我们要从a,b两个未知数中取一个较大的值，即$\text{max}(a,b)$，那么<strong>取到两个数字的概率分别是多少</strong>呢？我们可以构建一个概率模型：</p>
<ul>
<li>当a=b时，取任意一个数字都可以，所以各占50%，即$p(a)=p(b)=50\%$；</li>
<li>当a=4,b=6时，b比a大，此时更倾向于b，取到b的概率应该会更大些，即$p(a)=40\%,p(b)=60\%$；</li>
<li>当a=8,b=2时，a比b大，此时更倾向于a，取到a的概率应该会更大些，即$p(a)=80\%,p(b)=20\%$.</li>
</ul>
<p>当变量变多时，此时hardmax变成了$\text{max}(a_1, a_2, \cdots, a_K)$，根据假设的概率模型，取到每一个样本点的概率即为$p(a_i) = \frac{a_i}{\sum_{k=1}a_k}$</p>
<p>上述表达式与softmax“貌似”很熟，其实对于机器学习多分类问题来说，训练过程中需要对参数求导得到其梯度信息再用来更新参数，从上一节可知hardmax函数并不是处处可导的，那么此时就用hardmax的近似函数softmax来替代它，我们已知了二者之间的关系：</p>
<p>$$<br>f(a_1, a_2, \cdots, a_K) = \text{max}(a_1, a_2, \cdots, a_K) \approx g(a_1, a_2, \cdots, a_K) = \text{log}_e(e^{a_1} + e^{a_2} + \cdots + e^{a_K})<br>$$</p>
<p>因此，根据hardmax函数表达式，得出softmax的概率模型表达式为：</p>
<p>$$<br>\text{softmax}_{(i)} = \frac{e^{a_i}} {\sum_{k=1}^K e^{a_k}}<br>$$</p>
<p>以上就是softmax函数的由来。再次梳理下逻辑：softmax是hardmax的近似函数，也是对hardmax函数的优化（解决了hardmax不可导问题）。<strong>softmax函数＋概率模型</strong>得到了深度学习结构中常见的softmax概率模型表达式。</p>
<p>$$<br>\text{softmax}函数＋概率模型 \longrightarrow \text{softmax}概率模型表达式<br>$$</p>
<h3 id="2.2.3.softmax表达式与概率的关系">softmax表达式与概率的关系</h3>

<p>根据概率的定义，一个事件发生的概率这里用$p\in [0,1]$表示，事件一定发生的概率是1，不会发生的概率是0，事件结果所有可能性的概率之和等于1。</p>
<p>以神经网络最后一层输出为例，经过softmax层计算后的表达式为：</p>
<p>$$<br>s_i = \frac{e^{z_i}} {e^{z_1} + e^{z_2} + \cdots + e^{z_K}}  \qquad s_i \in [0,1]<br>$$</p>
<p>对所有神经元s求和，得到：</p>
<p>$$<br>\sum_{i=1}^K s_i = \frac {e^{z_1} + e^{z_2} + \cdots + e^{z_K}} {e^{z_1} + e^{z_2} + \cdots + e^{z_K}} = 1<br>$$</p>
<p>由此可知，神经网络输出层的数据，经过softmax层转换后的结果，即可满足概率的特性。因此，FC输出层的数据转化为概率表达式后的输出值就有了新的意义。对应分类问题来说，输出值表示这个结果属于不同类别的可能性大小。</p>
<h3 id="2.2.4.SoftmaxOp和SoftmaxCrossEntropyWithLogitsOp">SoftmaxOp和SoftmaxCrossEntropyWithLogitsOp</h3>


<p>SoftmaxOp多用于多分类问题，假设为K类，那么属于第i类的概率为：</p>
<p>$$<br>p_i = \frac {\exp(a_i)} {\sum_{k=1}^{K} \exp(a_k)}<br>$$</p>
<p><strong>Softmax计算优化</strong></p>
<p>因为是指数计算，为了避免_指数越界出现NAN问题_，通常会分别对分子分母乘以一个常量C，即</p>
<p>$$<br>p_i = \frac{\exp(a_i)} {\sum_{k=1}^{K} \exp(a_k) } = \frac{C \exp(a_i)} { C  \sum_{k=1}^{K} \exp(a_k) } = \frac {\exp(a_i + \log(C))} {\sum_{k=1}^{K} \exp(a_k + \log(C))}<br>$$</p>
<p>通常$\log(C)=-\max(a)$，如此相当于K维的向量a取值平移到[-, 0]之间，指数最大值是1而不是正无穷，避免了Nan问题。</p>
<p><strong>Softmax梯度</strong></p>
<p>$$<br>\frac {\partial p_i} {\partial a_j} = \frac {\partial \frac {\exp(a_i)} {\sum_{k=1}^{K} \exp(a_k)}} {\partial a_j}<br>$$</p>
<p>根据除法导数公式$f(x) = \frac {g(x)} {h(x)}$，存在$f’(x) = \frac {g’(x)h(x) - g(x)h’(x)} {h^2(x)}$。在这里$g(x) = \exp(a_i)$, $h(x) = \sum_{k=1}^K \exp(a_k)$。</p>
<p>对于$\frac {\partial h(x)} {\partial a_j}$，它的导数总是$\exp(a_j)$；对于$\frac{g(x)}{\partial a_j}$，分两种情况如果$i=j$ 梯度为$\exp(a_j)$，否则梯度为0（对于$a_j$来说，$a_i$是常数）。</p>
<p>所以Softmax梯度公式分两种情况:</p>
<p>当$i=j$时，</p>
<p>$$<br>\frac {\partial p_i} {\partial a_j} = \frac {\partial \frac {\exp(a_i)} {\sum_{k=1}^{K} \exp(a_k)}} {\partial a_j} = \frac {\exp(a_i) \cdot \sum_{k=1}^{K} \exp(a_k) - \exp(a_i) \cdot \exp(a_j)} {\left(\sum_{k=1}^{K} \exp(a_k)\right)^2} \\\<br>= \frac {\exp(a_i) } {\sum_{k=1}^{K} \exp(a_k)} \cdot \frac {\left(\sum_{k=1}^{K} \exp(a_k) - \exp(a_j) \right)} {\sum_{k=1}^{K} \exp(a_k)} = p_i * (1- p_j)<br>$$</p>
<p>当$i \neq j$时，</p>
<p>$$<br>\frac {\partial p_i} {\partial a_j} = \frac {\partial \frac {\exp(a_i)} {\sum_{k=1}^{K} \exp(a_k)}} {\partial a_j} = \frac {0 - \exp(a_i) \cdot \exp(a_j)} {\left(\sum_{k=1}^{K} \exp(a_k)\right)^2} = - p_i \cdot p_j<br>$$</p>
<p>因此Softmax梯度公式为：</p>
<p>$$<br>f(x) = \begin{cases} \displaystyle<br>p_i \cdot (1 - p_j) &amp; \text{if} \; i=j \\\<br>-p_i \cdot p_j &amp; \text{if} \; i \neq j<br>\end{cases}<br>\longrightarrow<br>\mathbf{\frac{\partial p_i} {\partial a_j} = p_i (\delta_{ij} - p_j)}, \;<br>\delta_{ij} = \begin{cases} \displaystyle<br>1 &amp; \text{if} \; i = j \\\<br>0 &amp; \text{if} \; i \neq j<br>\end{cases}<br>$$</p>
<h2 id="交叉熵损失（Cross_Entropy_Loss）">交叉熵损失（Cross Entropy Loss）</h2><p>交叉熵用来衡量<strong>相同事件空间里两个概率分布的差异</strong>。应用到分类问题里，就是实际label输出（概率）与期望输出（概率）之间的距离，交叉熵越小，说明两个概率分布越接近。这里假设概率分布Y为实际输出，P为期望输出，H(Y,P)为交叉熵，那么有：</p>
<p>$$<br>H(Y,P) = L(P) = - \sum_{x \in X} Y(x) \cdot \log P(x)<br>$$</p>
<p>具体含义和解释，详见<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/#%E7%9B%B8%E5%AF%B9%E7%86%B5%E3%80%81KL%E8%B7%9D%E7%A6%BB" target="_blank" rel="external">52CAML交叉熵</a>定义</p>
<p><strong>Softmax交叉熵损失梯度 with Logits</strong></p>
<blockquote>
<p>注意：交叉熵损失函数对logits的梯度，而不是对softmax的梯度。</p>
</blockquote>
<p>$$<br>\begin{align}<br>\frac {\partial L} {\partial a_i} &amp;= - \sum_{k=1}^{K} y_k \cdot \frac{\partial \log(p_k)} {\partial a_i} = - \sum_{k=1}^{K} y_k \cdot \frac{\partial \log(p_k)} {\partial p_k} \cdot \frac{\partial p_k} {\partial a_i} \\\<br>&amp;= - \sum_{k=1}^K y_k \frac{1}{p_k} \cdot \frac{\partial p_k} {\partial a_i} = - \sum_{k=1}^K y_k \frac{1}{p_k} \cdot \left(p_k (\delta_{ki} - p_i) \right) \\\<br>&amp;= - \sum_{k=1}^K y_k (\delta_{ki} - p_i) = - y_i (1 - p_i) + \sum_{k \neq i} y_k p_i \\\<br>&amp;= p_i \cdot \sum_{k=1}^K y_k  - y_i = \mathbf{p_i - y_i}<br>\end{align}<br>$$</p>
<p>最后，针对分类问题，给定的结果$y_k$最终只会有一个类别是1，其他类别都是0，所以有$\sum_k y_k = 1$。所以基于Softmax函数的交叉熵损失梯度（对于logits $a_i$）表达式为：</p>
<p>$$<br>\mathbf{\frac{\partial L} {\partial a_i} = p_i - y_i}<br>$$</p>
<h2 id="2.4.Normalization">Normalization</h2>

<p>参考：<a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/33173246</a></p>
<h3 id="2.4.1.为什么需要Normalization">为什么需要Normalization</h3>

<p><strong>独立同分布与白化</strong></p>
<p>机器学习最喜欢的数据特点莫过于“独立同分布”了，即Independent and Identically Distributed，简称IID。独立同步分并非所有机器学习模型的必然要求（比如Naive Bayes模型就建立在特征彼此独立的基础上，而LR和神经网络则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已是一个共识。</p>
<p>因此，在把数据喂给机器学习模型之前，“白化（whitening）”是一个重要的数据预处理步骤。whitening一般包含两个目的：</p>
<ul>
<li>去除特征之间的相关性$\rightarrow$独立；</li>
<li>使得所有特征具有相同的均值和方差$\rightarrow$同分布；</li>
</ul>
<blockquote>
<p>白化最典型的方法是PCA（主成分分析）。whitening的目的是获得独立同分布的数据，让模型更容易训练。</p>
</blockquote>
<p><strong>深度学习中的Internal Covariate Shift</strong></p>
<p>深度学习网络模型的训练为什么比较困难？其中一个重要原因是：深度神经网络涉及多层的叠加，而每一层的参数更新会导致上一层的输入数据分布发生变化，通过层层叠加，高层的数据分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训练好模型，我们需要非常谨慎的设定学习率、初始化权重以及尽可能细致的参数更新策略。</p>
<p>Google将这一现象总结为<strong>Internal Covariate Shift</strong>，简称ICS。</p>
<blockquote>
<p>什么是ICS呢？</p>
<p>统计机器学习中的一个经典假设是“<strong>源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的</strong>”。如果不一致，那么就出现了新的机器学习问题，如transfer learning／domain adaption等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指<strong>源空间与目标空间的条件概率是一致的，但是边缘概率不同</strong>，即对于所有的$x \in \mathcal{X}，有：$</p>
<p>$$<br>P_s(Y | X=x) = P_t(Y | X=x) \\ \quad P_s(X) \neq P_t(X)<br>$$</p>
<p>的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能指示的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对各层间信号的分析，也即是“internal”的由来。</p>
</blockquote>
<p><strong>Internal Covariate Shift会导致什么问题？</strong></p>
<p>深度神经网络，每个神经元的输入数据不再是“独立同分布”。具体表现在：</p>
<ul>
<li>上层参数需要不断<strong>适应</strong>新的输入数据分布，降低学习速度；</li>
<li>下层输入的变化问题趋向于变大或变小，导致上层落入<strong>饱和区</strong>，使得学习过早停止；</li>
<li>每层的更新都会影响到其它层，因此每层的<strong>参数更新</strong>策略需要尽可能的谨慎。</li>
</ul>
<h3 id="2.4.2.Normalization的通用框架">Normalization的通用框架</h3>

<p>这里我们以神经网络中的一个神经元为例，神经元接收一组输入向量$\mathbf{x}=(x_1, x_2, \cdots, x_d)$，通过中间运算后，输出一个标量值$y=f(\mathbf{x})$。</p>
<p>由于ICS问题的存在，$\mathbf{x}$的分布可能相差很大，要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行whitening操作。然而标准的白化操作代价很大，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。</p>
<p>因此，以Batch Normalization为代表的Normalization方法退而求其次，进行了简化的白化操作。基本思想是：在将$\mathbf{x}$送给神经元之前，先对其做<strong>平移和伸缩变换</strong>，将$\mathbf{x}$的分布规范化成在<strong>固定区间范围的标准分布</strong>。</p>
<p>通用变换框架如下所示：</p>
<p>$$<br>h = f \left( \mathbf{g} \cdot \frac{x-\mu}{\sigma} + \mathbf{b} \right)<br>$$</p>
<p>参数解释：</p>
<ul>
<li>$\mu$是平移参数（shift parameter），$\sigma$是缩放参数（scale parameter）。通过这两个参数进行shift和scale变换，得到的数据符合均值为0、方差为1的标准分布。</li>
<li>$\mathbf{b}$是再平移参数（re-shift parameter），$\mathbf{g}$是再缩放参数（re-scale parameter）。将上一步得到的结果进一步变换，最终得到的数据符合均值为$\mathbf{b}$、方差为$\mathbf{g}^2$的分布。</li>
</ul>
<p>第一步已经将x变成了标准分布，为什么第二步又变走了？答案是<strong>为了保证模型的表达能力不因规范化而下降。</strong></p>
<p>我们可以看到，第一步的变换是将输入数据限制到了一个全局统一的确定范围（均值0，方差1）。下层神经元可能很努力的在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴的重新调整到这一固定范围。</p>
<p>难道下层神经元在做无用功？</p>
<p>为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是<strong>针对该神经元量身定制的一个确定范围（均值为$\mathbf{b}$，方差为$\mathbf{g}^2$）</strong>。reshift和rescale的参数都是可以学习的，这就使得Normalization层可以学习如何去尊重下层的学习结果。</p>
<p>除了充分利用下层学习的能力，另一方面的重要意义在于保证<strong>获得非线性的表达能力</strong>。</p>
<p>经过这么变来变去，会不会跟没变一样？</p>
<p>不会。因为再变换引入的两个新参数$\mathbf{b}$和$\mathbf{g}$可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。<strong>在旧参数中，$\mathbf{x}$的均值取决于下层神经网络的复杂关联；但在新参数中，$\mathbf{y = g \cdot \hat{x} + b}$仅由$mathbf{b}$来决定，去除了与下层计算的密切耦合</strong>。新参数很容易通过梯度下降来学习，简化了神经网络的训练。</p>
<p>这里的Normalization离标准的白化还有多远？标准白化操作的目的是独立同分布。独立暂不考虑，变换为均值为$\mathbf{b}$、方差为$\mathbf{g}^2$的分布，并不是严格的同分布，只是映射到了一个确定的区间范围而已。</p>
<p>下面梳理下深度学习模型主流的Normalization方法。分别图示如下：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/deep_learning/dl_chapter2_normalization.png" ;="" width="400" height="120" alt="normalization"></p>
<h3 id="2.4.3.Batch_Normalization">Batch Normalization </h3>

<p>Batch Normalization是Google在2015年提出，开Normalization之先河。其规范化操作针对单个神经元进行，利用网络训练时一个batch的数据来计算该神经元$x_i$的均值和方差，因而称为BN。公式表示如下：</p>
<p>$$<br>\mu_i = \frac{1}{M} \sum x_i, \quad \sigma_i = \sqrt{\frac{1}{M} \sum(x_i - \mu_i)^2 + \epsilon}  \qquad(M为\text{batch size})<br>$$</p>
<p>相对于一层神经元的水平排列，BN可以看作是一种纵向的规范化。由于BN是针对单个维度定义的，因此标准公式中的计算均为element wise的。</p>
<p>BN适合的场景：每个mini batch比较大，且batch间分布差距较小，数据分布比较接近。在训练之前，要做好充分的shuffle，否则效果会差很多。</p>
<p>BN不适合的场景：由于BN需要在运行的过程中统计每个batch的一阶和二阶统计量，因此不适用于动态神经网络和RNN网络。</p>
<h3 id="2.4.4.Layer_Normalization">Layer Normalization</h3>

<p>与BN不同，LN是一种横向的规范化。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。公式表示为：</p>
<p>$$<br>\mu = \sum_i x_i, \quad \sigma = \sqrt{\sum_i (x_i - \mu)^2 + \epsilon}<br>$$</p>
<p>其中$i$表示一层所有神经元中第$i$个神经元。对应到通用框架中，四大参数$\mu, \sigma, \mathbf{g}, \mathbf{b}$均是标量（BN中是向量），所有输入共享一个规范化变换。</p>
<p>LN的适用场景：针对单个训练样本进行，不依赖其它数据，因此可以避免BN中受batch数据分布影响的问题，可以用于小batch的场景、动态神经网络和RNN，特别是NLP另一。此外，LN不需要保存batch的均值和方差，节省了额外的存储空间。</p>
<p>综合比较BN和LN：</p>
<ul>
<li>BN是针对单个神经元可训练的，不同神经元的输入经过再平移和再缩放后分布在不同的区间；</li>
<li>LN对于一整层的神经元训练得到同一个转换，所有的输入都在同一个区间范围内。</li>
</ul>
<blockquote>
<p>如果不同输入特征不属于相似的特征，那么LN的处理可能会降低模型的表达能力。</p>
</blockquote>
<h3 id="2.4.5.Weight_Normalization">Weight Normalization </h3>

<p>BN和LN均将规范化应用于输入的特征数据$\mathbf{x}$，而WN则另辟蹊径，将规范化应用于线性变换函数的权重$w$，这就是WN名称的由来。</p>
<p>WN的方案是：将权重向量$\mathbf{w}$分别为向量方向$\mathbf{\hat{v}}$和向量模$\mathbf{g}$两部分，即$\mathbf{w} = g \cdot \hat{\mathbf{v}} = g \cdot \frac{\mathbf{v}} {||\mathbf{v}||}$。其中，$\mathbf{v}$是与$\mathbf{w}$同维度的向量，$||\mathbf{v}||$是欧式范数，$g$是标量决定了w的长度。由于$||\mathbf{w}|| \equiv |g|$，因此这一权重分解的方式将权重向量的欧式范数进行了固定，从而实现了正则化的效果。</p>
<p>WN的公式貌似脱离了前面讲的通用框架？其实并没有，从最终实现的效果来看，异曲同工。公式推导如下：</p>
<p>$$<br>\begin{align}<br>f_w(WN(\mathbf{x})) &amp;= \mathbf{w} \cdot WN(\mathbf{x}) = g \cdot \frac{\mathbf{v}}{||\mathbf{v}||} \cdot \mathbf{x} \\\<br>&amp;= \mathbf{v} \cdot g \cdot \frac{\mathbf{x}}{||\mathbf{v}||} = f_v(g \cdot \frac{\mathbf{x}}{||\mathbf{v}||})<br>\end{align}<br>$$</p>
<p>对照通用框架，WN只需令：$\sigma=||\mathbf{v}||, \mu=0, b=0$。</p>
<p>对比BN／LN与WN的区别：<strong>BN和LN是用输入的特征数据的方差进行scale（特征数据规范化），而WN则是用神经元权重的欧式范数对输入数据进行scale（权重规范化），本质上都上线了是对数据的规范化，只是用于scale的参数来源不同。</strong></p>
<p>WN的规范化不直接使用输入数据的统计量，因此避免了BN过于依赖batch的不足，以及LN每层唯一转换器的限制，同时也可以用于动态网络结构。</p>
<p>另外，我们可以看到WN只是对数据做了scale，没有进行shift，因为我们简单的令$\mu=0$，但事实上，这里留下了与BN（或LN）相结合的余地，即利用BN（或LN）的方法来计算输入数据的均值$\mu$。</p>
<h3 id="2.4.6.Cosine_Normalization">Cosine Normalization</h3>

<p>我们先来看神经元的经典变换：$f_w(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x}$。</p>
<p>对输入数据$\mathbf{x}$的变换有BN和LN；对模型参数$\mathbf{w}$的变换也已经做过了-WN。好像没啥好做的了。</p>
<p>然而，研究者们对中间的那个dot打起了“主意”，他们认为，之所以对数据进行规范化，是因为数据经过NN的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源是我们的计算方式－点积－权重向量$\mathbf{w}$和特征数据向量$\mathbf{x}$的点积。向量点积是无界的（unbounded）。</p>
<p>向量点积是衡量两个向量相似度的方法之一，为了保证有界，使用余弦夹角计算方式，于是Cosine Normalization就诞生了。线性变换函数如下：</p>
<p>$$<br>f_w(\mathbf{x}) = \cos (\theta) = \frac{\mathbf{w} \cdot \mathbf{x}} {||\mathbf{w}|| \cdot ||\mathbf{x}||}<br>$$</p>
<p>CN与WN时比较相似的，分子都是\mathbf{w}和\mathbf{x}的内积，CN则是在WN的基础上加上了输入向量的模$||\mathbf{x}||$对输入向量进行了进一步的scale。</p>
<h3 id="2.4.7.Normalization为什么会有效">Normalization为什么会有效</h3>

<p>我们以一个简化的神经网络为例，解释Normalization为什么会有效？</p>
<p>神经网络结构表示为：$\text{x} \rightarrow FC_1 \rightarrow \text{wx} \rightarrow Norm(wx) \rightarrow FC_2 \rightarrow w_2 \cdot \text{Norm(wx)}$</p>
<ul>
<li><strong>Normalization的权重伸缩不变性</strong></li>
</ul>
<p>权重伸缩不变性（weight scale invariance）指的是，当权重$w$按照常量$\lambda$进行伸缩时，得到的规范化后的值保持不变，即$Norm(W’x) = Norm(Wx)$，其中$W’ = \lambda W$。</p>
<p>该性质对上述N规范化方法均成立，因为当权重$W$伸缩时，对应的均值和标准差均等比例伸缩，分子分母相抵。</p>
<p>$$<br>\begin{align}<br>Norm(W’x) &amp;= Norm \left(g \cdot \frac{W’x - \mu’}{\sigma’} + b \right) \\\<br>&amp;= Norm \left(g \cdot \frac{\lambda Wx - \lambda \mu}{\lambda \sigma} + b \right) = Norm(Wx)<br>\end{align}<br>$$</p>
<p><strong>伸缩权重不变性可以有效地提高反向传播的效率。</strong> 由于$\frac{\partial Norm(W’x)} {\partial x} = \frac{\partial Norm(Wx)} {\partial x}$，因此，权重的伸缩变化不会影响反向梯度的Jacobian矩阵，也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练。（？？？需要进一步理解）</p>
<p><strong>权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率</strong>。由于$\frac{\partial Norm(W’x)} {\partial W’} = \frac{1}{\lambda} \cdot \frac{\partial Norm(Wx)} {\partial W}$，因此，下层的权重值越大，其梯度值越小，这样参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。</p>
<ul>
<li><strong>Normalization的数据伸缩不变性</strong></li>
</ul>
<p>数据伸缩不变性（data scale invariance）指的是，当数据$\mathbf{x}$按照常量$\lambda$进行伸缩时，得到的规范化后的值保持不变，即$Norm(Wx’) = Norm(Wx)$，其中$x’=\lambda x$。</p>
<p><strong>数据伸缩不变性仅对BN、LN和CN成立</strong>。因为这三者对数据进行规范化，因此当数据进行常量伸缩时，其均值和方差都会相应变化，分子分母相互抵消。</p>
<p><strong>数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择</strong>。对于某一层神经元$h_l=f_{w_l}(x_l)$而言，展开可得</p>
<p>$$<br>h_l=f_{w_l}(x_l) ＝ f_{w_l} \left( f_{w_{l-1}}(x_{l-1}) \right) = \cdots = x_0 \prod_{k=0}^{l} w_k<br>$$</p>
<p>每一层神经元的输出依赖于底下各层的计算结果。如果没有正则化，当下层发生伸缩变化时，经过层层传递，可能会出现数据发生剧烈的膨胀或弥散，从而导致了反向计算时的梯度爆炸或<a href="https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98/22761355?fr=aladdin" target="_blank" rel="external">梯度弥散</a>。</p>
<blockquote>
<p>梯度弥散，又称梯度消失（vanishing gradient problem）。</p>
<ul>
<li>梯度弥散：接近loss的layer梯度变化较大，远离loss的layer梯度变化越小，参数更新慢，几乎处于初始状态；</li>
<li>梯度爆炸：接近loss的layer梯度变化较小，远离loss的layer梯度指数级增大，最终超出了实数值范围，出现NaN；</li>
</ul>
</blockquote>
<p>加入Normalization后，不论底层的数据如何变化，对于某一层神经元$h_l = f_{w_l}(x_l)$而言，其输入$x_l$永远保持标准的分布，这就使得高层的训练更加简单。从梯度计算公式来看：</p>
<p>$$<br>\frac{\partial Norm(Wx’)} {\partial W} = \frac{\partial Norm(Wx)} {\partial W}<br>$$</p>
<p>数据的伸缩变化也不会影响到对该层的权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。</p>
<p>本节参考资料：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="external">详解深度学习中的Normalization，BN/LN/WN</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深度学习/">深度学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Attention/">Attention</a><a href="/tags/Pooling/">Pooling</a><a href="/tags/Softmax/">Softmax</a><a href="/tags/反向传播/">反向传播</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/openmit/openmit-chapter6-optimizer/" title="第06章：OpenMIT-优化器" itemprop="url">第06章：OpenMIT-优化器</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2017-04-22T14:20:31.000Z" itemprop="datePublished"> 发表于 2017-04-22</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2017-04-08</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li><a href="#0.写在前面">写在前面</a></li>
<li><a href="#1.随机优化算法">随机优化算法</a><ul>
<li><a href="#1.1.Gradient Descent">Gradient Descent</a></li>
<li><a href="#1.2.Adaptive Gradient Descent">AdaGrad</a> </li>
<li><a href="#1.3.RMSProp">RMSProp</a></li>
<li><a href="#1.4.AdaDelta">AdaDelta</a></li>
<li><a href="#1.5.Adam">Adam</a></li>
<li><a href="#1.6.Follow The Regularized Leader">FTRL</a></li>
<li><a href="#1.7.Follow the Moving Leader">FTML</a></li>
<li><a href="#1.8.LBFGS">LBFGS</a></li>
<li><a href="#1.9.Alternating Least Square">ALS</a></li>
</ul>
</li>
</ul>
<ul>
<li><a href="#2.概率推理">概率推理</a><ul>
<li><a href="#2.1.MCMC">MCMC</a></li>
</ul>
</li>
</ul>
<h2 id="0.写在前面">写在前面</h2>


<p>随机优化算法（Stochastic Optimization）是指每次随机采样一个或少量几个（Mini-batch）训练样本对模型更新的优化方法。因其有内存消耗低、单次迭代计算复杂度低等优点，被广泛应用于大规模机器学习模型训练中，包括绝大多数深度学习模型的训练。本章要介绍的优化器（Optimizer）涵盖了大规模机器学习和深度学习最常用的优化算法。大致可将其分为以下3类：</p>
<ol>
<li>一阶随机优化。如SGD，AdaGrad，RMSProp，AdaDelta，Adam，FTRL，FTML等；</li>
<li>二阶随机优化。如LBFGS等；</li>
<li>非凸随机优化。</li>
</ol>
<p>此外，针对机器学习中的无监督学习，很多无法转化为非优化问题，需要应用<strong>概率推理</strong>算法求解，比如MCMC，EM等，在本章的后面也会介绍。</p>
<h2 id="1.随机优化算法">随机优化算法</h2> 


<h2 id="1.1.Gradient Descent">Gradient Descent</h2>


<p>梯度下降算法（简称GD）是求解机器学习问题（尤其是凸优化问题）最常用的求解算法。其中，随机梯度下降和批量梯度下降时两种常见的迭代优化思路。</p>
<p>假如优化目标是平方损失函数，以<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>公式$(ml.1.1.3)$为例：</p>
<p>$$<br>\min_{w} \quad J(w) = \frac{1}{2m} \sum_{i=1}^{m} \left(h_{w}(x^{(i)}) - y^{(i)} \right)^2<br>$$</p>
<p>看几类梯度下降算法是如何求解的？</p>
<h3 id="Batch_Gradient_Descent">Batch Gradient Descent</h3><hr>
<p>参数的每一轮迭代是基于所有数据集，对目标函数$J(w)$求偏导，得到每个$w_i$对应的梯度：</p>
<p>$$<br>\nabla_{w_j} = \frac{\partial{J(w)}}{\partial{w_j}} = \frac{1}{m} \sum_{i=1}^{m} \left(h_w(x^{(i)}) - y^{(i)}\right) \cdot x_{j}^{(i)}<br>$$</p>
<p>BGD算法的特点很明显：每次学习都使用整个训练集，即先计算损失函数在<strong>整个训练集</strong>上的梯度方向，着该方向search下一个迭代点。</p>
<ul>
<li><p>优点：在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点</p>
<blockquote>
<p>凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点</p>
</blockquote>
</li>
<li><p>缺点：在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能用于Online模型参数更新。</p>
</li>
</ul>
<h3 id="Stochastic_Gradient_Descent">Stochastic Gradient Descent</h3><hr>
<p>区别于批量梯度下降，这里损失函数对应的训练集中每个样本的粒度，每个样本的损失函数如下：</p>
<p>$$<br>J\left(w; (x^{(i)}, y^{(i)})\right) = \frac{1}{2} \left(y^{(i)} - h_w(x^{(i)}) \right)^2<br>$$</p>
<p>求每个参数$w_j$的梯度：</p>
<p>$$<br>\nabla_{w_j} = \frac{\partial J(w; (x^{(i)}, y^{(i)}))} {\partial w_j} = \left(h_w(x^{(i)}) - y^{(i)}\right) \cdot x_j^{(i)}<br>$$</p>
<h3 id="Mini-Batch_Gradient_Descent">Mini-Batch Gradient Descent</h3><hr>
<p>BGD的参数更新方式计算复杂度要比SGD高很多，大规模机器学习任务一般不采用BGD来训练模型（主要原因是效率低，以我们的CTR预估模型来说，每次训练时的样本量在10亿级别）；而SGD求解时带来的波动较大。这里的mini-batch方式综合了BGD和SGD之间的优点，在参数更新速度和更新次数之间取一个tradeoff。</p>
<p>这里假设每个mini-batch有b个样本，训练集被划分为$K$个batch数据。对应的梯度计算公式与BGD相同，只是样本集有变化，即：</p>
<p>$$<br>\nabla_{w_j} = \frac{\partial J(w)}{\partial w} = \frac{1}{b} \sum_{i=1}^{b} \left(h_w(x^{(i)}) - y^{(i)}\right) \cdot x_j^{(i)}<br>$$ </p>
<p>   相对于随机梯度下降，mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。</p>
<h3 id="Online_Gradient_Descent">Online Gradient Descent</h3><hr>
<p>Online GD是把梯度下降算法应用到了在线学习（Online Learning）领域。互联网很多应用场景都是实时的，以我们的推荐广告业务系统来说，首先request和response是实时的，过程中不断的产生新数据。而Online Learning要解决的就是充分利用实时数据来更新模型，进而捕捉实时行为用于实时反馈，这也是Online Learning的优势。</p>
<p>Online Learning是实时更新模型，因此训练数据只用一次，然后就丢弃。并且他与基于批量数据集训练模型不同，它不需要样本满足独立同分布（IID）的假设。</p>
<p>Online GD参数更新方式与SGD相同，不再赘述。</p>
<h3 id="GD参数更新">GD参数更新</h3><hr>
<p>不管是BGD、BGD、mini-batch GD还是Online GD，最小化优化目标，都需要按照每个参数的<strong>梯度负方向</strong>来更新参数，即：</p>
<p>$$<br>w_j \leftarrow w_j - \Delta{w_j}; \qquad  \Delta{w_j} = \eta \cdot \nabla_{w_j}<br>$$</p>
<p>其中$\eta$为学习率（又称步长）。</p>
<h3 id="GD问题与挑战">GD问题与挑战</h3><hr>
<p>虽然梯度下降算法效果很好，并且应用广告，但同时也存在一些问题待解决：</p>
<ol>
<li><p><strong>如何选择一个比较合理的学习率？</strong>如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。</p>
</li>
<li><p><strong>如何调整学习率？</strong> 可以尝试在每次更新过程中，改变学习速率。这种一般需要使用某种事先设定的策略抑或在每次迭代中衰减一个较小的阈值。无论如何调整，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点。</p>
</li>
<li><p><strong>所有模型参数每次更新都是使用相同的学习率</strong>。如果数据特征是稀疏的或者每个特征的分布或统计特征有差异，那么就不能在更新时每个参数使用相同的学习速率，对于很少出现的特征应该使用一个相对较大的学习速率。</p>
</li>
<li><p><strong>如何避免在求解非凸优化问题时，容易陷入到次优的局部极值点？</strong> 比如深度神经网路优化问题。</p>
</li>
</ol>
<h2 id="1.2.Adaptive Gradient Descent">Adaptive Gradient Descent</h2>


<p>自适应梯度下降算法，简称AdaGrad。与GD不同的是，AdaGrad在更新参数时，学习率不在设定为固定值。每次迭代过程中，每个参数更新时使用不同的学习率。</p>
<p>假设在第$k$轮迭代，$\nabla_{k,w_j}$表示目标函数对参数$w_j$的梯度。普通的GD算法，对于所有的$w_j$使用相同的学习率，因此在第$k$次迭代时，参数$w_j$的变化过程如上述公式<a href="">GD参数更新</a>.</p>
<p>而AdaGrad更新参数的规则，学习率$\eta$会随着迭代过程并根据历史梯度的变化而变化。如下所示：</p>
<p>$$<br>w_{j} \leftarrow w_j - \frac{\eta}{\sqrt{n_{k,w_j} + \epsilon}} \cdot \nabla_{k, w_j}<br>$$</p>
<p>$n_{k,w_j} = \sum_{i=1}^{k} \nabla_{i,w_j}$表示参数$w_j$在1到第$k$次梯度平方的累加和。$\epsilon$用于平滑，保证分母不为0，一般取较小值（如1e-8）。</p>
<p>AdaGrad算法总结如下：</p>
<ol>
<li><p>开始时$\sqrt{n_{k,w_j}}$较小，学习率较大，相当于放大梯度，加快寻找最优解的步伐；后期$\sqrt{n_{k,w_j}}$越来越大，学习率越来越小，能够约束梯度。</p>
</li>
<li><p>仍然需要依赖于人工设置的一个全局学习率$\eta$用于初始化。$\eta$如果初始过大，会使整个学习率变大，对梯度的调节也大；</p>
</li>
<li><p>随着迭代进行，分母上梯度平方累加和将会越来越大，使$w_j$的更新量$\rightarrow 0$，导致学习率急剧下降，训练提前结束。</p>
</li>
<li><p>更新$w_j$时，左右两边的单位不同一。</p>
</li>
</ol>
<h2 id="1.3.RMSProp">RMSProp</h2>

<p>均方根传播算法（Root Mean Square Propagationd，简称RMSProp）是由Hinton在2012年的文章<a href="">《RmsProp: divide the gradient by a running average of its recent magnitude》</a>中提出。他是针对AdaGrad算法在迭代后期学习率下降过快给出的改进版本。RMSProp通过引入一个衰减系数$\gamma$（又称遗忘因子，forgetting factor），每次计算的梯度平方都按照$\gamma$衰减一定比例来计算梯度累计量$n$。</p>
<p>$$<br>n_{k,w_j} = \gamma \cdot n_{k-1,w_j} + (1 - \gamma) \cdot \nabla_{k, w_j} \cdot \nabla_{k,w_j}<br>$$</p>
<p>参数的更新方式与AdaGrad算法相同。</p>
<p>RMSProp算法总结如下：</p>
<ol>
<li>相比AdaGrad，该算法很好的解决了模型训练过早结束的问题（尤其在深度学习模型中较常见）；</li>
<li>引入了新的超参数：衰减稀疏$\gamma$ (默认值可设置为0.95，但本人试验发现接近于1效果更好，比如0.99)；</li>
<li>算法依然依赖于全局学习率；</li>
</ol>
<h2 id="1.4.AdaDelta">AdaDelta</h2>


<p>除了RMSProp算法外，Adadelta算法也是针对Adagrad算法在迭代后期比较难找到有用解的问题作了改进。该算法由Matthew D. Zeiler在2012年的论文<a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="external">ADADELTA: An Adaptive Learning Rate Method</a>中提出。有意思的是<strong>Adadelta算法没有学习率这一超参数</strong>。</p>
<p>AdaDelta算法也像RMSProp算法一样，使用了batch随机梯度信息$\nabla_{k, w_j}$按元素平方的指数加权移动平均变量$n_{k,w_j}$：</p>
<p>$$<br>n_{k,w_j} = \rho \cdot n_{k-1,w_j} + (1 - \rho) \cdot \nabla_{k, w_j} \cdot \nabla_{k,w_j}<br>$$</p>
<p>这里$\rho$是衰减系数，通过这个衰减系数，我们令每一个时刻的$g_t$随之时间按照$\rho$指数衰减，这样就相当于我们仅使用离当前时刻比较近的梯度信息，从而使得还很长时间之后，参数仍然可以得到更新。</p>
<p>与RMSProp不同的是，AdaDelta算法还维护一个额外的<strong>状态变量$\Delta w_k$</strong>，其元素同样在时间步0时被初始化为0。我们使用$\Delta w_{k-1}$来计算<strong>参数的变化量</strong>：</p>
<p>$$<br>g_{k}^’ \leftarrow \sqrt{\frac{\Delta w + \epsilon}{n_{k,w_j} + \epsilon}} \cdot  g_k<br>$$</p>
<p>接着更新参数：</p>
<p>$$<br>w_j \leftarrow w_j - g’<br>$$</p>
<p>最后，使用$\Delta w$来记录参数变化量$g’$按元素平方的指数加权移动平均：</p>
<p>$$<br>\Delta w_k \leftarrow \rho \cdot \Delta w_{k-1} + (1-\rho) g’ \cdot g’<br>$$</p>
<p>可以看到，如果不考虑$\epsilon$的影响，AdaDelta与RMSProp的不同之处在于使用$\sqrt{\Delta w_{k-1}}$来替代超参数$\eta$。</p>
<h2 id="1.5.Adam">Adam</h2>

<p>自适应矩估计算法（Adaptive Moment Estimation，简称Adam）是另外一种计算自适应学习率的随机优化算法，由Diederik P. Kingma发表在2015年ICLR会议的论文<a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="external">《Adam: A Method for Stochastic Optimization》</a>中提出。除了像RMSProp和AdaDelta一样计算过去梯度平方的$v_k$指数衰减均值（二阶矩估计），也会计算过去梯度$m_k$的指数衰减均值（一阶矩估计）。算法更新过程如下：</p>
<p>$$<br>m_k = \beta_1 \dot m_{k-1} + (1 - \beta_1) \cdot \nabla_k  \quad(1) \\\<br>v_k = \beta_2 \dot m_{k-1} + (1 - \beta_2) \cdot \nabla_k \cdot \nabla_k \quad(2) \\\<br>\hat{m_t} = \frac{m_k}{1 - \beta_1}  \quad(3) \\\<br>\hat{v_t} = \frac{v_t}{1 - \beta_2}    \quad(4)  \\\<br>w_k = w_{k-1} - \frac{\eta}{\sqrt{v_t + \epsilon}} \cdot m_k  \quad(5)<br>$$</p>
<p>其中，$(3)$和$(4)$是为了做偏差校正：如果$m_k$和$v_k$被初始化为0向量，那它们就会向0偏置，这里通过计算偏差校正后的$m_k$和$v_k$来抵消这些偏差。$(5)$是参数更新规则。</p>
<p>$(3), (4), (5)$步等价于：</p>
<p>$$<br>\hat{\eta} = \eta \cdot \frac{\sqrt{1 - \beta_2}} {1 - \beta_1} \\\<br>\hat{\epsilon} = \epsilon \cdot \sqrt{1 - \beta_2} \\\<br>w_k = w_{k-1} - \frac{w_k} {\sqrt{v_t + \hat{\epsilon}}}<br>$$</p>
<p>超参数取值：论文中给出了建议的超参数取值（$\beta_1 = 0.9, \; \beta_2=0.999, \; \epsilon=10e-8$）。在实验中发现$\beta_1$为0.99效果更好（仅仅是在我们的数据场景中的表现，不一定具有普世性）。</p>
<h2 id="1.6.Follow The Regularized Leader">Follow The Regularized Leader-Proximal</h2>



<p>FTRL-Proximal算法是Google在2013年的一篇论文《Ad Click Prediction: a View from the Trenches》中提到的参数在线学习算法，论文中提到该算法用在了Google的搜索广告在线学习系统中。因为算法比较容易理解且工程实现不复杂，业内诸多公司都有尝试并取得了不错的收益。</p>
<p>FTRL-Proximal算法不仅可用于在线学习（不要求数据IID，样本序列学习），同时也可以用于离线基于batch数据的参数求解。所以这里也把该算法作为“优化器”的一种，用于大规模机器学习任务的参数求解（该算法本身就是为了很好的求解大规模在线学习任务而提出的）。</p>
<h3 id="FTRL-Proximal演化进程">FTRL-Proximal演化进程</h3><ul>
<li><p><strong>Online Gradient Descent (OGD, 在线梯度下降）</strong></p>
<p>  可以有非常好的预估准确性，并且占用较少的资源。但是它不能得到有效的稀疏模型（非零参数），即便优化目标添加L1惩罚项也不能产生严格意义上的稀疏解（会使参数接近于0，而不是0）。</p>
<blockquote>
<p>这里OGD其实就是随机梯度下降，之所以强调Online是因为这里不适用于解决batch数据问题的，而是用于样本序列化问题求解的，后者不要求样本是独立同分布（IID））的。</p>
</blockquote>
</li>
<li><p><strong>Truncated Gradient and FOBOS</strong><br>  参数\(w_j\)设定阈值，当参数小雨阈值时置为0，可得稀疏解。</p>
</li>
<li><p><strong>Regularized Dual Averaging（RDA, 正则化对偶平均）</strong><br>  RDA可以得到稀疏解，在预估准确性和模型稀疏性方面优于FOBOS算法。</p>
</li>
</ul>
<p>有没有既能结合RDA获得模型稀疏性，同时又具备OGD的精度的学习算法呢？答案是肯定的，那就是：”Follow The (Proximal) Regularized Leader”算法。</p>
<h3 id="FTRL-Proximal工作原理">FTRL-Proximal工作原理</h3><p>为了更容易的理解FTRL的工作原理，这里以学习LR模型参数为例，看FTRL是如何求解的：</p>
<ul>
<li><p>首先，对于一个实例\(\mathbf{x}^{(i)} \in R^n\)预测其标签$y=1$的概率$p$（第$i$次迭代的模型参数\(\mathbf{w}^{(i)}\)）。公式\(p^{(i)} = \sigma(\mathbf{w}^{(i)} \cdot \mathbf{x}^{(i)}), \; \sigma(a) = \frac{1}{1 + \exp(-a)}\).</p>
</li>
<li><p>然后，观测标签\(y^{(i)} \in \{0,1\}\)，LR模型的损失函数（Logistic Loss）为：</p>
<p>  $$<br>  \mathcal{l} \; (w^{(i)}) = -y^{(i)} \log p^{(i)} - (1 - y^{(i)}) \log (1- p^{(i)}) \qquad (1)<br>  $$</p>
<p>  梯度方向：</p>
<p>  $$<br>  \mathbf{g}_i = \nabla_i = \frac{\partial l \;(w)} {\partial w} = (\sigma(w \cdot x^{(i)}) - y^{(i)}) = (p^{(i)} - y^{(i)}) \cdot \mathbf{x}^{(i)} \qquad(2)<br>  $$</p>
<p>  OGD算法迭代公式：</p>
<p>  $$<br>  w_{i+1} = w_i - \eta_i \mathbf{g}_i \qquad(3)<br>  $$</p>
<p>  其中\(\eta_i\)表示非递增的学习率，如\(\eta_i=\frac{1}{\sqrt{i}}\)，\(\mathbf{\nabla}_i\)表示当前梯度值。</p>
</li>
<li><p>FTRL-Proximal</p>
<p>  $$<br>  \mathbf{w}_{i+1} = \text{arg} \min_w \left( \underline{ \sum_{s=1}^{i} \mathbf{g}_s} \cdot \mathbf{w} + \frac{1}{2} \sum_{s=1}^{i} \sigma_s {\Vert \mathbf{w} - \mathbf{w}_s \Vert}_2^2 + \lambda_1 {\Vert \mathbf{w} \Vert}_1 \right) \qquad(4)<br>  $$</p>
<p>  \(\sigma_s\)为学习率参数，有\(\sigma_{1:i} = \frac{1}{\eta_i}\)。上式等价于：</p>
<p>  $$<br>  \left( \sum_{s=1}^{i} \mathbf{g}_s  - \sum_{s=1}^{i} \sigma_s \mathbf{w}_s \right) \cdot \mathbf{w} + \frac{1}{\eta_i} {\Vert \mathbf{w} \Vert}_2^2 + \lambda_1 {\Vert \mathbf{w} \Vert}_1    + (\text{const})  \qquad(5)<br>  $$</p>
<p>  如果我们存储\(\mathbf{z}_{i-1} = \sum_{s=1}^{i-1} \mathbf{g}_s  - \sum_{s=1}^{i-1} \sigma_s \mathbf{w}_s\)，在第\(i\)轮迭代时，\(\mathbf{z}_i = \mathbf{z}_{i-1} + \mathbf{g}_i + \left(\frac{1}{\eta_i} - \frac{1}{\eta_{i-1}} \right) \mathbf{w}_i\)。求\(\mathbf{w}_{i+1}\)每一维参数的闭式解为：</p>
<p>  $$<br>  w_{i+1, j} =<br>  \left \{<br>  \begin{array}{ll}<br>  0, &amp; \text{if} \; |z_{i,j}| \le \lambda_1 \\\<br>  -\eta_i \left(z_{i,j} - \text{sgn}(z_{i,j}) \lambda_1 \right) &amp; \text{otherwise}.<br>  \end{array}<br>  \right.     \qquad(6)<br>  $$</p>
<p>  每一维特征的学习率计算公式（与对应特征维度梯度累加和 &amp;&amp; 梯度平方和相关）：</p>
<p>  $$<br>  \eta_{i,j} = \frac{\alpha}{\beta + \sqrt{\sum_{s=1}^{i} g_{s,j}^2}} = \frac{1}{\sqrt{n_{i+1,j}}}<br>  \qquad(7)<br>  $$</p>
<p>  \(n_i = \sum_{s=1}^{i-1} g_{s}^2\)表示梯度平方累加和，参数\(\mathbf{z} \in R^n\)在内存存储。</p>
</li>
</ul>
<h3 id="FTRL-Proximal伪代码">FTRL-Proximal伪代码</h3><p>\(\{ \\\<br>    \quad01. \;\text{Per-Coordinate FTRL-Proximal with L}_1 \text{ and L}_2 \text{ Regularization for Logistic Regression. }  \\\<br>    \quad02. \;\text{// 支持} L_1 \text{与} L_2  \text{正则项的FTRL-Proximal算法. Per-Coordinate学习率为公式(7). } \\\<br>    \quad03. \;\text{Input: parameters } \alpha, \beta, \lambda_1, \lambda_2. 初始化z_i = 0 和n_i=0.\quad //  参数 \alpha, \beta 用于\text{Per-Coordinate}计算学习率 \\\<br>    \quad04. \;\mathbf{\text{for}} \; t=1 \; to \; T; do \\\<br>    \quad05. \quad 接受特征向量\mathbf{x}_t, \; I = \{i| x_i \neq 0 \}. \quad //取非0特征index集合。 \\\<br>    \quad06. \quad\text{For i} \in I, 计算  \\\<br>    \quad07. \quad\qquad w_{t, i} =<br>    \; \left \{<br>    \; \begin{array}{ll}<br>    0, &amp; \text{if} \; |z_{i}| \le \lambda_1 \\\<br>    -\left( \frac{\beta + \sqrt{n_i}}{\alpha} + \lambda_2 \right)^{-1} \cdot \left(z_{i} - \text{sign}(z_{i}) \lambda_1 \right) &amp; \text{otherwise}<br>    \end{array}<br>    \right.  。\\\<br>    \quad08. \quad 预测 p_t = \sigma(\mathbf{x}_t \cdot \mathbf{w}) 使用w_{t,i}计算。\\\<br>    \quad09. \quad \text{For} \; i \in I; 更新参数 \\\<br>    \quad10. \qquad g_i = (p_t - y_t) \cdot x_i \qquad // 第i维特征的梯度（libsvm格式数据，x_i多为1.）\\\<br>    \quad11. \qquad \sigma_i = \frac{1}{\alpha} \left(\sqrt{n_i + g_i^2} - \sqrt{n_i} \right) \qquad // n_i表示第i维梯度（前t-1次）的平方累加和 \\\<br>    \quad12. \qquad z_i \leftarrow z_{i-1} + g_i - \sigma_i w_{t,i} \qquad // 更新\text{FTRL}目标函数的梯度公式  \\\<br>    \quad13. \qquad n_i \leftarrow n_i + g_i^2  \qquad // 更新第i维梯度平方累加和值。\\\<br>    \}<br>    \)</p>
<p>FTRL算法添加了per-coordinate学习率，目标函数支持L2正则。</p>
<ul>
<li><p>解释per-coordinate</p>
<p>  FTRL学习过程是对参数向量\(\mathbf{w}\)每一维分开训练更新的，每一维使用不同的学习率。与\(n\)个特征维度使用统一的学习率相比，此种方法考虑到了训练样本本身在不同特征维度上分布不均匀的特点。</p>
<blockquote>
<p>如果包含\(w\)某一个维度特征的训练样本很少，每一个样本都很珍贵，那么该特征维度对应的训练速率可以独自保持比较大的值，每来一个包含该特征的样本，就可以在该样本的梯度上前进一大步，而不需要与其他特征维度的前进步调强行保持一致。</p>
</blockquote>
</li>
</ul>
<h3 id="FTRL实验经验">FTRL实验经验</h3><table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">变化</th>
<th style="text-align:center">$\mathcal{\sigma}$变化</th>
<th style="text-align:center">$z$变化</th>
<th style="text-align:center">$n$变化</th>
<th style="text-align:center">lrate变化</th>
<th style="text-align:center">$w$变化</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\alpha$</td>
<td style="text-align:center">$\uparrow$</td>
<td style="text-align:center">$\downarrow$</td>
<td style="text-align:center">振幅变小</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">$\uparrow$</td>
<td style="text-align:center">振幅不确定</td>
</tr>
<tr>
<td style="text-align:center">$\beta$</td>
<td style="text-align:center">$\uparrow$</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">$\downarrow$</td>
<td style="text-align:center">振幅变大</td>
<td>实践$\beta$从0.1调至2，效果正向</td>
</tr>
<tr>
<td style="text-align:center">$\mathcal{l_1}$</td>
<td style="text-align:center">$\uparrow$</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">$-$</td>
<td style="text-align:center">稀疏性增加，绝对值变小</td>
<td>从0.5调到0.01效果正向</td>
</tr>
<tr>
<td style="text-align:center">$\mathcal{l_2}$</td>
</tr>
<tr>
<td style="text-align:center">$\nabla_g$</td>
<td style="text-align:center">$\downarrow$</td>
<td style="text-align:center">$\downarrow$</td>
<td style="text-align:center">振幅变小</td>
<td style="text-align:center">变小$\downarrow$</td>
<td style="text-align:center">变大</td>
</tr>
</tbody>
</table>
<ol>
<li><p>alpha持续调大，训练集拟合越好越好，测试集表现变差，原因分析: </p>
<blockquote>
<p>alpha变大，sigma变小，z的振幅变小</p>
</blockquote>
</li>
</ol>
<h2 id="1.7.Follow the Moving Leader">Follow the Moving Leader</h2>



<p>FTML算法是2017年香港科技大学发表在ICML会议上的一篇论文《Follow the Moving Leader in Deep Learning》提到的。论文中主要为解决深度学习中参数学习收敛效率问题而出来的新算法。</p>
<p>在深度学习中，参数以及数据分布都会随着迭代进行不断变化，这使得深度学习模型的训练一直是一个具有挑战性的问题。针对这一问题，本文提出了全新的 FTML 算法，具有更快收敛速度。与已有优化算法（如 FTRL）不同的是，本文的 FTML 算法迭代中，越新样本具有越大权重，这使算法更能适应数据分布变化，有更快收敛速度。论文的最后，通过在多个数据集上训练深度学习模型训练实验结果做对比，得出FTML 比其他已有算法收敛更快的结论。</p>
<p>$<br>\{ \\\<br>\quad//\;\text{Follow the Moving Leader (FTML)}   \\\<br>\quad01. \;\text{Input: parameters } \eta; \; \beta_1,\beta_2 \in [0, 1); \epsilon &gt; 0;  \qquad //\; \eta:初始化学习率，\epsilon:防止分母为0，\\\<br>\quad02. \;\mathbf{w}; d \leftarrow 0; v \leftarrow 0; z \leftarrow 0; \qquad\qquad // 初始化中间变量和参数 \\\<br>\quad03. \;\mathbf{\text{for}} \; t=1 \; to \; T; do \\\<br>\quad04. \quad 特征向量\mathbf{x}_t, \; I = \{i| x_i \neq 0 \}. \quad //取非0特征index集合。 \\\<br>\quad05. \quad p_t = predict(\mathbf{w_t}, \mathbf{x_t}) \qquad // \;预测\\\<br>\quad06. \quad \text{for} \; i \in I; \; do \qquad\qquad\; //\; 更新参数 \\\<br>\quad07. \qquad g_i = (p_t - y_t) \cdot x_i \qquad\qquad\qquad // 计算梯度 \\\<br>\quad08. \qquad v_t \leftarrow \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_i^2 \qquad // 二阶距估计 \\\<br>\quad09. \qquad d_t \leftarrow \frac{1 - \beta_1^t} {\eta_t} \cdot \left(\sqrt{\frac{v_t} {1 - \beta_2^t}}  + \epsilon \cdot \mathbf{1} \right) \qquad // \\\<br>\quad10. \qquad \sigma_t \leftarrow d_t - \beta_1 \cdot d_{t-1}; \qquad // 中间变量 \\\<br>\quad11. \qquad z_t \leftarrow \beta_1 z_{t-1} + (1 - \beta_1) g_t - \sigma_t w_{t-1}; \qquad // \\\<br>\quad12. \qquad w \leftarrow \prod_{W}^{\text{diag} \left(\frac{d_t}{(1 - \beta_1^t)}\right)} \left(-\frac{z_t}{d_t}\right) \quad // \\\<br>\quad13. \quad \text{end for} \\\<br>\quad14.\; \mathbf{end \;for} \\\<br>\quad15.\; \mathbf{Output:\; w}. \\\<br>\}<br>$</p>
<h2 id="参考资料">参考资料</h2><ol>
<li>《An overview of gradient descent optimization algorithms》</li>
<li>《Follow the Moving Leader in Deep Learning》</li>
<li>验证梯度计算是否正确：<a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a></li>
<li>技术博客：<a href="http://www.cnblogs.com/ranjiewen/p/5938944.html" target="_blank" rel="external">http://www.cnblogs.com/ranjiewen/p/5938944.html</a></li>
<li>优化算法总结文档：<a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">http://sebastianruder.com/optimizing-gradient-descent/</a></li>
<li>优化算法：<a href="http://www.cnblogs.com/neopenx/p/4768388.html" target="_blank" rel="external">http://www.cnblogs.com/neopenx/p/4768388.html</a></li>
</ol>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/OpenMIT/">OpenMIT</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/ALS/">ALS</a><a href="/tags/AdaDelta/">AdaDelta</a><a href="/tags/AdaGrad/">AdaGrad</a><a href="/tags/Adam/">Adam</a><a href="/tags/FTML/">FTML</a><a href="/tags/FTRL/">FTRL</a><a href="/tags/GD/">GD</a><a href="/tags/LBFGS/">LBFGS</a><a href="/tags/MCMC/">MCMC</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter8-reinforcement-learning-family/" title="第08章：深入浅出ML之Reinforcement Learning家族" itemprop="url">第08章：深入浅出ML之Reinforcement Learning家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2016-02-28T13:31:36.000Z" itemprop="datePublished"> 发表于 2016-02-28</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2016-02-18</li>
<li>sina weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>马尔可夫决策过程</li>
<li>\(\epsilon\)-贪心法</li>
<li>…</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<p>强化学习这个领域是我在接触机器学习一年多才开始真正关注的，虽然读书的时候修过《高级人工智能》这门课，里面提到了Q-Learning，但当时没怎么重视，也没学好。</p>
<p>直到快毕业那年，接触了一个互联网广告中的一个问题：广告效果的归因分析。在查阅资料和寻求解决方案，发现其归因问题可以用强化学习的马尔可夫决策过程来建模。从此才关注和学习这部分。</p>
<p>本章原定题目是：markov，之所以改为强化学习，是因为在广告和推荐策略，尤其是解决广告或商品的冷启动问题时，可以用强化学习的思想来建模。</p>
<p><br></p>
<h3 id="马尔可夫决策过程">马尔可夫决策过程</h3><hr>
<p><br></p>
<h3 id="\(\epsilon\)-贪心法">\(\epsilon\)-贪心法</h3><hr>
<p>\(\epsilon\)-贪心法是通过概率对探索和利用进行折中:</p>
<table>
<thead>
<tr>
<th>每次尝试时，以\(\epsilon\)的概率进行探索，即以均匀挂了随机选取一个摇臂；以\(1-\epsilon\)的概率进行利用，即选择平均奖赏最多的摇臂（只选一个）。</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>用\(Q(k)\)纪录摇臂\(k\)的平均奖赏。若摇臂\(k\)被尝试了\(n\)次，得到的奖赏分别为\(v_1, v_2, \cdots, v_n\)，那么平均奖赏为：</p>
<p>$$<br>Q(k) = \frac{1}{n} \sum_{i=1}^{n} v_i<br>$$</p>
<p>若直接根据上式计算平均奖赏，则需要记录\(n\)个奖赏值，\(n\)个值都需要保存，空间上不够高效。下面给出一个更高效的方法：对均值进行增量式的计算，即每尝试一次就立即更新\(Q(k)\)。</p>
<p>用下标表示尝试的次数，初始时\(Q_0(k)＝0\)。对于任意的\(n \ge 1\)，若第\(n-1\)次尝试后的平均奖赏为\(Q_{n-1}(k)\)，则在经过第\(n\)次尝试获得\(v_n\)后，平均奖赏可更新为：</p>
<p>$$<br>\begin{align}<br>Q_n(k) &amp;= \frac{1}{n-1} \left[ (n-1) \cdot Q_{n-1}(k) + v_n \right] \\<br>&amp;= Q_{n-1}(k) + \frac{1}{n} (v_n - Q_{n-1}(k))<br>\end{align}<br>$$</p>
<p>这样，无论摇臂被尝试多少次，只需要记录两个值：已尝试次数\(n-1\)和最近平均奖赏\(Q_{n-1}(k)\)。</p>
<p>…</p>
<hr>
<ul>
<li>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/epsilon-贪心法/">epsilon-贪心法</a><a href="/tags/任务与奖赏/">任务与奖赏</a><a href="/tags/摇臂赌博机/">摇臂赌博机</a><a href="/tags/马尔可夫决策过程/">马尔可夫决策过程</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/hello-world/" title="测试Hexo功能" itemprop="url">测试Hexo功能</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2016-02-13T03:21:55.000Z" itemprop="datePublished"> 发表于 2016-02-13</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="公式">公式</h2><p>$$J_\alpha(x)=\sum_{m=0}^\infty \frac{(-1)^ m}{m! \, \Gamma (m + \alpha + 1)}{\left({\frac{x}{2}}\right)}^{2 m + \alpha }$$</p>
<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h4 id="1-2-1_线性回归模型">1.2.1 线性回归模型</h4><ul>
<li><p>模型表达</p>
<p>  $$<br>  y(x, w) = w_0 + w_1 x_1 + \cdots + w_n x_n \quad (n \ge 1) \qquad (ml.1.1.1)<br>  $$ </p>
<p>  其中，\(x_1,x_2,\cdots,x_n\)表示自变量（集合），\(y\)是因变量，\(w\)为参数向量，\(w_i\)表示对应自变量（特征）的权重，\(w_0\)是偏倚项（又称为截距）。</p>
<blockquote>
<p> 关于参数\(w\)： <br></p>
<ol>
<li>在物理上可以这样解释：<strong>在自变量（特征）之间相互独立的前提下</strong>，\(w_i\)反映自变量\(x_i\)对因变量影响程度，\(w_i\)越大，说明\(x_i\)对结果\(y\)的影响越大。<br></li>
<li>通过每个字变量（特征）前面的参数，可以很直观的看出哪些特征分量对结果的影响比较大。<br></li>
<li>在统计中，\(w_1,w_2,\cdots,w_n\)称为偏回归系数，\(w_0\)称为截距。</li>
</ol>
</blockquote>
<p>  如果令\(x_0=1, y(x,w)=h_{w}(x)\), 可以将公式\((ml.1.1.1)\)写成向量形式，即：</p>
<p>  $$<br>  h_{w}(x) = \sum_{i=0}^{n} w_i x_i = w^T x \qquad(ml.1.1.2)<br>  $$</p>
<p>  其中，\(w=(w_0, w_1, \cdots, w_n)\)，\(x=(1, x_1, x_2, \cdots, x_n)\) 均为向量，\(w^T\)为\(w\)的转置。</p>
</li>
</ul>
<p>   上式是对一条样本进行建模的数据表达。对于多条样本，假设每条样本生成过程独立，在整个样本空间中（\(m\)个样本）的概率分布为：</p>
<p>   $$P(Y|X; w) = \prod_{i=1}^{m} \left( (h_{w}(x^{(i)}))^{y^{(i)}} \cdot (1 - h_{w}(x^{(i)}))^{1-y^{(i)}} \right) \qquad(ml.1.8)$$</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter9-factorization-family/" title="第09章：深入浅出ML之Factorization家族" itemprop="url">第09章：深入浅出ML之Factorization家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2016-01-12T14:43:47.000Z" itemprop="datePublished"> 发表于 2016-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-12-18</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>Factorization Machine</li>
<li>Field-aware Factorization Machine</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<ul>
<li>FM</li>
<li>FFM</li>
</ul>
<p><br></p>
<h3 id="因子分解机">因子分解机</h3><hr>
<p>因子分解机（Factorization Machine，简称FM），又称分解机器。是由<a href="http://www.uni-konstanz.de/" target="_blank" rel="external">Konstanz大学（德国康斯坦茨大学）</a>Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？</p>
<p>用户在网站上的行为数据会被Server端以日志的形式记录下来，这些数据通常会存放在多台存储机器的硬盘上。</p>
<blockquote>
<p>以<a href="http://www.sina.com.cn/" target="_blank" rel="external">我浪</a>为例，各产品线纪录的用户行为日志会通过flume等日志收集工具交给数据中心托管，它们负责把数据定时上传至HDFS上，或者由数据中心生成Hive表。</p>
</blockquote>
<p>我们会发现日志中大多数出现的特征是categorical类型的，这种特征类型的取值仅仅是一个标识，本身并没有实际意义，更不能用其取值比较大小。比如日志中记录了用户访问的频道（channel）信息，如”news”, “auto”, “finance”等。</p>
<blockquote>
<p>假设channel特征有10个取值，分别为\(\{\text{“auto”}, \text{“finance”}, \text{“ent”}, \text{“news”}, \text{“sports”}, \text{“mil”}, \text{“weather”}, \text{“house”}, \text{“edu”}, \text{“games”} \}\)。部分训练数据如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">user</th>
<th style="text-align:center">channel</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">user1</td>
<td style="text-align:center">sports</td>
</tr>
<tr>
<td style="text-align:center">user2</td>
<td style="text-align:center">news</td>
</tr>
<tr>
<td style="text-align:center">user3</td>
<td style="text-align:center">finance</td>
</tr>
<tr>
<td style="text-align:center">user4</td>
<td style="text-align:center">house</td>
</tr>
<tr>
<td style="text-align:center">user5</td>
<td style="text-align:center">edu</td>
</tr>
<tr>
<td style="text-align:center">user6</td>
<td style="text-align:center">news</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</blockquote>
<p><strong>特征ETL</strong>过程中，需要对categorical型特征进行one-hot编码（独热编码），即将categorical型特征转化为数值型特征。channel特征转化后的结果如下：</p>
<blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">user</th>
<th style="text-align:center">chn-auto</th>
<th style="text-align:center">chn-finance</th>
<th style="text-align:center">chn-ent</th>
<th style="text-align:center">chn-news</th>
<th style="text-align:center">chn-sports</th>
<th style="text-align:center">chn-mil</th>
<th style="text-align:center">chn-weather</th>
<th style="text-align:center">chn-house</th>
<th style="text-align:center">chn-edu</th>
<th style="text-align:center">chn-games</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">user1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">user6</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</blockquote>
<p>可以发现，<strong>由one-hot编码带来的数据稀疏性会导致特征空间变大</strong>。上面的例子中，一维categorical特征在经过one-hot编码后变成了10维数值型特征。真实应用场景中，未编码前特征总维度可能仅有数十维或者到数百维的categorical型特征，经过one-hot编码后，达到数千万、数亿甚至更高维度的数值特征在业内都是常有的。</p>
<blockquote>
<p>我组广告和推荐业务的点击预估系统，编码前是特征不到100维，编码后（包括feature hashing）的维度达百万维量级。</p>
</blockquote>
<p>此外也能发现，<strong>特征空间增长的维度取决于categorical型特征的取值个数</strong>。在数据稀疏性的现实情况下，我们如何去利用这些特征来提升learning performance？</p>
<p>或许在学习过程中考虑特征之间的关联信息。针对特征关联，我们需要讨论两个问题：1. 为什么要考虑特征之间的关联信息？2. 如何表达特征之间的关联？</p>
<ol>
<li><p>为什么要考虑特征之间的关联信息？</p>
<p> 大量的研究和实际数据分析结果表明：某些特征之间的关联信息（相关度）对事件结果的的发生会产生很大的影响。从实际业务线的广告点击数据分析来看，也正式了这样的结论。</p>
</li>
<li><p>如何表达特征之间的关联？</p>
<p> 表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征：</p>
<ol>
<li>人工特征工程（数据分析＋人工构造）；</li>
<li>通过模型做组合特征的学习（深度学习方法、FM/FFM方法）</li>
</ol>
</li>
</ol>
<p>本章主要讨论FM和FFM用来学习特征之间的关联。我们在<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>看到的多项式回归模型，其中的交叉因子项\(x_i x_j\)就是组合特征最直观的例子。</p>
<blockquote>
<p>\(x_i x_j\)表示特征\(x_i\)和\(x_j\)的组合，当\(x_i\)和\(x_j\)都非零时，组合特征\(x_i x_j\)才有意义。</p>
</blockquote>
<p>这里我们以二阶多项式模型（degree=2时）为例，来分析和探讨FM原理和参数学习过程。</p>
<p><br></p>
<h4 id="FM模型表达">FM模型表达</h4><hr>
<p>为了更好的介绍FM模型，我们先从多项式回归、交叉组合特征说起，然后自然地过度到FM模型。</p>
<p><strong>二阶多项式回归模型</strong></p>
<p>我们先看二阶多项式模型的表达式：</p>
<blockquote>
<p>$$<br>\hat{y}(x) := \underbrace {w_0 + \sum_{i=1}^{n} w_i x_i }_{\text{线性回归}} + \underbrace {\sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j}_{\text{交叉项（组合特征）}} \qquad \text{(n.ml.1.9.1)}<br>$$</p>
</blockquote>
<p>其中，\(n\)表示样本特征维度，截距\(w_0 \in R,\; w ＝ \{w_1, w_2, \cdots, w_n\}\in R^n, w_{ij} \in R^{n \times n}\)为模型参数。</p>
<p>从公式\(\text{(n.ml.1.9.1)}\)可知，交叉项中的组合特征参数总共有\(\frac{n(n-1)}{2}\)个。在这里，<strong>任意两个交叉项参数\(w_{ij}\)都是独立的</strong>。然而，在数据非常稀疏的实际应用场景中，交叉项参数的学习是很困难的。why？</p>
<p>因为我们知道，<strong>回归模型的参数\(w\)的学习结果就是从训练样本中计算充分统计量（凡是符合<a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank" rel="external">指数族分布</a>的模型都具有此性质）</strong>，而在这里交叉项的每一个参数\(w_{ij}\)的学习过程需要大量的\(x_i、x_j\)同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“\(x_i\)和\(x_j\)都非零”的样本数就会更少。训练样本不充分，学到的参数\(w_{ij}\)就不是充分统计量结果，导致参数\(w_{ij}\)不准确，而这会严重影响模型预测的效果（performance）和稳定性。How to do it ?</p>
<p>那么，如何在降低数据稀疏问题给模型性能带来的重大影响的同时，有效地解决二阶交叉项参数的学习问题呢？矩阵分解方法已经给出了解决思路。这里借用CMU讨论课中提到的<a href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf" target="_blank" rel="external">FM课件</a>和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">美团－深入FFM原理与实践</a>中提到的矩阵分解例子（美团技术团队的分享很赞👍）。</p>
<blockquote>
<p>在基于Model-Based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。如下图所示。</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_1_fm_mf_example.png" width="700" height="400" alt="cf_mf"></p>
<p>上图把每一个user表示成了一个二维向量，同时也把item表示成一个二维向量，两个向量的内积就是矩阵中user对item的打分。</p>
</blockquote>
<p>根据矩阵分解的启发，如果把多项式模型中二阶交叉项参数\(w_{ij}\)组成一个对称矩阵\(W\)（对角元素设为正实数），那么这个矩阵就可以分解为\(W = V V^T\)，\(V \in R^{n \times k}\)称为系数矩阵，其中第\(i\)行对应着第\(i\)维特征的隐向量 (这部分在FM公式解读中详细介绍)。</p>
<p><strong>将每个交叉项参数\(w_{ij}\)用隐向量的内积\(\langle \mathbf{v}_i, \mathbf{v}_j\rangle\)表示，是FM模型的核心思想</strong>。下面对FM模型表达式和参数求解过程，给出详细解读。</p>
<p><strong>FM模型表达</strong></p>
<p>这里我们只讨论二阶FM模型（degree＝2），其表达式为：</p>
<p>$$<br>\hat{y}(\mathbf{x}) := w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j \qquad \text{(ml.1.9.1)}<br>$$  </p>
<p>其中，\(\mathbf{v}_i\)表示第\(i\)特征的隐向量，\(\langle \cdot, \cdot\rangle\)表示两个长度为\(k\)的向量的内积，计算公式为：</p>
<p>$$<br>\langle \mathbf{v}_i, \mathbf{v}_j \rangle := \sum_{f=1}^{k} v_{i,f} \cdot v_{j,f} \qquad \text{(ml.1.9.2)}<br>$$</p>
<blockquote>
<p>公式解读：</p>
<ul>
<li><p><strong>线性模型＋交叉项</strong></p>
<p> 直观地看FM模型表达式，前两项是<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/#线性回归模型" target="_blank" rel="external">线性回归模型</a>的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将<strong>两个互异的特征分量</strong>之间的关联信息考虑进来。<strong>用交叉项表示组合特征，从而建立特征与结果之间的非线性关系</strong>。</p>
</li>
<li><p><strong>交叉项系数 \(\to\) 隐向量内积</strong></p>
<p> 由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数\(w_{ij}\)（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积\(\langle \mathbf{v}_i, \mathbf{v}_j\rangle\)表示\(w_{ij}\)。</p>
<p>具体的，FM求解过程中的做法是：对每一个特征分量\(x_i\)引入**隐向量\(\mathbf{v}_i ＝ (v_{i,1}, v_{i,2}, \cdots, v_{i,k})\)**，利用\(v_i v_j^T\)内积结果对交叉项的系数\(w_{ij}\)进行估计，公式表示：\(\hat{w}_{ij} := v_i v_j^T\).</p>
</li>
</ul>
</blockquote>
<p>隐向量的长度\(k\)称为超参数\((k \in N^+, k \ll n)\)，\(\mathbf{v}_i = (v_{i,1}, v_{i,2}, \cdots, v_{i,k})\)的含义是用\(k\)个描述特征的因子来表示第\(i\)维特征。根据公式\(\text{(ml.1.9.1)}\)，二阶交叉项的参数由\(n \cdot n\)个减少到\(n \cdot k\)个，远少于二阶多项式模型中的参数数量。</p>
<p>此外，<strong>参数因子化表示后，使得\(x_h x_i\)的参数与\(x_i x_j\)的参数不再相互独立</strong>。这样我们就可以在样本稀疏情况下相对合理的估计FM模型交叉项的参数。具体地：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\langle \mathbf{v}_h, \mathbf{v}_i \rangle &amp;:= \sum_{f=1}^{k} v_{h,f} \cdot v_{i,f}  \quad(1)\\<br>\langle \mathbf{v}_i, \mathbf{v}_j \rangle &amp;:= \sum_{f=1}^{k} v_{i,f} \cdot v_{j,f} \;\quad(2)<br>\end{align} \qquad (n.ml.1.9.2)<br>$$</p>
</blockquote>
<p>\(x_h x_i\)与\(x_i x_j\)的系数分别为\(\langle \mathbf{v}_h, \mathbf{v}_i \rangle\)和\(\langle \mathbf{v}_i, \mathbf{v}_j \rangle\)，他们之间有共同项\(\mathbf{v}_i\)。也就是说，所有包含\(x_i\)的非零组合特征（存在某个\(j \neq i\)，使得\(x_ix_j \neq 0\)）的样本都可以用来学习隐向量\(\mathbf{v}_i\)，这在很大程度上避免了数据稀疏行造成参数估计不准确的影响。</p>
<blockquote>
<p>在二阶多项式模型中，参数\(w_{hi}\)和\(w_{ij}\)的学习过程是相互独立的。</p>
</blockquote>
<p>论文中还提到FM模型的应用场景，并且说公式\(\text{(ml.1.9.1)}\)作为一个通用的拟合模型（Generic Model），可以采用不同的损失函数来解决具体问题。比如：</p>
<table>
<thead>
<tr>
<th style="text-align:center">FM应用场景</th>
<th style="text-align:center">损失函数</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">回归</td>
<td style="text-align:center">均方误差（MSE）损失</td>
<td style="text-align:center">Mean Square Error，与平方误差类似</td>
</tr>
<tr>
<td style="text-align:center">二类分类</td>
<td style="text-align:center">Hinge/Cross-Entopy损失</td>
<td style="text-align:center">分类时，结果需要做sigmoid变换</td>
</tr>
<tr>
<td style="text-align:center">排序</td>
<td style="text-align:center">.</td>
<td style="text-align:center">. </td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="FM参数学习">FM参数学习</h4><hr>
<p><strong>等式变换</strong></p>
<p>公式\(\text{(ml.1.9.1)}\)中直观地看，FM模型的复杂度为\(O(kn^2)\)，但是通过下面的等价转换，可以将FM的二次项化简，其复杂度可优化到\(O(kn)\)。即：</p>
<p>$$<br>\sum_{i=1}^{n} \sum_{j=i+1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j = \frac{1}{2} \sum_{f=1}^{k} {\left \lgroup \left(\sum_{i=1}^{n} v_{i,f} x_i \right)^2 - \sum_{i=1}^{n} v_{i,f}^2 x_i^2\right \rgroup} \qquad(ml.1.9.3)<br>$$</p>
<p>下面给出详细推导：</p>
<blockquote>
<p>$$<br>\begin{align}<br>&amp; \sum_{i=1}^{n} \sum_{j=i+1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j \qquad\qquad\qquad\qquad\qquad\qquad(1)\\<br>= &amp; \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j - \frac{1}{2} \sum_{i=1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_i \rangle} x_i x_i \qquad\qquad\;\;(2)\\<br>= &amp; \frac{1}{2} \left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} v_{i,f} v_{j,f} x_i x_j - \sum_{i=1}^{n} \sum_{f=1}^{k} v_{i,f} v_{i,f} x_i x_i \right) \qquad\,(3) \\<br>= &amp; \frac{1}{2} \sum_{f=1}^{k} {\left \lgroup \left(\sum_{i=1}^{n} v_{i,f} x_i \right) \cdot \left(\sum_{j=1}^{n} v_{j,f} x_j \right) - \sum_{i=1}^{n} v_{i,f}^2 x_i^2 \right \rgroup} \quad\;\;\,(4) \\<br>= &amp; \frac{1}{2} \sum_{f=1}^{k}  {\left \lgroup \left(\sum_{i=1}^{n} v_{i,f} x_i \right)^2 - \sum_{i=1}^{n} v_{i,f}^2 x_i^2\right \rgroup} \qquad\qquad\qquad\;\;(5)<br>\end{align} \qquad(n.ml.1.9.3)<br>$$</p>
<p>解读第（1）步到第（2）步，这里用\(A\)表示系数矩阵\(V\)的上三角元素，\(B\)表示对角线上的交叉项系数。由于系数矩阵\(V\)是一个对称阵，所以下三角与上三角相等，有下式成立：</p>
<p>$$<br>A = \frac{1}{2} (2A+B) - \frac{1}{2} B.  \quad \underline{ A=\sum_{i=1}^{n} \sum_{j=i+1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_j \rangle} x_i x_j } ; \quad \underline{ B = \frac{1}{2} \sum_{i=1}^{n} {\langle \mathbf{v}_i, \mathbf{v}_i \rangle} x_i x_i } \quad (n.ml.1.9.4)<br>$$</p>
</blockquote>
<p>如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：</p>
<p>$$<br>\frac{\partial}{\partial \theta} y(\mathbf{x}) =<br>\left \{<br>\begin{array}{ll}<br>1,         &amp; \text{if}\; \theta\; \text{is}\; w_0 \qquad \text{(常数项)} \\<br>x_i     &amp; \text{if}\; \theta\; \text{is}\; w_i \;\qquad \text{(线性项)} \\<br>x_i \sum_{j=1}^{n} v_{j,f} x_j - v_{i,f} x_i^2, &amp; \text{if}\; \theta\; \text{is}\; v_{i,f} \qquad \text{(交叉项)}<br>\end{array}<br>\right. \qquad\quad(ml.1.9.4)<br>$$</p>
<p>其中， \(v_{j,f}\)是隐向量\(\mathbf{v}_j\)的第\(f\)个元素。</p>
<p><strong>梯度法训练FM</strong></p>
<p>给出伪代码</p>
<p><strong>FM训练复杂度</strong></p>
<p>由于\(\sum_{j=1}^{n} v_{j,f} x_j\)只与\(f\)有关，在参数迭代过程中，只需要计算第一次所有\(f\)的\(\sum_{j=1}^{n} v_{j,f} x_j\)，就能够方便地得到所有\(v_{i,f}\)的梯度。显然，计算所有\(f\)的\(\sum_{j=1}^{n} v_{j,f} x_j\)的复杂度是\(O(kn)\)；已知\(\sum_{j=1}^{n} v_{j,f} x_j\)时，计算每个参数梯度的复杂度是\(O(n)\)；得到梯度后，更新每个参数的复杂度是 \(O(1)\)；模型参数一共有\(nk + n + 1\)个。因此，FM参数训练的时间复杂度为\(O(kn)\)。</p>
<p>综上可知，FM算法可以在线性时间内完成模型训练，以及对新样本做出预测，所以说FM是一个非常高效的模型。</p>
<p><br></p>
<h4 id="FM总结">FM总结</h4><hr>
<p>上面我们主要是从FM模型引入（多项式开始）、模型表达和参数学习的角度介绍的FM模型，这里我把我认为FM最核心的精髓和价值总结出来，与大家讨论。FM模型的核心作用可以概括为以下3个：</p>
<p><strong>1. <font color="#EE0000">FM降低了交叉项参数学习不充分的影响</font></strong></p>
<p>one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用\(k\)维的隐向量表示，交叉项的参数\(w_{ij}\)用对应特征隐向量的内积表示，即\(\langle \mathbf{v}_i, \mathbf{v}_j \rangle\)（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数\(w_{ij}\)的过程，转变为学习\(n\)个单特征对应\(k\)维隐向量的过程。</p>
<p>很明显，单特征参数（\(k\)维隐向量\(\mathbf{v}_i\)）的学习要比交叉项参数\(w_{ij}\)学习得更充分。示例说明：</p>
<blockquote>
<p>假如有10w条训练样本，其中出现<code>女性</code>特征的样本数为3w，出现<code>男性</code>特征的样本数为7w，出现<code>汽车</code>特征的样本数为2000，出现<code>化妆品</code>的样本数为1000。特征共现的样本数如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">共现交叉特征</th>
<th style="text-align:center">样本数</th>
<th>注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>&lt;女性，汽车&gt;</code></td>
<td style="text-align:center">500</td>
<td>同时出现<code>&lt;女性，汽车&gt;</code>的样本数</td>
</tr>
<tr>
<td style="text-align:center"><code>&lt;女性，化妆品&gt;</code></td>
<td style="text-align:center">1000</td>
<td>同时出现<code>&lt;女性，化妆品&gt;</code>的样本数</td>
</tr>
<tr>
<td style="text-align:center"><code>&lt;男性，汽车&gt;</code></td>
<td style="text-align:center">1500</td>
<td>同时出现<code>&lt;男性，汽车&gt;</code>的样本数</td>
</tr>
<tr>
<td style="text-align:center"><code>&lt;男性，化妆品&gt;</code></td>
<td style="text-align:center">0</td>
<td>样本中无此特征组合项</td>
</tr>
</tbody>
</table>
</blockquote>
<p><code>&lt;女性，汽车&gt;</code>的含义是<font color="#EE0000">女性看汽车广告</font>。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。 </p>
<p>因此，可以说<strong>FM降低了因数据稀疏，导致交叉项参数学习不充分的影响</strong>。</p>
<p><strong>2. <font color="#EE0000">FM提升了模型预估能力</font></strong>    </p>
<p>依然看上面的示例，样本中没有<code>&lt;男性，化妆品&gt;</code>交叉特征，即没有<font color="#EE0000">男性看化妆品广告</font>的数据。如果用多项式模型来建模，对应的交叉项参数<font color="#EE0000">\(w_{\text{男性,化妆品}}\)</font>是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的<font color="#EE0000">男性看化妆品广告</font>场景给出准确地预估。</p>
<p>FM模型是否能得到交叉项参数\(w_{\text{男性,化妆品}}\)呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为\(w_{\text{男性,化妆品}} = \langle \mathbf{v}_{男性}, \mathbf{v}_{化妆品}\rangle\)。</p>
<blockquote>
<p>用<code>男性</code>特征隐向量\(\mathbf{v}_{男性}\)和<code>化妆品</code>特征隐向量\(\mathbf{v}_{化妆品}\)的内积表示交叉项参数\(w_{\text{男性,化妆品}}\)。</p>
</blockquote>
<p>由于FM学习的参数就是单特征的隐向量，那么<font color="#EE0000">男性看化妆品广告</font>的预估结果可以用\(\langle \mathbf{v}_{男性}, \mathbf{v}_{化妆品}\rangle\)得到。这样，即便训练集中没有出现<font color="#EE0000">男性看化妆品广告</font>的样本，FM模型仍然可以用来预估，提升了预估能力。</p>
<p><strong>3. <font color="#EE0000">FM提升了参数学习效率</font></strong></p>
<p>这个显而易见，参数个数由\((n^2 + n + 1)\)变为\((nk + n + 1)\)个，模型训练复杂度也由\(O(m n^2)\)变为\(O(m n k)\)。\(m\)为训练样本数。对于训练样本和特征数而言，都是线性复杂度。</p>
<p>此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人<strong>把FM模型称为多项式的广义线性模型</strong>，也是恰如其分的。</p>
<p><strong>从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式</strong>，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑<font color="#EE0000"><code>User-Ad-Context</code></font>三个维度特征之间的关系，在FM模型中对应的degree为3。</p>
<p>最后一句话总结，FM最大特点和优势：</p>
<table>
<thead>
<tr>
<th><strong><font color="#EE0000">FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力。</font></strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><br></p>
<h3 id="场感知分解机">场感知分解机</h3><hr>
<p>场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自于Yu-Chin Juan与其比赛队员，它们借鉴了辣子Michael Jahrer的论文中field概念，提出了FM的升级版模型。</p>
<p>通过引入field的概念，FFM吧相同性质的特征归于同一个field。在FM开头one-hot编码中提到用于访问的channel，编码生成了10个数值型特征，这10个特征都是用于说明用户PV时对应的channel类别，因此可以将其放在同一个field中。那么，我们可以把同一个categorical特征经过one-hot编码生成的数值型特征都可以放在同一个field中。</p>
<blockquote>
<p>同一个categorical特征可以包括用户属性信息（年龄、性别、职业、收入、地域等），用户行为信息（兴趣、偏好、时间等），上下文信息（位置、内容等）以及其它信息（天气、交通等）。</p>
</blockquote>
<p>在FFM中，每一维特征\(x_i\)，针对其它特征的每一种”field” \(f_j\)，都会学习一个隐向量\(\mathbf{v}_{i,f_j}\)。因此，隐向量不仅与特征相关，也与field相关。</p>
<p>假设每条样本的\(n\)个特征属于\(f\)个field，那么FFM的二次项有\(nf\)个隐向量。而在FM模型中，每一维特征的隐向量只有一个。因此可以吧FM看作是FFM的特例，即把所有的特征都归属到一个field是的FFM模型。根据FFM的field敏感特性，可以导出其模型表达式：</p>
<p>$$<br>\hat{y}(\mathbf{x}) := w_0 + \sum_{i=1}^{n} w_i x_i + \sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_{i,\,f_j}, \mathbf{v}_{j,\,f_i} \rangle x_i x_j \qquad(ml.1.9.5)<br>$$</p>
<p>其中，\(f_j\)是第\(j\)个特征所属的field。如果隐向量的长度为\(k\)，那么FFM的二交叉项参数就有\(nfk\)个，远多于FM模型的\(nk\)个。此外，由于隐向量与field相关，FFM的交叉项并不能够像FM那样做化简，其预测复杂度为\(O(kn^2)\)。</p>
<p>这里以<a href="http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf" target="_blank" rel="external">NTU_FFM.pdf</a>和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">美团－深入FFM原理与实践</a>都提到的例子，给出FFM－Fields特征组合的工作过程。</p>
<blockquote>
<p>给出一下输入数据：</p>
<table>
<thead>
<tr>
<th style="text-align:center">User</th>
<th style="text-align:center">Movie</th>
<th style="text-align:center">Genre</th>
<th style="text-align:center">Price</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">YuChin</td>
<td style="text-align:center">3Idiots</td>
<td style="text-align:center">Comedy, Drama</td>
<td style="text-align:center">$9.99</td>
</tr>
</tbody>
</table>
<p>Price是数值型特征，实际应用中通常会把价格划分为若干个区间（即连续特征离散化），然后再one-hot编码，这里假设$9.99对应的离散化区间tag为”2”。当然不是所有的连续型特征都要做离散化，比如某广告位、某类广告／商品、抑或某类人群统计的历史CTR（pseudo－CTR）通常无需做离散化。</p>
<p>该条记录可以编码为5个数值特征，即<code>User^YuChin</code>, <code>Movie^3Idiots</code>, <code>Genre^Comedy</code>, <code>Genre^Drama</code>, <code>Price^2</code>。其中<code>Genre^Comedy</code>, <code>Genre^Drama</code>属于同一个field。为了说明FFM的样本格式，我们把所有的特征和对应的field映射成整数编号。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Field Name</th>
<th style="text-align:center">Field Index</th>
<th style="text-align:center">Feature Name</th>
<th style="text-align:center">Feature Index</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">User</td>
<td style="text-align:center"><font color="#EE0000">1</font></td>
<td style="text-align:center"><code>User^YuChin</code></td>
<td style="text-align:center"><font color="#0000FF">1</font></td>
</tr>
<tr>
<td style="text-align:center">Movie</td>
<td style="text-align:center"><font color="#EE0000">2</font></td>
<td style="text-align:center"><code>Movie^3Idiots</code></td>
<td style="text-align:center"><font color="#0000FF">2</font></td>
</tr>
<tr>
<td style="text-align:center">Genre</td>
<td style="text-align:center"><font color="#EE0000">3</font></td>
<td style="text-align:center"><code>Genre^Comedy</code></td>
<td style="text-align:center"><font color="#0000FF">3</font></td>
</tr>
<tr>
<td style="text-align:center">－</td>
<td style="text-align:center">－</td>
<td style="text-align:center"><code>Genre^Drama</code></td>
<td style="text-align:center"><font color="#0000FF">4</font></td>
</tr>
<tr>
<td style="text-align:center">Price</td>
<td style="text-align:center"><font color="#EE0000">4</font></td>
<td style="text-align:center"><code>Price^2</code></td>
<td style="text-align:center"><font color="#0000FF">5</font></td>
</tr>
</tbody>
</table>
<p>那么，FFM所有的（二阶）组合特征共有10项（\(\mathbf{C}_{5}^{2} = \frac{5 \times 4}{ 2!}= 10\)），即为：</p>
<p></p><p style="text-align:left"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_2_ffm_samples.png" width="700" height="200" alt="ffm_samples"></p>
<p>其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有\(\mathbf{C}_{n}^{2} = \frac{n(n-1)}{2}\)个。</p>
</blockquote>
<h4 id="FFM应用场景">FFM应用场景</h4><p>在我们的广告业务系统、商业推荐以及自媒体－推荐系统中，<strong>FFM模型作为点击预估系统中的核心算法之一，用于预估广告、商品、文章的点击率（CTR）和转化率（CVR）</strong>。</p>
<p>在鄙司广告算法团队，点击预估系统已成为基础设施，支持并服务于不同的业务线和应用场景。预估模型都是离线训练，然后定时更新到线上实时计算，因此预估问题最大的差异就体现在数据场景和特征工程。以广告的点击率为例，特征主要分为如下几类：</p>
<ul>
<li>用户属性与行为特征：</li>
<li>广告特征：</li>
<li>上下文环境特征：</li>
</ul>
<p>为了使用开源的FFM模型，所以的特征必须转化为<code>field_id:feat_id:value</code>格式，其中<code>field_id</code>表示特征所属field的编号，<code>feat_id</code>表示特征编号，value为特征取值。数值型的特征如果无需离散化，只需分配单独的field编号即可，如历史pseudo-ctr。categorical特征需要经过one-hot编码转化为数值型，编码产生的所有特征同属于一个field，特征value只能是0/1, 如用户年龄区间、性别、兴趣、人群等。</p>
<p><strong>开源工具FFM使用时，注意事项（参考新浪广告算法组的实战经验和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">美团－深入FFM原理与实践</a>）:</strong></p>
<ul>
<li>样本归一化：</li>
<li>特征归一化：</li>
<li>省略0值特征：</li>
</ul>
<p>回归、分类、排序等。推荐算法，预估模型（如CTR预估等）</p>
<h4 id="参考资料">参考资料</h4><ul>
<li>Sina广告点击预估系统实践</li>
<li>FM、FFM相关Paper、技术博客</li>
<li><a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html</a></li>
</ul>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/FFM/">FFM</a><a href="/tags/FM/">FM</a><a href="/tags/Factorization-Machine/">Factorization Machine</a><a href="/tags/Field-aware-FM/">Field-aware FM</a><a href="/tags/分解机器/">分解机器</a><a href="/tags/因子分解机/">因子分解机</a><a href="/tags/场感知分解机器/">场感知分解机器</a><a href="/tags/特征交叉/">特征交叉</a><a href="/tags/组合特征/">组合特征</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter6-boosting-family/" title="第06章：深入浅出ML之Boosting家族" itemprop="url">第06章：深入浅出ML之Boosting家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-12-12T14:42:16.000Z" itemprop="datePublished"> 发表于 2015-12-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-11-12</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li><a href="#0.写在前面">写在前面</a></li>
<li><a href="#1.Boosting">Boosting</a><ul>
<li><a href="#1.1.Boosting介绍">Boosting介绍</a></li>
<li><a href="#1.2.前向分步加法模型">前向分步加法模型</a></li>
<li><a href="#1.3.Boosting四大家族">Boosting四大家族</a></li>
</ul>
</li>
<li><a href="#2.AdaBoost">AdaBoost</a><ul>
<li><a href="#2.1.算法学习过程">算法学习过程</a> </li>
<li><a href="#2.2.算法实例">算法实例</a>    </li>
<li><a href="#2.3.训练误差分析">训练误差分析</a></li>
<li><a href="#2.4.前向分步加法模型与AdaBoost">前向分步加法模型与AdaBoost</a></li>
</ul>
</li>
<li><a href="#3.Boosted-Decision-Tree">Boosted Decision Tree</a></li>
<li><a href="#4.Gradient-Boosting">Gradient Boosting</a></li>
</ul>
<h1 id="0.写在前面">写在前面</h1>

<p>提升（boosting）方法是一类应用广泛且非常有效的统计学习方法。</p>
<p>在2006年，Caruana和Niculescu-Mizil等人完成了一项实验，比较当今世界上现成的分类器（off-the-shelf classifiers）中哪个最好？实现结果表明Boosted Decision Tree（提升决策树）不管是在misclassification error还是produce well-calibrated probabilities方面都是最好的分离器，以ROC曲线作为衡量指标。（效果第二好的方法是随机森林）</p>
<blockquote>
<p>参见paper：《An Empirical Comparison of Supervised Learning Algorithms》ICML2006.</p>
</blockquote>
<p> 下图给出的是Adaboost算法（Decision Stump as Weak Learner）在处理二类分类问题时，随着弱分类器的个数增加，训练误差与测试误差的曲线图。</p>
<center><br><table><tr><br><td><p style="text-align:left"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_0_0_adboost_binary_classification_1.png" width="420" height="340" alt="损失函数示意图"><br></p></td><br><td><p style="text-align:right"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_0_0_adboost_binary_classification_2.png" width="400" height="320" alt="损失函数示意图"><br></p></td><br></tr></table><br></center>

<p>从图中可以看出，Adaboost算法随着模型复杂度的增加，测试误差（红色点线）基本保持稳定，并没有出现过拟合的现象。</p>
<p>其实不仅是Adaboost算法有这种表现，Boosting方法的学习思想和模型结构上可以保证其不容易产生过拟合（除非Weak Learner本身出现过拟合）。</p>
<p>下面我们主要是从损失函数的差异，来介绍Boosting的家族成员；然后我们针对每个具体的家族成员，详细介绍其学习过程和核心公式；最后从算法应用场景和工具方法给出简单的介绍。</p>
<h1 id="1.Boosting">Boosting</h1>


<h2 id="1.1.Boosting介绍">Boosting介绍</h2>


<ul>
<li><p>基本思想</p>
<p>  Boosting方法基于这样一种思想：</p>
<blockquote>
<p>对于一个复杂任务来说，将多个专家的判断进行<strong>适当的综合</strong>得出的判断，要比其中任何一个专家单独的判断好。翻译一下就是”三个臭皮匠顶个诸葛亮”的意思…😄😄😄。</p>
</blockquote>
</li>
<li><p>历史由来</p>
<p>  历史上，Kearns和Valiant首先提出了”强可学习（strongly learnable）”和“弱可学习（weakly learnable）”的概念。他们指出：</p>
<blockquote>
<p><strong>在概率近似正确（probably approximately correct，PAC）学习框架中：</strong><br><br>①. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，<strong>并且正确率很高</strong>，那么就称这个概念是强可学习的；<br><br>②. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，<strong>学习的正确率仅比随机猜测略好</strong>，那么就称这个概念是弱可学习的。</p>
<p>Schapire后来证明了: <strong>强可学习和弱可学习是等价的。</strong> 也就是说，<strong>在PAC学习的框架下，一个概念是强可学习的 充分必要条件 是这个概念是弱可学习的。</strong> 表示如下：</p>
<p>  $$<br>  强可学习 \Leftrightarrow 弱可学习<br>  $$</p>
</blockquote>
<p>  如此一来，问题便成为：在学习中，如果已经发现了”弱学习算法”，那么能否将它提升为”强学习算法”？ 通常的，发现弱学习算法通常要比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，最具代表性的当数AdaBoost算法（是1995年由Freund和Schapire提出的）。</p>
</li>
<li><p>Boosting学习思路</p>
<p>  对于一个学习问题来说（以分类问题为例），给定训练数据集，求一个弱学习算法要比求一个强学习算法要容易的多。Boosting方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合弱分类器，得到一个强分类器。Boosting方法在学习过程中通过改变训练数据的权值分布，针对不同的数据分布调用弱学习算法得到一系列弱分类器。</p>
<p>  这里面有两个问题需要回答：</p>
<ol>
<li>在每一轮学习之前，如何改变训练数据的权值分布？</li>
<li><p>如何将一组弱分类器组合成一个强分类器？</p>
<blockquote>
<p>具体不同的boosting实现，主要区别在弱学习算法本身和上面两个问题的回答上。</p>
</blockquote>
<p>针对第一个问题，Adaboost算法的做法是：    </p>
<p><strong>提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。</strong></p>
<blockquote>
<p>如此，那些没有得到正确分类的样本，由于其权值加大而受到后一轮的弱分类器的更大关注。</p>
</blockquote>
<p>第二个问题，弱分类器的组合，AdaBoost采取<strong>加权多数表决</strong>的方法。具体地：</p>
<p><strong>加大 分类误差率小 的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</strong></p>
<p>AdaBoost算法的巧妙之处就在于它将这些学习思路自然并且有效地在一个算法里面实现。</p>
</li>
</ol>
</li>
</ul>
<h2 id="1.2.前向分步加法模型">前向分步加法模型</h2>


<p>英文名称：Forward Stagewise Additive Modeling</p>
<ul>
<li><p>加法模型（addtive model）</p>
<p>  $$<br>  f(x) = \sum_{k=1}^{K} \beta_k \cdot b(x; \gamma_k) \qquad(ml.1.6.1)<br>  $$ </p>
<p>  其中，\(b(x; \gamma_k)\) 为基函数，\(\gamma_k\)为基函数的参数，\(\beta_k\)为基函数的系数。</p>
</li>
<li><p>前向分步算法</p>
<p>  在给定训练数据及损失函数\(L(y,f(x))\)的条件下，学习加法模型\(f(x)\)成为经验风险极小化（即损失函数极小化）的问题：</p>
<p>  $$<br>  \min_{\beta_k, \gamma_k} \quad \sum_{i=1}^{M} L \left[y^{(i)}, \sum_{k=1}^{K} \beta_k b(x^{(i)}; \gamma_k)\right] \qquad(ml.1.6.2)<br>  $$</p>
<p>  通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：<strong>因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式\((ml.1.6.1)\)，那么就可以简化优化的复杂度</strong>。具体地，每步只需优化如下损失函数：</p>
<blockquote>
<p>$$<br>  \min_{\beta, \gamma} \quad \sum_{i=1}^{M} L(y^{(i)}, \beta b(x^{(i)}; \gamma)) \qquad(n.ml.1.6.1)<br>  $$</p>
</blockquote>
<p>  给定训练数据集\(D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y} = \)\(\{-1, +1\}\)。损失函数\(L(y, f(x))\)和基函数的集合\(\{b(x; \gamma)\}\)，学习加法模型\(f(x)\)的前向分步算法如下：</p>
<blockquote>
<p>\(<br>  \{ \\\<br>  \quad 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}; 损失函数L(y, f(x));\\\<br>  \qquad\quad  基函数集\{b(x, \gamma)\}； \\\<br>  \quad 输出：加法模型f(x)。 \\\<br>  \quad 计算过程：\\\<br>  \qquad (1). 初始化f_0(x) = 0 \\\<br>  \qquad (2). 对于k=1,2,\cdots,K \\\<br>  \qquad\qquad (a). 极小化损失函数 \\\<br>  \qquad\qquad\qquad (\beta_k, \gamma_k) = \arg \min_{\beta, \gamma} \sum_{i=1}^{M} L(y^{(i)}, f_{k-1}(x^{(i)}) + \beta b(x; \gamma)) \quad(n.ml.1.6.2)\\\<br>  \qquad\qquad 得到参数\beta_k, \gamma_k. \\\<br>  \qquad\qquad (b). 更新 \\\<br>  \qquad\qquad\qquad f_k(x) = f_{k-1}(x) + \beta_k b(x; \gamma_k) \qquad(n.ml.1.6.3) \\\<br>  \qquad (3). 得到加法模型 \\\<br>  \qquad\qquad\qquad f(x) = f_K(x) = \sum_{k=1}^{K} \beta_k b(x; \gamma_k) \qquad(n.ml.1.6.4) \\\<br>  \}<br>  \)</p>
</blockquote>
<p>  这样，前向分步算法将<strong>同时求解</strong>从\(k=1\)到\(K\)的所有参数\(\beta_k, \gamma_k\)的优化问题简化为<strong>逐次求解</strong>各个\(\beta_k, \gamma_k\)的优化问题。</p>
</li>
</ul>
<p><strong>问题：为什么需要前向分布算法？</strong></p>
<p>_主要为了简化优化问题的复杂度，因为基于多个弱分类器的强分类器全局最优解不容易求，退而求其次，每次仅学习一个弱分类器和对应模型权重，来逼近模型最优解。_</p>
<h2 id="1.3.Boosting四大家族">Boosting四大家族</h2>

<p>Boosting并非是一个方法，而是一类方法。这里按照损失函数的不同，将其细分为若干类算法，下表给出了4种不同损失函数对应的Boosting方法：</p>
<table>
<thead>
<tr>
<th style="text-align:center">名称(Name)</th>
<th style="text-align:center">损失函数(Loss)</th>
<th style="text-align:center">导数(Derivative)</th>
<th style="text-align:center">目标函数\(f^*\)</th>
<th style="text-align:center">算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">平方损失<br>(Squared Error)</td>
<td style="text-align:center">\(\frac{1}{2} (y^{(i)} - f(x^{(i)}))^2\)</td>
<td style="text-align:center">\(y^{(i)} - f(x^{(i)})\)</td>
<td style="text-align:center">\(E[y \vert x^{(i)}]\)</td>
<td style="text-align:center">L2Boosting</td>
</tr>
<tr>
<td style="text-align:center">绝对损失<br>(Absolute Error)</td>
<td style="text-align:center">\(\vert y^{(i)} - f(x^{(i)}) \vert\)</td>
<td style="text-align:center">\(sign(y^{(i)} - f(x^{(i)})\)</td>
<td style="text-align:center">\(median(y \vert x^{(i)})\)</td>
<td style="text-align:center">Gradient Boosting</td>
</tr>
<tr>
<td style="text-align:center">指数损失<br>(Exponentail Loss)</td>
<td style="text-align:center">\(\exp(- \tilde {y^{(i)}} f(x^{(i)}))\)</td>
<td style="text-align:center">\(- \tilde {y^{(i)}} exp(-\tilde {y^{(i)}} f(x^{(i)}))\)</td>
<td style="text-align:center">\(\frac{1}{2} \log \frac{\pi_i}{1 - \pi_i}\)</td>
<td style="text-align:center">AdaBoost</td>
</tr>
<tr>
<td style="text-align:center">对数损失<br>(LogLoss)</td>
<td style="text-align:center">\(\log (1+e^{- \tilde{y^{(i)}} f_i})\)</td>
<td style="text-align:center">\(y^{(i)} - \pi_i\)</td>
<td style="text-align:center">\(\frac{1}{2} \log \frac{\pi_i}{1 - \pi_i}\)</td>
<td style="text-align:center">LogitBoost</td>
</tr>
</tbody>
</table>
<blockquote>
<p>说明：</p>
<p>该表来自于<a href="">Machine Learning: A Probabilistic Perspective</a>P587页</p>
<p><strong>L2Boosting全称：Least Squares Boosting；该算法由Buhlmann和Yu在2003年提出。</strong></p>
</blockquote>
<p>二分类问题时损失函数示意图：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_1_1_loss_function_graph.png" width="550" height="440" alt="损失函数示意图"></p>
<p>下面主要以AdaBoost算法作为示例，给出以下3个问题的解释：</p>
<ul>
<li>AdaBoost为什么能够提升学习精度？</li>
<li>如何解释AdaBoost算法？</li>
<li>Boosting方法更具体的实例－Boosting Tree。</li>
</ul>
<p>下面首先介绍Adaboost算法。</p>
<h1 id="2.Adaboost">Adaboost</h1>

<p>问题引导：</p>
<ol>
<li>为什么adaboost算法的损失函数是指数损失？问题有误导，如果不是指数损失，那么就不是adaboost算法了～ adaboost算法采用前向分步算法学习得到一个加法模型，学习目标是通过调整样本权重分布使得下一个分类器误差率越小越好；而误分类的样本权重相当于放大了\(e^{2*分类器权重}\)倍，并且证明得到训练误差率呈指数速率下降.</li>
<li>adaboost模型特点？样本权重分布重新计算，模型线性累加得到，训练误差率呈指数下降，输出值不能表示概率；</li>
</ol>
<h2 id="2.1.算法学习过程">算法学习过程</h2>


<p>Adaboost算法在分类问题中的主要特点：<strong>通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。</strong> AdaBoost－算法描述（伪代码）如下：</p>
<p>\(<br>    \begin{align}<br>    &amp;\{ \\\<br>    &amp;\quad 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)},y^{(M)})\}，x^{(i)} \in \mathcal{X} \subseteq R^N，\\\<br>    &amp;\qquad\qquad y^{(i)} \in \mathcal{Y} = \{-1, +1\}, 弱分类器; \\\<br>    &amp;\quad 输出：最终分类器G(x). \\\<br>    &amp;\quad 过程：\\\<br>    &amp;\qquad (1). 初始化训练数据的权值分布 \\\<br>    &amp;\qquad\qquad\qquad D_1=(w_{11}, w_{12}, \cdots, w_{1M}), \quad w_{1i}=\frac{1}{M}, \; i=1,2,\cdots,M \\\<br>    &amp;\qquad (2). 训练K个弱分类器 k=1,2,\cdots,K \\\<br>    &amp;\qquad\qquad (a). 使用具有权值分布D_k的训练数据集学习，得到基本分类器 \\\<br>    &amp;\qquad\qquad\qquad\qquad G_k(x): \mathcal{X} \rightarrow \{-1, +1\} \qquad\qquad(ml.1.6.3) \\\<br>    &amp;\qquad\qquad (b). 计算G_k(x)在训练数据集上的分类误差率 \\\<br>    &amp;\qquad\qquad\qquad\qquad e_k = P(G_k(x^{(i)}) \not= y^{(i)}) = \sum_{i=1}^{M} w_{ki} I(G_k(x^{(i)}) \not= y^{(i)}) \qquad(ml.1.6.4) \\\<br>    &amp;\qquad\qquad (c). 计算G_k(x)的系数 \\\<br>    &amp;\qquad\qquad\qquad\qquad \alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k} \quad(e是自然对数) \qquad(ml.1.6.5) \\\<br>    &amp;\qquad\qquad (d). 更新训练数据集的权值分布 \\\<br>    &amp;\qquad\qquad\qquad\qquad D_{k+1} = (w_{k+1,1}, w_{k+1,2}, \cdots, w_{k+1,M})\\\<br>    &amp;\qquad\qquad\qquad\qquad w_{k+1,i} = \frac{w_{k,i}}{Z_k} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})), \quad i=1,2,\cdots,M \quad(ml.1.6.6) \\\<br>    &amp;\qquad\qquad\qquad Z_k是规范化因子 \\\<br>    &amp;\qquad\qquad\qquad\qquad Z_k = \sum_{i=1}^{M} w_{k,i} \cdot \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \qquad(ml.1.6.7) \\\<br>    &amp;\qquad\qquad\qquad 使D_{k+1}成为一个概率分布。\\\<br>    &amp;\qquad (3). 构建基本分类器的线性组合 \\\<br>    &amp;\qquad\qquad\qquad f(x) = \sum_{k=1}^{K} \alpha_k G_k(x) \qquad(ml.1.6.8)\\\<br>    &amp;\qquad 得到最终的分类器 \\\<br>    &amp;\qquad\qquad G(x) = sign (f(x)) = sign \left(\sum_{k=1}^{K} \alpha_k G_k(x) \right) \qquad(ml.1.6.9) \\\<br>    &amp;\}<br>    \end{align}<br>    \)</p>
<ul>
<li><p><strong>AdaBoost算法描述说明</strong></p>
<ul>
<li><p>步骤（1）假设训练数据集具有均匀（相同）的权值分布，即每个训练样本在基本分类器的学习中作用相同。</p>
<blockquote>
<p>这一假设保证，第一步能在原始数据上学习基本分类器\(G_1(x)\)。</p>
</blockquote>
</li>
<li><p>步骤（2）AdaBoost反复学习基本分类器，在每一轮\(k=1,2,\cdots,K\)顺序地执行下列操作：</p>
<ul>
<li>(a) 学习基本分类器：使用当前分布\(D_k\)加权的训练数据集，学习基本分类器\(G_k(x)\)；</li>
<li><p>(b) 误差率：计算基本分类器\(G_k(x)\)在加权训练数据集上的分类误差率</p>
<blockquote>
<p>$$<br>  e_k = P(G_k(x^{(i)}) \not= y^{(i)}) = \sum_{G_k(x^{(i)}) \not= y^{(i)}} w_{ki} \qquad(n.ml.1.6.5)<br>  $$</p>
</blockquote>
<p>  这里，\(w_{ki}\)表示第\(k\)轮中第\(i\)个样本的权值，\(\sum_{i=1}^{M} w_{ki} = 1\)。</p>
<blockquote>
<p>这表明，\(G_k(x)\)在加权的训练数据集上的分类误差率 是 被\(G_k(x)\)误分类样本的权值之和。由此可以看出数据权值分布\(D_k\)与基本分类器\(G_k(x)\)的分类误差率的关系。</p>
</blockquote>
</li>
<li><p>(c) 分类器权重：计算基本分类器\(G_k(x)\)的系数\(\alpha_k\)，\(\alpha_k\)表示\(G_k(x)\)在最终分类器中的重要性</p>
<blockquote>
<p>根据(ml.1.6.3)中公式可知，当\(e_k \le 0.5\)时，\(\alpha_k \ge 0\)，并且\(\alpha_k\)随着\(e_k\)的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。</p>
</blockquote>
</li>
<li><p>(d) 更新训练数据的权值分布，为下一轮做准备，公式\((ml.1.6.6)\)可以写成：</p>
<blockquote>
<p>$$<br>  w_{k+1, i} =<br>  \begin{cases}<br>  \frac{w_{ki}} {Z_k} e^{-\alpha_k}, &amp;\quad G_k(x^{(i)}) = y^{(i)} \\\<br>  \frac{w_{ki}} {Z_k} e^{\alpha_k}, &amp;\quad G_k(x^{(i)}) \not= y^{(i)}<br>  \end{cases} \qquad(n.ml.1.6.6)<br>  $$</p>
<p>由此可知，被基本分类器\(G_k(x)\)误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。相比较来说，<strong>误分类样本的权值被放大\(\mathbf{e^{2\alpha_k}} = \frac{e_k}{1-e_k}\)倍</strong>。因此，误分类样本在下一轮学习中起更大的作用。</p>
</blockquote>
<p><strong>不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器中起不同的作用，这也是AdaBoost的一个特点。</strong></p>
</li>
</ul>
</li>
<li><p>步骤（3）线性组合\(f(x)\)实现\(K\)个基本分类器的加权表决。系数\(\alpha_k\)表示了极本分类器\(G_k(x)\)的重要性。</p>
<blockquote>
<p> 注意：在这里所有\(\alpha_k\)之和并不为1。\(f(x)\)的符号决定实例\(x\)的类别，\(f(x)\)的绝对值表示分类的精确度。</p>
</blockquote>
<p>  <strong>利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</strong></p>
</li>
</ul>
</li>
</ul>
<p><strong>问题：Adaboost算法的特点有哪些？</strong></p>
<p>_样本权重分布，弱分类器线性组合。_</p>
<h2 id="2.2.示例：AdaBoost算法">示例：AdaBoost算法</h2>

<p>此示例参考李航老师的《统计学习方法》.</p>
<blockquote>
<p>给定下表所示训练数据。</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>9</td>
</tr>
<tr>
<td>y</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>-1</td>
</tr>
</tbody>
</table>
<p>假设弱分类器由\(x \le v 或 x&gt;v\)产生，其阈值\(v\)使该分类器在训练数据集上分类误差率最低。试用AdaBoost算法学习一个强分类器。</p>
<p>解： 首先初始化数据权值分布（均匀分布）：</p>
<p>$$<br>D_1 = (w_{1,1}, w_{1,2}, \cdots, w_{1,10}), \quad w_{1,i}=0.1, \;  i=1,2,\cdots,10<br>$$</p>
<p>对\(k=1\)，</p>
<p>(a). 在权值分布为\(D_1\)的训练数据上，阈值\(v\)取2.5时，分类误差率最低，故基本分类器为：</p>
<p>$$<br>G_1(x) =<br>\begin{cases}<br>1,&amp; \quad x &lt; 2.5 \\\<br>-1, &amp; \quad x &gt; 2.5<br>\end{cases}<br>$$</p>
<p>(b). \(G_1(x)\)在训练数据集上的误差率\(e_1=P(G_1(x^{(i)}) \not= y^{(i)})=0.3\);</p>
<p>(c). 计算\(G_1(x)\)的系数：\(\alpha_1 = \frac{1}{2} \log \frac{1-e_1}{e_1} = 0.4236\)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$<br>    \begin{align}<br>    &amp; D_2 = (w_{2,1}, w_{2,2}, \cdots, w_{2,10}) \\\<br>    &amp; w_{2,i} = \frac{w_{1,i}} {Z_1} \exp(-\alpha_1 y^{(i)} G_1(x^{(i)})), \quad i=1,2,\cdots,10 \\\<br>    &amp; D_2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, \\\<br>    &amp;\qquad\;\; 0.1666, 0.1666, 0.1666, 0.0715) \\\<br>    &amp; f_1(x) = 0.4236 G_1(x)<br>    \end{align}<br>$$</p>
<p>分类器\(sign[f_1(x)]\)在训练数据上有3个误分类点。</p>
<p>对\(k=2\)，</p>
<p>(a). 在权值分布为\(D_2\)的训练数据上，阈值\(v\)取8.5时，分类误差率最低，基本分类器为：</p>
<p>$$<br>    G_2(x) =<br>    \begin{cases}<br>    1,&amp; \quad x &lt; 8.5 \\\<br>    -1, &amp; \quad x &gt; 8.5<br>    \end{cases}<br>    $$</p>
<p>(b). \(G_2(x)\)在训练数据集上的误差率\(e_2=0.2143\);</p>
<p>(c). 计算\(G_2(x)\)的系数：\(\alpha_2 = 0.6496\)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$<br>    \begin{align}<br>    &amp; D_3=(0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.1667, \\\<br>    &amp;\qquad\;\; 0.1060, 0.1060, 0.1060, 0.0455) \\\<br>    &amp; f_2(x) = 0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x)<br>    \end{align}<br>$$</p>
<p>分类器\(sign[f_2(x)]\)在训练数据上有3个误分类点。</p>
<p>对\(k=3\)，</p>
<p>(a). 在权值分布为\(D_3\)的训练数据上，阈值\(v\)取5.5时，分类误差率最低，基本分类器为：</p>
<p>$$<br>G_3(x) =<br>\begin{cases}<br>1,&amp; \quad x &lt; 5.5 \\\<br>-1, &amp; \quad x &gt; 5.5<br>\end{cases}<br>$$</p>
<p>(b). \(G_3(x)\)在训练数据集上的误差率\(e_3=0.1820\);</p>
<p>(c). 计算\(G_3(x)\)的系数：\(\alpha_2 = 0.7514\)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$<br>    \begin{align}<br>    &amp; D_4=(0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065, 0.065, 0.065, 0.125)<br>    \end{align}<br>$$</p>
<p>于是得到模型线性组合</p>
<p>$$<br>    f_3(x) = 0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7514 \cdot G_3(x)<br>    $$</p>
<p>分类器\(sign[f_3(x)]\)在训练数据上误分类点个数为0。</p>
<p>于是最终分类器为：</p>
<p>$$<br>    \begin{align}<br>    G(x) &amp;= sign[f_3(x)] \\<br>    &amp;= sign[0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7514 \cdot G_3(x)]<br>    \end{align}<br>$$</p>
</blockquote>
<h2 id="2.3.训练误差分析">训练误差分析</h2>


<p><strong>AdaBoost算法最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。</strong> 对于这个问题，有个定理可以保证分类误差率在减少－AdaBoost的训练误差界。</p>
<ul>
<li><strong>定理：AdaBoost训练误差界</strong></li>
</ul>
<table>
<thead>
<tr>
<th>［定理］AdaBoost训练误差界</th>
</tr>
</thead>
<tbody>
<tr>
<td>$$ \frac{1}{M} \sum_{i=1}^{M} I(G(x^{(i)}) \not= y^{(i)}) \le \frac{1}{M} \sum_{i} \exp(-y^{(i)} f(x^{(i)})) = \prod_{k=1}^{K} Z_k \qquad(ml.1.6.10) $$</td>
</tr>
<tr>
<td>其中，\(G(x), f(x)\)和\(Z_k\)分别由公式\((ml.1.6.9), (ml.1.6.8), (ml.1.6.7)\)给出</td>
</tr>
</tbody>
</table>
<blockquote>
<p>证明如下：</p>
<p>当\(G(x^{(i)}) \not= y^{(i)}\)时，\(y^{(i)} f(x^{(i)}) &lt; 0\)，因而\(\exp(-y^{(i)} f(x^{(i)})) \ge 1\)。由此，可以直接推导出前半部分。</p>
<p>后半部分的推导要用到\(Z_k\)的定义式\((ml.1.6.7)\)和\((ml.1.6.6)\)的变形:</p>
<p>$$<br>w_{k,i} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) = Z_k w_{k+1,i} \qquad(n.ml.1.6.7)<br>$$</p>
<p>推导如下：</p>
<p>$$<br>    \begin{align}<br>    \frac{1}{M} \sum_{i=1}^{M} \exp(-y^{(i)} f(x^{(i)})) &amp;= \underline{ \frac{1}{M} \sum_{i=1}^{M} } \exp \left(-\sum_{k=1}^{K} \alpha_k y^{(i)} G_k(x^{(i)}) \right) \\\<br>    &amp;= \underline{ \sum_{i=1}^{M} w_{1,i} } \prod_{k=1}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\\<br>    &amp;= \sum_{i=1}^{M} w_{1,i} \underline{ \exp(-\alpha_1 y^{(i)}) G_k(x^{(i)}) \prod_{k=2}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) } \\\<br>    &amp;= Z_1 \sum_{i=1}^{M} w_{2,i} \prod_{k=2}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\\<br>    &amp;= Z_1 Z_2 \sum_{i=1}^{M} w_{3,i} \prod_{k=3}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\\<br>    &amp;= \cdots \cdots \\\<br>    &amp;= Z_1 Z_2 \cdots Z_{K-1} \sum_{i=1}^{M} w_{K,i} \exp(-\alpha_K y^{(i)} G_K(x^{(i)})) \\\<br>    &amp;= \prod_{k=1}^{K} Z_k<br>    \end{align} \quad(n.ml.1.6.8)<br>$$</p>
<p>注意：\(w_{1,i} = \frac{1}{M}\)</p>
</blockquote>
<p>这一定理说明：<strong>可以在每一轮选取适当的\(G_k\)使得\(Z_k\)最小，从而使训练误差下降最快。</strong> 对于二类分类问题，有如下定理。</p>
<ul>
<li>定理：二类分类问题AdaBoost训练误差界</li>
</ul>
<table>
<thead>
<tr>
<th>［定理］二类分类问题AdaBoost训练误差界</th>
</tr>
</thead>
<tbody>
<tr>
<td>$$\prod_{k=1}^{K} Z_k = \prod_{k=1}^{K} \left[2 \sqrt{e_k(1-e_k)} \;\right] = \prod_{k=1}^{K} \sqrt{(1-4\gamma_k^2)} \le \exp \left(-2 \sum_{k=1}^{K} \gamma_k^2 \right) \qquad(ml.1.6.9)$$</td>
</tr>
<tr>
<td>这里，\(\gamma_k = 0.5 - e_k \)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>证明：由公式\((ml.1.6.7)\)和\((n.ml.1.6.5)\)可得：</p>
<p>$$<br>\begin{align}<br>Z_k &amp;= \sum_{i=1}^{M} w_{k,i} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\<br>&amp;= \sum_{y^{(i)} = G_k(x^{(i)})} w_{k,i} \cdot e^{-\alpha_k} + \sum_{y^{(i)} \not= G_k(x^{(i)})} w_{k,i} \cdot e^{\alpha_k} \\<br>&amp;= (1-e_k) \cdot e^{-\alpha_k} + e_k \cdot e^{\alpha_k} \\<br>&amp;= 2 \sqrt{e_k (1-e_k)} = \sqrt{1-4\gamma_m^2}<br>\end{align} \qquad(n.ml.1.6.9)<br>$$</p>
<p>注：\(\alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k}, e^{\alpha_k} = \sqrt{\frac{1-e_k}{e_k}}\)</p>
<p>对于不等式部分</p>
<p>$$<br>\prod_{k=1}^{K} \sqrt{1-4\gamma_m^2} \le \exp \left(-2 \sum_{k=1}^{K} \gamma_k^2 \right) \qquad(n.ml.1.6.10)<br>$$</p>
<p>则可根据\(e^x\)和\(\sqrt{1-x}\)在点\(x=0\)的泰勒展开式推出不等式\(\sqrt{1-4\gamma_m^2} \le \exp(-2 \gamma_m^2)\)。</p>
</blockquote>
<table>
<thead>
<tr>
<th>[推论] AdaBoost训练误差指数速率下降</th>
</tr>
</thead>
<tbody>
<tr>
<td>如果存在\(\gamma &gt; 0\)，对所有的\(m\)有\(\gamma_k \ge \gamma\)，则有</td>
</tr>
</tbody>
</table>
<p>$$\frac{1}{M} \sum_{i=1}^{M} I(G(x^{(i)}) \not= y^{(i)}) \le \exp(-2K\gamma^2) \qquad(ml.1.6.12)$$|</p>
<p>推论表明，在此条件下，<strong>AdaBoost的训练误差是以指数速率下降的</strong>。这一性质对于AdaBoost计算（迭代）效率是利好消息。</p>
<blockquote>
<p>注意：AdaBoost算法不需要知道下界\(\gamma\)，这正是Freund和Schapire设计AdaBoost时所考虑的。与一些早期的提升方法不同，AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率。这也是其算法名称的由来（适应的提升）。Ada是Adaptive的简写。</p>
</blockquote>
<h2 id="2.4.前向分步加法模型与Adaboost">前向分步加法模型与Adaboost</h2>

<p>AdaBoost算法还有另一个解释，即可以认为<strong>AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法</strong>时的学习方法。</p>
<p>根据前向分步算法可以推导出AdaBoost，用一句话叙述这一关系.</p>
<table>
<thead>
<tr>
<th>AdaBoost算法是前向分步加法算法的特例</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>此时，模型是由基本分类器组成的加法模型，损失函数是指数函数。</p>
<blockquote>
<p>证明：<strong>前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器：</strong></p>
<p>$$<br>f(x) = \sum_{k=1}^{K} \alpha_k G_k(x) \qquad (n.ml.1.6.11)<br>$$</p>
<p>由基本分类器\(G_k(x)\)及其系数\(\alpha_k\)组成，\(k=1,2,\cdots,K\)。前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。</p>
<p>下面证明：</p>
<p><strong>前向分步算法的损失函数是指数损失函数(\(L(y, f(x)) = \exp [-y f(x)]\)) 时，其学习的具体操作等价于AdaBoost算法学习的具体操作。</strong> </p>
<p>假设经过\(k-1\)轮迭代，前向分步算法已经得到\(f_{k-1}(x)\):</p>
<p>$$<br>\begin{align}<br>f_{k-1} (x) &amp;= f_{k-2}(x) + \alpha_{k-1} G_{k-1}(x) \\\<br>&amp;= \alpha_1 G_1(x) + \cdots + \alpha_{m-1} G_{m-1}(x)<br>\end{align}  \qquad(n.ml.1.6.12)<br>$$</p>
<p>在第\(k\)轮迭代得到\(\alpha_k, G_k(x)\)和\(f_k(x)\)。</p>
<p>$$<br>f_{k} (x) = f_{k-1}(x) + \alpha_{k} G_{k}(x) \qquad(n.ml.1.6.13)<br>$$</p>
<p><strong>学习的目标是使前向分步算法得到的\(\alpha_k\)和\(G_k(x)\)使\(f_k(x)\)在训练数据集\(D\)上的指数损失最小</strong>，即</p>
<p>$$<br>(\alpha_k, G_k(x)) = \arg \min_{\alpha, G} \underbrace{ \sum_{i=1}^{M} \exp [-y^{(i)} (f_{k-1}(x) + \alpha G(x^{(i)}))] }_{指数损失表达式} \quad(n.ml.1.6.14)<br>$$</p>
<p>进一步可表示为：</p>
<p>$$<br>(\alpha_k, G_k(x)) = \arg \min_{\alpha, G} \sum_{i=1}^{M} \overline{w}_{k,i}  \cdot \exp [-y^{(i)} \alpha G(x^{(i)})] \quad(n.ml.1.6.15)<br>$$</p>
<p>其中，<strong>\(\overline{w}_{k,i} = \exp [-y^{(i)} f_{k-1} (x^{(i)})]\)表示第\(i\)样本在之前模型上的指数损失</strong>。因为\(\overline{w}_{k,i}\)既不依赖\(\alpha\)也不依赖\(G\)，所以与最小化无关。但\(\overline{w}_{k,i}\)依赖于\(f_{k-1}(x)\)，随着每一轮迭代而发生变化。</p>
<p>现在使公式\((n.ml.1.6.15)\)达到最小的\(\alpha_k^{\ast}\)和\(G_k^{\ast}\)就是AdaBoost算法所得到的\(\alpha_k\)和\(G_k(x)\)。求解公式\((n.ml.1.6.15)\)可分为两步：</p>
<p>第一步：求\(G_k^{\ast}\). 对于任意\(\alpha &gt; 0\)，使公式\((n.ml.1.6.15)\)最小的\(G(x)\)由下式得到：</p>
<p>$$<br>G_k^{\ast}(x) = \arg \min_{G} \sum_{i=1}^{M} \overline{w}_{k,i} \cdot I(y^{(i)} \not= G(x^{(i)})) \qquad(n.ml.1.6.16)<br>$$</p>
<p> 此分类器\(G_k^{\ast}(x)\)即为AdaBoost算法的基本分类器\(G_k(x)\)，因为它是使第\(k\)轮加权训练数据分类误差率最小的基本分类器。</p>
<p>之后，求\(\alpha_k^{\ast}\)。参考公式\((n.ml.1.6.5)\)，公式\((n.ml.1.6.15)\)中的：</p>
<p>$$<br>\begin{align}<br>\sum_{i=1}^{M} \overline{w}_{k,i}  \cdot \exp [-y^{(i)} \alpha G(x^{(i)})] &amp;= \sum_{y^{(i)} = G_k(x^{(i)})} \overline{w}_{k,i} \cdot e^{-\alpha} + \sum_{y^{(i)} \neq G_k(x^{(i)})} \overline{w}_{k,i} \cdot e^{\alpha} \\\<br>&amp;= (e^{\alpha} - e^{-\alpha}) \sum_{i=1}^{M} \overline{w}_{k,i} I(y^{(i)} \neq G(x^{(i)})) + e^{-\alpha} \sum_{i=1}^{M} \overline{w}_{k,i}<br>\end{align} \quad(n.ml.1.6.17)<br>$$</p>
<p>把已得到的\(G_k^{\ast}\)带入公式\((n.ml.1.6.17)\)，并对\(\alpha\)求导（导数为0），即得到使公式\((n.ml.1.6.15)\)最小的\(\alpha\)。</p>
<p>$$<br>\alpha_{k}^{\ast} = \frac{1}{2} \log \frac{1-e_k}{e_k}<br>$$</p>
<p>\(e_k\)为分类误差率</p>
<p>$$<br>e_k = \frac{ \sum_{i=1}^{M} \overline{w}_{k,i} I(y^{(i)} \neq G_k(x^{(i)})) } {\sum_{i=1}^{M} \overline{w}_{k,i} } = \sum_{i=1}^{M} w_{k,i} I(y^{(i)} \neq G_k(x^{(i)}))<br>$$</p>
<p>可以看出，这里求得的\(\alpha_k^{\ast}\)与AdaBoost算法\((2)-(c)\)步的\(\alpha_k\)完全一致。</p>
<p>最后看一下每一轮样本权值的更新。由：\(f_k(x) = f_{k-1}(x) + \alpha_k G_k(x)\)以及\(\overline{w}_{k,i} = \exp[-y^{(i)} f_{k-1}(x^{(i)})]\)，可得：</p>
<p>$$<br>\overline{w}_{k+1,i} = \overline{w}_{k,i} \exp[-y^{(i)} \alpha_k G_k(x)]<br>$$</p>
<p>这与Adaboost算法的第\((2)-(d)\)步的样本权值的更新，可看出二者是等价的（只相差规范化因子）。</p>
</blockquote>
<ul>
<li><p>AdaBoost算法缺点</p>
<ul>
<li><p><strong>对异常点敏感</strong></p>
<p>  指数损失存在的一个问题是不断增加误分类样本的权重（指数上升，样本权重放大\( e^{(2*弱分类器权重)} \) ）。如果数据样本是异常点（outlier），会极大的干扰后面基本分类器学习效果；</p>
</li>
<li><p><strong>模型无法用于概率估计</strong></p>
<blockquote>
<p>MLAPP中的原话：”\(e^{-\tilde{y}f}\) is not the logarithm of any pmf for binary variables \(\tilde{y} \in \{-1, +1\}\); consequently we cannot recover probability estimate from \(f(x)\).” </p>
<p>意思就是说对于取值为\(\tilde{y} \in \{-1, +1\}\)的随机变量来说，\(e^{-\tilde{y}f}\)不是任何概率密度函数的对数形式，模型\(f(x)\)的结果无法用概率解释。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h1 id="3.Boosted-Decision-Tree">Boosted Decision Tree</h1>

<p>提升决策树是指以<strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#分类与回归树" target="_blank" rel="external">分类与回归树（CART）</a></strong>为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一。</p>
<blockquote>
<p>提升决策树简称提升树，Boosting Tree.</p>
</blockquote>
<h2 id="3.1.提升树模型">提升树模型</h2>

<p>提升树模型实际采用加法模型（即基函数的线性组合）与前向分步算法，以_决策树为基函数_的提升方法称为提升树（Boosting Tree）。</p>
<blockquote>
<p>对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。在6.1.3节AdaBoost例子中，基本分类器是\(x<v\\)或\\(x>v\)，可以看作是由一个跟结点直接连接两个叶结点的简单决策树，即所谓的<a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#写在前面" target="_blank" rel="external">决策树桩（Decision Stump）</a>。</v\\)或\\(x></p>
</blockquote>
<p>提升树模型可以表示为CART决策树的加法模型：</p>
<p>$$<br>f_K(x) = \sum_{k=1}^{K} T(x; \Theta_k) \qquad(ml.1.6.13)<br>$$</p>
<p>其中，\(T(x; \Theta_k)\)表示二叉决策树，\(\Theta_k\)为决策树的参数，\(K\)为树的个数。</p>
<blockquote>
<p>基本学习器－CART决策树，请参考<a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/" target="_blank" rel="external">第03章：深入浅出ML之Tree-Based家族</a></p>
</blockquote>
<h2 id="3.2.提升树算法">提升树算法</h2>

<p>提升树算法采用前向分步算法。首先确定初始提升树\(f_0(x) = 0\)，第\(k\)步的模型为：</p>
<p>$$<br>f_k(x) = f_{k-1}(x) + T(x; \Theta_k) \qquad(ml.1.6.14)<br>$$</p>
<p>其中，\(f_{k-1}(x)\)为当前模型，通过<strong>经验风险极小化</strong>确定下一颗决策树的参数\(\Theta_k\)，</p>
<p>$$<br>\hat{\Theta}_k = \arg \min_{\Theta_k} \sum_{i=1}^{M} L(y^{(i)}, \; f_{k-1}(x) + T(x^{(i)}; \Theta_k))  \qquad(ml.1.6.15)<br>$$</p>
<blockquote>
<p>由于树的线性组合可以很好的拟合训练数据，即使数据中的输入和输出之间的关系很复杂也是如此，所以提升树是一个高效的学习算法。 </p>
</blockquote>
<p><strong>提升树家族</strong></p>
<p>不同问题的提升树学习算法，其主要区别在于<strong>损失函数</strong>不同。平方损失函数常用于回归问题，用指数损失函数用于分类问题，以及绝对损失函数用于决策问题。</p>
<ul>
<li><p><strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#二叉分类树" target="_blank" rel="external">二叉分类树</a></strong></p>
<p>  对于二分类问题，提升树算法只需要将AdaBoost算法例子中的基本分类器限制为<strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#二叉分类树" target="_blank" rel="external">二叉分类树</a></strong>即可，可以说此时的决策树算法时AdaBoost算法的特殊情况。</p>
<blockquote>
<p>损失函数仍为指数损失，提升树模型仍为前向加法模型。</p>
</blockquote>
</li>
<li><p><strong><a href="http://www.52caml.com/head_first_ml/ml-chapter3-tree-based-family/#二叉回归树" target="_blank" rel="external">二叉回归树</a></strong></p>
<p>  已知训练数据集\(D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y} \) \( \subseteq R, \mathcal{Y}\)为输出空间。如果将输入空间\(\mathcal{X}\)划分为\(J\)个互不相交的区域\(R_1, R_2, \cdots, R_J\)，并且在每个区域上确定输出的常量\(c_j\)，那么树可以表示为：</p>
<p>  $$<br>  T(x; \Theta) = \sum_{j=1}^{J} c_j I(x \in R_j)  \qquad(ml.1.6.16)<br>  $$</p>
<p>  其中，参数\(\Theta=\{(R_1, c_1), (R_2, c_2), \cdots, (R_J, c_J)\}\)表示树的区域划分和各区域上的常数。\(J\)是回归树的复杂度即叶结点的个数。</p>
</li>
<li><p>回归问题提升树－前向分步算法</p>
<blockquote>
<p>回归问题提升树使用以下前向分步算法：</p>
<p>  $$<br>  \begin{align}<br>  f_0(x) &amp;= 0 \\\<br>  f_k(x) &amp;= f_{k-1}(x) + T(x; \Theta_k) \quad k=1,2,\cdots,K \\\<br>  f_K(x) &amp;= \sum_{k=1}^{K} T(x; \Theta_k)<br>  \end{align} \qquad(n.ml1.6.17)<br>  $$</p>
<p>在前向分布算法的第\(k\)步，给定当前模型\(f_{k-1}(x)\)，需求解：</p>
<p>  $$<br>  \hat{\Theta}_k = \arg \min_{\Theta_k} \sum_{i=1}^{M} \underbrace{ L(y^{(i)}, \; f_{k-1}(x) + T(x^{(i)}; \Theta_k)) }_{损失函数} \qquad(n.ml.1.6.18)<br>  $$</p>
<p>得到\(\hat{\Theta}_k\)，即第\(k\)颗树的参数。</p>
<p>当采用<strong>平方误差损失函数</strong>时，</p>
<p>  $$<br>  L(y, f(x)) = (y - f(x))^2 \qquad (n.ml.1.6.19)<br>  $$</p>
<p>将平方误差损失函数展开为：</p>
<p>  $$<br>  \begin{align}<br>  &amp;L(y, f_{k-1}(x) + T(x; \Theta_k)) \\\<br>  &amp;\qquad = [y - f_{k-1}(x) - T(x; \Theta_k)]^2 \\\<br>  &amp;\qquad = [r - T(x; \Theta_k)]^2<br>  \end{align} \qquad(n.ml.1.6.20)<br>  $$</p>
<p>这里\( r = y - f_{k-1}(x) \)，表示当前模型的拟合数据的残差（residual）。所以，对回归问题的提升树算法来说，只需要简单地拟合当前模型的残差。</p>
<blockquote>
<p>由于损失函数是平方损失，因此该方法属于<strong>L2Boosting</strong>的一种实现。</p>
</blockquote>
</blockquote>
</li>
<li><p><strong>回归问题提升树－算法描述</strong></p>
<p>  \(<br>  \{ \\\<br>  \quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\\<br>  \quad 输出：提升树f_K(x). \\\<br>  \quad 过程: \\\<br>  \qquad (1). 初始化模型f_0(x) = 0； \\\<br>  \qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\\<br>  \qquad\qquad (a). 计算残差：r_{ki} = y^{(i)} - f_{k-1}(x^{(i)}), \quad i=1,2,\cdots,M \\\<br>  \qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到T(x;\Theta_k) \\\<br>  \qquad\qquad (c). 更新f_k(x) = f_{k-1}(x) + T(x; \Theta_k) \\\<br>  \qquad\; (3). 得到回归提升树 \\\<br>  \qquad\qquad f_K(x) = \sum_{k=1}^{K} T(x; \Theta_k) \\\<br>   \}<br>  \)</p>
</li>
</ul>
<h1 id="4.Gradient-Boosting">Gradient Boosting</h1>

<p>提升树方法是利用加法模型与前向分布算法实现整个优化学习过程。Adaboost的指数损失和回归提升树的平方损失，在前向分布中的每一步都比较简单。但对于一般损失函数而言（比如绝对损失），每一个优化并不容易。</p>
<p>针对这一问题。Freidman提出了梯度提升（gradient boosting）算法。该算法思想：</p>
<p><strong>利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，拟合一个回归树。</strong></p>
<p>损失函数的负梯度为：</p>
<p>$$<br>-\left[ \frac{\partial L(y^{(i)}, f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \approx r_{m,i}<br>$$</p>
<ul>
<li><p>Gradient Boosting－算法描述</p>
<p>  \(<br>  \{ \\\<br>  \quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\\<br>  \qquad\quad\; 损失函数L(y, f(x)); \\\<br>  \quad 输出：提升树\hat{f}(x). \\\<br>  \quad 过程: \\\<br>  \qquad (1). 初始化模型 \\\<br>  \qquad\qquad\qquad f_0(x) = \arg \min_c \sum_{i=1}^{M} L(y^{(i)}, c)； \\\<br>  \qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\\<br>  \qquad\qquad (a). 计算残差：对于i=1,2,\cdots,M \\\<br>  \qquad\qquad\qquad\qquad r_{ki} = -\left[ \frac{\partial L(y^{(i)}, \; f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \\\<br>  \qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到第k颗树的叶结点区域R_{kj}，\quad j=1,2,\cdots,J \\\<br>  \qquad\qquad (c). 对j=1,2,\cdots,J, 计算：\\\<br>  \qquad\qquad\qquad\qquad c_{kj} = \arg \min_c \sum_{x^{(i)} \in R_{kj}} L(y^{(i)}, \; f_{k-1}(x^{(i)}) + c)\\\<br>  \qquad\qquad (d). 更新模型：\\\<br>  \qquad\qquad\qquad\qquad    f_k(x) = f_{k-1}(x) + \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\\<br>  \qquad\; (3). 得到回归提升树 \\\<br>  \qquad\qquad\qquad \hat{f}(x) = f_K(x) = \sum_{k=1}^{K} \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\\<br>   \}<br>  \)</p>
<blockquote>
<p>算法解释：</p>
<ol>
<li>第（1）步初始化，估计使损失函数极小化的常数值（是一个只有根结点的树）；</li>
<li>第(2)(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。(对于平方损失函数，他就是残差；对于一般损失函数，它就是残差的近似值)</li>
<li>第(2)(b)步估计回归树的结点区域，以拟合残差的近似值；</li>
<li>第(2)(c)步利用线性搜索估计叶结点区域的值，使损失函数极小化；</li>
<li>第(2)(d)步更新回归树。</li>
</ol>
</blockquote>
</li>
</ul>
<p><br></p>
<h3 id="Boosting利器"><strong>Boosting利器</strong></h3><hr>
<p>Boosting类方法在不仅在二分类、多分类上有着出色的表现，在预估问题上依然出类拔萃。</p>
<blockquote>
<p>2012年KDD cup竞赛和Kaggle上的许多数据挖掘竞赛，Boosting类方法帮助参赛者取得好成绩提供了强有力的支持。</p>
</blockquote>
<p>“工欲善其事，必先利其器”。Github上和机器学习工具包（如sklearn）中有很多优秀的开源boosting实现。在这里重点介绍两个Boosting开源工具。</p>
<ul>
<li><p><a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">XGBoost</a> </p>
<p>  说到Boosting开源工具，首推<a href="http://www.weibo.com/u/2397265244?is_all=1" target="_blank" rel="external">@陈天奇怪</a>同学的<a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">XGBoost</a> (eXtreme Gradient Boosting)。上面说的各种竞赛很多优秀的战果都是用@陈天奇同学的神器。</p>
<p>  从名称可以看出，该版本侧重于Gradient Boosting方面，提供了Graident Boosting算法的框架，给出了GBDT，GBRT，GBM具体实现。提供了多语言接口（C++, Python, Java, R等），供大家方便使用。</p>
<p>  更令人振奋的一件事情是，最新版本的xgboost是基于分布式通信协议rabit开发的，可部署在分布式资源调度系统上（如yarn，s3等）。我们完全可以利用最新版的xgboost在分布式环境下解决分类、预估等场景问题。</p>
<blockquote>
<p>注：</p>
<ol>
<li>XGBoost是<a href="https://github.com/dmlc" target="_blank" rel="external">DMLC</a>（即分布式机器学习社区）下面的一个子项目，由@陈天奇怪，@李沐等机器学习大神发起。</li>
<li><a href="https://github.com/dmlc/rabit" target="_blank" rel="external">Rabit</a>是一个为分布式机器学习提供Allreduce和Broadcast编程范式和容错功能的开源库（也是@陈天奇同学的又一神器）。它主要是解决MPI系统机器之间无容错功能的问题，并且主要针对Allreduce和Broadcast接口提供可容错功能。</li>
</ol>
<p>题外话：</p>
<p>2014年第一次用XGBoost时，用于Kaggle移动CTR预估竞赛。印象比较深刻的是，同样的训练数据（特征工程后），分别用XGBoost中的GBDT和MLLib中的LR模型（LBFGS优化），在验证集上的表现前者比后者好很多（logloss和auc都是如此）。线上提交结果时，名次直接杀进前100名，当时给我留下了非常好的印象。后来，因为项目原因，没有过多的使用xgboost，但一直关注着。</p>
<p>目前，我个人更加关注的是：<strong>基于Rabit开发/封装一些在工业界真正能发挥重要价值的（分布式）机器学习工具</strong>，用于解决超大规模任务的学习问题。这里面会涉及到分布式环境下的编程范式，可以高效地在分布式环境下工作的优化算法（<code>admm</code>等）和模型(<code>loss + regularization term</code>)等。</p>
<p>关于大数据下的机器学习发展，个人更看好将<strong>计算引擎模块</strong>与<strong>资源调度模块</strong>独立开来，专注做各自的事情。计算引擎可以在任意的分布式资源调度系统上工作，实现真正的可插拔，是一个不错的方向。</p>
<p>与之观念对应的是，spark上集成的graphx和mllib中的许多计算模块，虽然使用起来很简便（几十行核心代码就能搭建一个学习任务的pipeline）。但可以想象的是，随着spark的进一步发展，该分布式计算平台会变的非常重，功能也会越来越多。离专注、专一和极致的解决某类问题越来越远，对每一类问题给出的解决方案并不会特别好。</p>
</blockquote>
</li>
<li><p><a href="http://www.multiboost.org/" target="_blank" rel="external">MultiBoost</a></p>
<p>  <a href="http://www.multiboost.org/" target="_blank" rel="external">MultiBoost</a>工具的侧重点不同于<a href="https://github.com/dmlc/xgboost.git" target="_blank" rel="external">XGBoost</a>，是Adaboost算法的多分类版本实现，更偏向于解决<strong>multi-class / multi-label / multi-task的分类问题</strong>。</p>
<p>  我们曾经基于该工具训练了用于<strong>用户兴趣画像</strong>的多标签（multi-label）分类模型，其分类效果（Precision / Recall作为指标）要比Naive Bayes好。</p>
<p>  <a href="http://www.multiboost.org/" target="_blank" rel="external">MultiBoost</a>是用C++实现的。值得一提的是，由我们组的算法大神和男神<a href="http://weibo.com/baigang111?is_all=1" target="_blank" rel="external">@BaiGang</a>实现了MulitBoost的spark版本（Scala语言），详见<a href="https://github.com/BaiGang/spark_multiboost" target="_blank" rel="external">Github: Spark_MultiBoost</a></p>
</li>
</ul>
<hr>
<ul>
<li>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/AdaBoost/">AdaBoost</a><a href="/tags/BGDT/">BGDT</a><a href="/tags/Boosting/">Boosting</a><a href="/tags/Gradient-Boosting/">Gradient Boosting</a><a href="/tags/XGBoost/">XGBoost</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter5-bayes-based-family/" title="第05章：深入浅出ML之Bayes-Based家族" itemprop="url">第05章：深入浅出ML之Bayes-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-11-27T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-11-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-11-15</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>判别模型与生成模型</li>
<li>高斯判别分析</li>
<li>朴素贝叶斯算法</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<p>本章虽然是谈贝叶斯，但是先不从贝叶斯开始说起。先介绍有监督学习中两大类经典的模型：判别模型与生成模型。</p>
<p><br></p>
<h3 id="判别模型与生成模型">判别模型与生成模型</h3><hr>
<p>实际上，机器学习的有监督学习的所有模型大体上可以划分为两大类：一类是判别模型，另一类是生成模型。</p>
<ul>
<li><p>判别模型</p>
<p>  判别模型主要是根据特征值求结果的概率，形式化表示为\(P(y|x;w)\)（比如LR模型）。在参数\(w\)确定的情况下，求解<strong>条件概率\(P(y|x)\)</strong>。</p>
<blockquote>
<p>举例：</p>
<p>比如要确定一只羊是山羊还是绵羊，用判别模型的方法是：先从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率和绵羊的概率（即条件概率\(P(y|x)\)）。 </p>
</blockquote>
</li>
</ul>
<table>
<thead>
<tr>
<th>常见的判别模型：</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性回归、对数回归、线性判别分析、SVM、Boosting、CRF、神经网络等</td>
</tr>
</tbody>
</table>
<ul>
<li><p>生成模型</p>
<p>  延续判别模型中的例子，换一种思路考虑：</p>
<blockquote>
<p>我们可以根据山羊的特征，首先学习出一个山羊模型；然后根据绵羊的特征学习出一个绵羊模型。然后从某一只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是哪个。</p>
</blockquote>
<p>  上面描述形式化表示为\(P(x|y)\)（包括\(P(y)\)），\(y\)是模型结果，\(x\)是特征（向量）。利用贝叶斯公式可以发现两个模型的关联性：</p>
<p>  $$<br>  P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x|y) \cdot P(y)}{P(x)} \qquad(ml.1.5.1)<br>  $$</p>
<blockquote>
<p>在生成模型里面，由于我们关系的是\(y\)的离散值结果中哪个概率大（比如山羊概率和绵羊概率哪个大），而不是关心具体的概率，因此上式可改写为：</p>
<p>$$<br>  \begin{align}<br>  \arg \max_{y} P(y|x) &amp; = \arg \max_{y} \frac{P(x|y) P(y)}{P(x)} \quad(1) \\<br>  &amp; = \arg \max_{y} P(x|y) P(y)    \;\quad(2)<br>  \end{align} \qquad(n.ml.1.5.1)<br>  $$</p>
<p>其中，\(P(y)\)称为先验概率，\(P(y|x)\)是后验概率，\(P(x|y)\)为似然函数。公式\((1)\)等价于\((2)\)是因为对于离散值\(y\)来说，\(P(x)\)都是一样的。</p>
</blockquote>
</li>
</ul>
<table>
<thead>
<tr>
<th>常见的生成模型：</th>
</tr>
</thead>
<tbody>
<tr>
<td>隐马尔可夫模型、朴素贝叶斯模型、高斯混合模型、主题模型（LDA等）、受限波尔斯曼机等</td>
</tr>
</tbody>
</table>
<ul>
<li>联系与区别<ul>
<li>由于\(P(x,y) = P(x|y) \cdot P(y)\)，判别模型求的是条件概率，生成模型求的是联合概率。</li>
</ul>
</li>
</ul>
<ol>
<li>概率论的乘法法则</li>
<li>流派之争：频率学派与贝叶斯学派</li>
<li>先验概率与后验概率</li>
</ol>
<p><br></p>
<h3 id="高斯判别分析">高斯判别分析</h3><hr>
<p>这里在介绍朴素贝叶斯算法之前，先介绍一下高斯判别分析模型 (Gaussian Discriminant Analysis)。</p>
<ul>
<li><p>多变量正态分布</p>
<p>  多变量正态分布（又称多值正态分布）描述的是\(n\)维随机变量的分布情况。单变量正态分布中的期望\(\mu\)在这里变成了向量，\(\sigma\)也变成了矩阵\(\Sigma\)，记作\(N(\mu,\Sigma)\)。</p>
<p>  假设有\(n\)个随机变量\(X_1, X_2, \cdots, X_n\)。\(\mu\)的第\(i\)个分量是\(E(X_i)\)，其中\(\Sigma_{ii}=Var(X_i)\)，\(\Sigma_{ij} = Cov(X_i, X_j)\)。多变量概率分布对应的概率密度函数如下：</p>
<p>  $$<br>  P(x; \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot |\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \right) \qquad(ml.1.5.2)<br>  $$ </p>
<p>  其中\(|\Sigma|\)是\(\Sigma\)的行列式，\(\Sigma\)是协方差矩阵，并且是对称半正定的。</p>
<p>  （二维空间示意图）</p>
<blockquote>
<p>注：\(\mu\)决定中心位置，\(\Sigma\)决定投影椭圆的朝向和大小。</p>
</blockquote>
</li>
<li><p>高斯判别分析模型</p>
<p>  如果输入特征\(x\)是连续性随机变量，那么可以使用高斯判别分析模型来确定\(P(x|y)\)。模型如下：</p>
<p>  $$<br>  \begin{align}<br>  y &amp; \thicksim Bernoulli(\phi) \\<br>  x|y=0 &amp; \thicksim \mathcal{N}(\mu_0, \Sigma) \\<br>  x|y=1 &amp; \thicksim \mathcal{N}(\mu_1, \Sigma)<br>  \end{align}  \qquad\qquad(ml.1.5.3)<br>  $$</p>
<p>  其中，输出结果\(y\)服从伯努利分布（\(\phi\)为参数）；在给定的模型下，特征（向量）符合多变量高斯分布。</p>
<blockquote>
<p>回到上面山羊与绵羊的例子，在山羊模型下，它的体征如胡须长度、角大小、毛长度等连续型变量符合高斯分布，他们组成的特征向量符合多变量高斯分布。</p>
</blockquote>
<p>  下面给出该模型中涉及到的概率密度函数：</p>
<p>  $$<br>  \begin{align}<br>  P(y) &amp; = \phi^{y} (1-\phi)^{1-y} \\<br>  P(x|y=0) &amp; = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot |\Sigma|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (x-\mu_0)^T \Sigma^{-1} (x-\mu_0) \right) \\<br>  P(x|y=1) &amp; = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot |\Sigma|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (x-\mu_1)^T \Sigma^{-1} (x-\mu_1) \right) \\<br>  \end{align}  \quad(ml.1.5.4)<br>  $$</p>
<p>  最大似然估计（对数似然）公式：</p>
<p>  $$<br>  \begin{align}<br>  \ell(\phi, \mu_0, \mu_1, \Sigma) &amp; = \log \prod_{i=1}^{m} P(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) \\<br>  &amp; = \log \prod_{i=1}^{m} P(x^{(i)}|y^{(i)};\mu_0, \mu_1, \Sigma) \cdot P(y^{(i)}; \phi)<br>  \end{align} \qquad(ml.1.5.5)<br>  $$</p>
<blockquote>
<p>\(m\)表示样本数，这里有两个\(\mu\)，表示在不同的结果模型下，特征均值不同。但我们假设协方差相同。反映在图形上表现为：不同模型中心位置不同，但形状相同。这样就可以用直线来进行分隔判别。</p>
</blockquote>
<p>  将\((ml.1.5.4)\)带入\((ml.1.5.5)\)，求导后，得到参数估计公式：</p>
<p>  $$<br>  \begin{align}<br>  \phi &amp; = \frac{1}{m} \sum_{i=1}^{m} 1\{y^{(i)}=1\} \qquad\qquad(1)\\<br>  \mu_0 &amp; = \frac{\sum_{i=1}^{m} 1\{y^{(i)}=0\}x^{(i)}} {\sum_{i=1}^{m} 1\{y^{(i)}=0\}} \quad\qquad(2) \\<br>  \mu_1 &amp; = \frac{\sum_{i=1}^{m} 1\{y^{(i)}=1\}x^{(i)}} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}} \quad\qquad(3) \\<br>  \Sigma &amp; = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - \mu_{y^{(i)}}) (x^{(i)} - \mu_{y^{(i)}})^T \quad(4)<br>  \end{align}  \qquad(ml.1.5.6)<br>  $$</p>
<p>  其中，\(\phi\)是训练样本中结果\(y=1\)的样本占比；\(\mu_0\)和\(\mu_1\)分别表示\(y=0\)和\(y=1\)的样本中特征均值；\(\Sigma\)是样本特征方差均值。</p>
<p>  （高斯判别分析模型 示意图）</p>
</li>
<li><p>高斯判别分析（GDA）与LR的关系</p>
<blockquote>
<p>如果将GDA用条件概率方式表示的话，可表示为：</p>
<p>  $$<br>  P(y=1|x; \phi, \mu_0, \mu_1, \Sigma) \qquad\quad(n.ml.1.5.2)<br>  $$</p>
<p>\(y\)是\(x\)的函数，其中\(\phi, \mu_0, \mu_1, \Sigma\)都是参数。我们进一步推导，可得：</p>
<p>  $$<br>  P(y=1|x; \phi, \mu_0, \mu_1, \Sigma) = \frac{1}{1+\exp(-w^Tx)} \qquad(n.ml.1.5.3)<br>  $$</p>
<p>这里\(w\)是\(\phi, \mu_0, \mu_1, \Sigma\)的函数。恰好公式\((n.ml.1.5.3)\)是LR模型的表达式。</p>
</blockquote>
<p>  总结如下：<strong>如果P(x|y)符合多变量高斯分布，那么P(y|x)符合Logisti回归模型</strong>。反之不成立，因为GDA有着更强的假设条件和约束。</p>
<blockquote>
<p>如果我们认定训练数据满足多变量高斯分布，那么GDA能够在训练集上是最好的模型。然而，我们往往事先并不知道训练数据满足什么样的分布，不能做很强的假设。<strong>Logistic回归的条件假设要弱于GDA，因此更多时候采用LR方法。</strong></p>
<p>例如，训练数据满足泊松分布，\(x|y=0 \thicksim Poisson(\lambda_0); \; x|y=1 \thicksim Poisson(\lambda_1)\)。那么可以得到\(P(y|x)\)也是Logistic回归模型。（此时如果采用GDA，效果就会比较差，因此训练样本的特征分布是泊松分布，非多变量高斯分布。）</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h3 id="朴素贝叶斯">朴素贝叶斯</h3><hr>
<p><br></p>
<h4 id="朴素贝叶斯模型">朴素贝叶斯模型</h4><hr>
<p>在高斯判别分析中，我们要求特征向量\(x\)是连续型的实数向量。如果\(x\)是离散型向量的话，可以考虑采用朴素贝叶斯的分类方法。</p>
<ul>
<li><p>多项式分布（Multinomial Distribution）</p>
<blockquote>
<p>这里以经典的邮件分类（文本分类的一种应用）为例，假如邮件要分为两类：垃圾邮件和正常邮件。采用最简单的特征描述方法，首先要找一个词库，然后将每封邮件中的内容表示成一个向量（根据词库进行过滤后），向量中每一维都是词库中的一个词的0/1值（1表示该词在词库中出现，0表示未出现）。</p>
<p>比如一封邮件中出现了“的”和“打折”，没有出现“通知”，“提交”，“预约”等，那么形式化表示为：</p>
<p>  $$<br>  \phi(x) =<br>  \begin{equation}<br>  \left[</p>
<pre><code><span class="command">\begin</span><span class="special">{</span>array<span class="special">}</span><span class="special">{</span>cc<span class="special">}</span>
    1 <span class="command">\\</span><span class="command">\
</span>    0 <span class="command">\\</span><span class="command">\
</span>    <span class="command">\vdots</span> <span class="command">\\</span><span class="command">\
</span>    1 <span class="command">\\</span><span class="command">\
</span>    <span class="command">\vdots</span> <span class="command">\\</span><span class="command">\
</span>    0 <span class="command">\\</span><span class="command">\
</span><span class="command">\end</span><span class="special">{</span>array<span class="special">}</span>
</code></pre><p>  \right] \; \leftarrow \;<br>  \left[</p>
<pre><code><span class="command">\begin</span><span class="special">{</span>array<span class="special">}</span><span class="special">{</span>cc<span class="special">}</span>
    的 <span class="command">\\</span><span class="command">\
</span>    提交 <span class="command">\\</span><span class="command">\
</span>    <span class="command">\vdots</span> <span class="command">\\</span><span class="command">\
</span>    打折 <span class="command">\\</span><span class="command">\
</span>    <span class="command">\vdots</span> <span class="command">\\</span><span class="command">\
</span>    通知 <span class="command">\\</span><span class="command">\
</span><span class="command">\end</span><span class="special">{</span>array<span class="special">}</span>
</code></pre><p>  \right]<br>  \end{equation} \qquad(exp.ml.1.5.1)<br>  $$</p>
<p>假设词库中有10000个词，那么\(x\)是10000维的。</p>
</blockquote>
<p>  此时如果要建立多项式分布模型，对该问题来说，就是把每封邮件看作一次随机试验，那么结果的可能性就有\(x^{10000}\)种。(意味着参数\(p_i\)有\(x^{10000}\)个，参数太多，不可能直接用该思路建模。)</p>
<blockquote>
<p>多项式分布：</p>
<p>假设某随机试验有\(k\)个可能结果\(X_1, X_2, \cdots, X_k\)，它们的概率分布分别是\(p_1, p_2, \cdots, p_k\)。在\(N\)次采样的总结果中，\(X_1, X_2, \cdots, X_k\)出现的次数分别为\(n_1, n_2, \cdots, n_k\)（\(N = \sum_{i=1}^{k} n_i\)）。那么这次事件出现的概率\(P\)可表示为（公式中\(x_i\)代表出现\(n_i\)次）：</p>
<p>  $$<br>  P(X_1 = n_1, \cdots, X_k = n_k) =<br>  \begin{cases}<br>  \frac{N!}{n_1! n_2! \cdots n_k!} p_1^{n_1} p_2^{n_2} \cdots p_k^{n_k} &amp;     \; 当N = \sum_{i=1}^{k} n_i 时，\\<br>  \quad\qquad 0 &amp; \; 其它<br>  \end{cases}    \;(n.ml.1.5.4)<br>  $$</p>
</blockquote>
</li>
<li><p>朴素贝叶斯假设</p>
<p>  我们先换一种思路，我们要求的是\(P(y|x)\)，根据生成模型定义，可以求\(P(x|y)\)和\(P(y)\)。假设\(x\)中的特征都是独立的，这个作为朴素贝叶斯假设。</p>
<blockquote>
<p>特征之间相互条件独立是朴素贝叶斯模型的前提条件。如果一封邮件是垃圾邮件\(y=1\)，且这封邮件出现词”buy”与这封邮件是否出现”price”无关，那么”buy”和”price”之间是<strong>条件独立</strong>的。</p>
<p>在给定\(Z\)的情况下，\(X\)和\(Y\)条件独立，形式化表示为：</p>
<p>  $$<br>  P(X|Z) = P(X|Y,Z)    \qquad (n.ml.1.5.5)<br>  $$</p>
<p>也可以表示为：</p>
<p>  $$<br>  P(X,Y|Z) = P(X|Z) \cdot P(Y|Z)    \qquad(n.ml.1.5.6)<br>  $$</p>
</blockquote>
<p>  那么对于一条样本来说，有下式成立：</p>
<p>  $$<br>  \begin{align}<br>  P(x_1, \cdots, x_{10000}) = &amp; P(x_1|y) P(x_2|y, x_1) P(y_3|y, x_1, x_2) \cdots P(x_{10000}|y, x_1, x_2, \cdots, x_{9999}) \\<br>  = &amp; P(x_1|y) P(x_2|y) \cdots P(x_{10000}|y) \\<br>  = &amp; \sum_{i=1}^{n} P(x_i|y)    \qquad\qquad\qquad(ml.1.5.7)<br>  \end{align}<br>  $$</p>
<p>  公式\((ml.1.5.7)\)与NLP中的n-gram语言模型非常类似，这里相当于1-gram。同时也可以看出，朴素贝叶斯假设是约束性很强的假设。通常来说，”buy”与”price”是有关系的，而在NB这里，假设条件独立。</p>
<blockquote>
<p>注：条件独立与独立是两个不同的概念。</p>
</blockquote>
</li>
<li><p><strong>朴素贝叶斯模型（Naive Bayes Model）</strong></p>
<p>  建立朴素贝叶斯的形式化模型：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \phi_{j|y=1} = P(x_j=1 | y=1) \quad(1)\\<br>  &amp; \phi_{j|y=0} = P(x_j=1 | y=0) \quad(2)\\<br>  &amp; \phi_y = P(y=1) \qquad\qquad\;\;(3)<br>  \end{align}    \qquad (ml.1.5.8)<br>  $$</p>
<p>  而我们想要的是模型在所有训练样本上的概率累积最大，即最大似然估计：</p>
<p>  $$<br>  \mathcal{L}(\phi_y, \phi_{i|y=0}, \phi_{i|y=1}) = \prod_{i=1}^{m} P(x^{(i)}, y^{(i)})     \qquad(ml.1.5.9)<br>  $$</p>
<blockquote>
<p>注意：这里是联合概率分布积最大（非LR中的条件概率积），说明朴素贝叶斯是省城模型。</p>
</blockquote>
<p>  公式\((ml.1.5.9)\)求解，可得：</p>
<p>  $$<br>  \begin{align}<br>  \phi_{j|y=1} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=1 \}} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}} \quad(1)\\<br>  \phi_{j|y=0} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=0 \}} {\sum_{i=1}^{m} 1\{y^{(i)}=0\}} \quad(2)\\<br>  \phi_y &amp; = \frac{\sum_{i=1}^{m} 1\{y^{(i)}=0\}} {m} \;\;\,\qquad\qquad(3)<br>  \end{align}    \qquad(ml.1.5.10)<br>  $$</p>
<blockquote>
<p>(3)式表示y=1的样本数在全部样本中的占比。前两个分别表示在\(y=1\)和\(y=0\)的样本中，特征\(x_j=1\)的比例。</p>
</blockquote>
<p>  然而我们最终要求的是：</p>
<p>  $$<br>  \begin{align}<br>  P(y=1|x) &amp; = \frac{P(x|y=1) P(y=1)}{P(x)} \\<br>  &amp; = \frac{\left(\prod_{j=1}^{n}P(x_j|y=1)\right) P(y=1)} {\left(\prod_{j=1}^{n}P(x_j|y=1)\right) P(y=1) + \left(\prod_{j=1}^{n}P(x_j|y=0)\right) P(y=0)}<br>  \end{align} \qquad(ml.1.5.11)<br>  $$</p>
</li>
</ul>
<pre><code>&gt; 实际上，只需要求分子即可，分母对于<span class="command">\\</span>(y=1<span class="command">\\</span>)和<span class="command">\\</span>(y=0<span class="command">\\</span>)都一样。
&gt;
&gt; 朴素贝叶斯模型可以扩展到<span class="command">\\</span>(x<span class="command">\\</span>)和<span class="command">\\</span>(y<span class="command">\\</span>)都有多个值的情况。对于特征是连续的情况，可采用分段的方法将连续值转化为离散值。这涉及到了特征离散化的一些技术，比如等分、等差、信息增益、基尼指数等。（该部分在第11章：特征工程方法中会详细介绍。）
</code></pre><p><br></p>
<h4 id="拉普拉斯平滑">拉普拉斯平滑</h4><hr>
<p>朴素贝叶斯方法有一个致命的缺点就是对数据的稀疏问题过于敏感。</p>
<blockquote>
<p>比如，5.2.1节中介绍的邮件分类，加入现在又来了一封邮件，邮件中出现”NIPS call for papers”。其中NIPS在现有的词典库中不存在（10000个词）。现在我们使用更大的词典库来分类（词的数目由10000变为50000），假设NIPS这个词在词典中的位置是50000。而这个词在训练数据中未曾出现过，那么此时算条件概率的结果是：</p>
<p>$$<br>\begin{align}<br>\phi_{50000|y=1} = \frac {\sum_{i=1}^{m} 1\{x_{50000}^{(i)}=1 \wedge y^{(i)}=1\}} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}} = 0 \\<br>\phi_{50000|y=0} = \frac {\sum_{i=1}^{m} 1\{x_{50000}^{(i)}=1 \wedge y^{(i)}=0\}} {\sum_{i=1}^{m} 1\{y^{(i)}=0\}} = 0<br>\end{align}  \qquad(exp.ml.1.5.2)<br>$$</p>
<p>由于NIPS在以前的所有邮件中，故而其对应的条件概率为0。带入公式\((ml.1.5.11)\)，得到最终的条件概率也为0。</p>
</blockquote>
<p>上述导致条件概率为0的主要原因是朴素贝叶斯的假设－特征之间条件独立，而模型又实用相乘的方式得到结果。为解决此问题，也为训练样本中未出现的特征值，赋予一个较小的值（非0）。具体方法如下：</p>
<blockquote>
<p>假设离散随机变量\(x\)有\(\{1,2,\cdots, k\}\)个值，这里用\(\phi_i=P(x=i)\)来表示每个值的概率。在\(m\)个训练样本中，\(x\)的观察值表示为\(\{x^{(1)}, x^{(2)}, \cdots, x^{(k)} \}\)。其中每个观察值对应k个值中的一个，那么根据之前的方法可以得到：</p>
<p>$$<br>\phi_j = \frac{\sum_{i=1}^{m} 1\{x^{(i)}=j\}}{m}  \qquad(exp.ml.1.5.3)<br>$$</p>
<p>即\(x=j\)出现的比例。</p>
</blockquote>
<p>拉普拉斯平滑法(Laplace Smooth)将每个\(k\)值出现次数事先都加1，通俗地讲就是假设他们都出现过一次，那么修改后的表达式为：</p>
<p>$$<br>\phi_j = \frac{\sum_{i=1}^{m} 1\{x^{(i)}=j\}+1} {m+k}  \qquad(ml.1.5.12)<br>$$</p>
<blockquote>
<p>每个\(x=j\)的分子都加1，分母加k。有\(\sum_{j=1}^{k}=1\)成立。那么在邮件分类问题中，修改后的公式为：</p>
<p>$$<br>\begin{align}<br>\phi_{j|y=1} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=1 \} + 1} {\sum_{i=1}^{m} 1\{y^{(i)}=1\}+2} \quad(1)\\<br>\phi_{j|y=0} &amp; = \frac{\sum_{i=1}^{m} 1\{x^{(i)}_j = 1 \wedge y^{(i)}=0 \} + 1} {\sum_{i=1}^{m} 1\{y^{(i)}=0\} + 2} \quad(2)<br>\end{align} \qquad(n.ml.1.5.7)<br>$$</p>
</blockquote>
<p><br></p>
<h4 id="NB示例：文本分类">NB示例：文本分类</h4><hr>
<p>上面介绍的邮件分类其实就是一个典型的文本分类的例子，用于该文本分类的朴素贝叶斯模型又称作多变量伯努利事件模型。</p>
<blockquote>
<p>邮件生成过程描述：</p>
<p>（第1步）首先随机选定了邮件的类型（垃圾邮件与正常邮件，即\(P(y)\)），（第2步）然后观察词库中的每一个词，随机决定是否要出现在邮件中，出现则标记为1，否则标记为0。然后将出现的词组成一封邮件。决定一个词是否出现时根据概率\(P(x_i|y)\)，那么一封邮件的概率可表示为：</p>
<p>$$<br>一封邮件的生成概率=P(x,y) = P(y) \cdot \prod_{i=1}^{n} P(x_i|y) \qquad(exp.ml.1.5.3)<br>$$</p>
<p>注意：这里的\(n\)是词典中词的数目；\(x_i\)表示一个词是否出现，只有两个值0和1，概率之和为1；\(x\)向量是长度为\(n\)的0/1值。</p>
</blockquote>
<p>现在我么换一种思路，不先从词典入手，而是选择从邮件入手。让((i\)表示邮件中的第\(i\)个词，\(x_i\)表示这个词在词典中的位置，那么\(x_i\)的取值范围是\(\{1,2,\cdots,|V|\}\)（\(|V|\)是字典中词的数目）。这样，一封邮件可以表成\(n\)维向量\((x_1, x_2, \cdots, x_n)\)（\(n\)为邮件中词的个数，可以变化，因为每封邮件长度可不同）。</p>
<blockquote>
<p>这就相当于重复投掷\(|V|\)面的骰子，将观察值纪录下来就形成了一封邮件。当然每个面的概率服从\(P(x_i|y)\)，而且每次实验条件独立，这样我们得到的邮件概率为\(P(y) \prod_{i=1}^{n} P(x_i|y)\)。</p>
<p>注意：这里的公式虽然与共公式\((exp.ml.1.5.3)\)相同，但符号意义完全不同。本式中的\(n\)表示邮件中词的数目；\(x_i\)表示\(|V|\)中的一个值；\(x\)向量中元素是词典中的位置。</p>
</blockquote>
<p>形式化表示如下：</p>
<p>\(m\)个训练样本：\(\{(x^{(i)}, y^{(i)}); i=1,\cdots,m\}\)；第\(i\)条样本表示为：\(x^{(i)} = (x_1^{(i)}, x_2^{(i)}, \cdots, x_{n_i}^{(i)})\)，第\(i\)个样本有\(n_i\)个词。</p>
<p>我们仍然按照朴素贝叶斯的方法求最大似然估计，似然函数表示为：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\mathcal{L} (\phi, \phi_{i|y=0}, \phi_{i|y=1}) &amp; = \prod_{i=1}^{m} P(x^{(i)}, y^{(i)}) \\<br>&amp; = \prod_{i=1}^{m} \left(\prod_{j=1}^{n_i} P(x_j^{(i)}|y^{(i)}; \phi_{i|y=0}, \phi_{i|y=1}) \right) \cdot P(y^{(i)}; \phi_y)<br>\end{align} \qquad(n.ml.1.5.8)<br>$$</p>
</blockquote>
<p>求解结果为：</p>
<p>$$<br>\begin{align}<br>\phi_{k|y=1} &amp;= \frac{\sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)} = k \wedge y^{(i)}=1\}} {\sum_{i=1}^{m} 1\{y^{(i)} = 1\} \cdot n_i} \\<br>\phi_{k|y=0} &amp;= \frac{\sum_{i=1}^{m} \sum_{j=1}^{n_i} 1\{x_j^{(i)} = k \wedge y^{(i)}=0\}} {\sum_{i=1}^{m} 1\{y^{(i)} = 0\} \cdot n_i} \\<br>\phi_y &amp; = \frac { \sum_{i=1}^{m} 1\{y^{(i)} = 1\} } {m}<br>\end{align}<br>$$</p>
<p>与之前的分子相比，分母多了\(n_i\)，分子由\(0/1\)变成了\(k\).</p>
<p><br></p>
<h3 id="Next_…"><br><strong>Next …</strong></h3><hr>
<ul>
<li>贝叶斯网络</li>
<li>概率图模型</li>
</ul>
<p><br></p>
<h3 id="参考资料">参考资料</h3><hr>
<ul>
<li><a href="http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971903.html" target="_blank" rel="external">判别模型、生成模型与朴素贝叶斯方法</a></li>
<li>微博：<a href="http://weibo.com/p/1005051667005453/home?" target="_blank" rel="external">@JerryLead</a></li>
</ul>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Gaussian-Discriminant-Analysis/">Gaussian Discriminant Analysis</a><a href="/tags/Naive-Bayes/">Naive Bayes</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter4-kernel-based-family/" title="第04章：深入浅出ML之Kernel-Based家族" itemprop="url">第04章：深入浅出ML之Kernel-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-11-20T14:42:16.000Z" itemprop="datePublished"> 发表于 2015-11-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-11-12</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>对偶优化问题</li>
<li>核方法</li>
<li>支持向量机</li>
</ul>
<p><br></p>
<h3 id="写在前面"><strong>写在前面</strong></h3><hr>
<p>机器学习中有这样一个结论：低维空间线性不可分的数据通过<strong>非线性映射</strong>到高维特征空间则可能线性可分。极端地，假设数据集中有\(m\)个样本，一定可以在\(m-1\)维空间中线性可分。</p>
<blockquote>
<p>每一个样本在特征空间中对应一个点，试想一下，2个点可以用一维空间分开；3个点可以在2维空间（平面）上线性可分；… ；\(m\)个点，可以在\(m-1\)维空间中线性可分。</p>
</blockquote>
<p>高维空间可以让样本线性可分，这固然是优点。但是如果直接采用非线性映射技术在高维空间进行一个学习任务（如分类或回归），则需要确定<strong>非线性映射函数的形式和参数、高维特征空间维数等问题</strong>；而最大的问题在于在高维特征空间运算时可能存在的<strong>“<a href="http://baike.baidu.com/link?url=9DrIsp0paMuo49hhywnp2SXTvo1hKSrFI5nGkapFvgq8O1bI2PoUTEjVcKQw1pAePZOqFquqHVp_yUV8NPVZRK" target="_blank" rel="external">维数灾难</a>“</strong>。How to solve it?</p>
<p>本章要介绍的核函数方法可以有效地解决这类问题。这里先给出核函数工作的基本思路：</p>
<blockquote>
<p>设样本集\(X\)（\(X \in R^k\)）中有两条样本\(x和z\)，非线性函数\(\phi\)实现输入空间\(R^k\)到特征空间\(R^n\)的映射（即\(\phi(z) \in R^n\)），\( k &lt;&lt; n\)。核函数公式：</p>
<p>$$<br>k(x,z) = &lt;\phi(x), \phi(z)&gt;<br>$$</p>
<p>其中，<code>&lt; , &gt;</code>表示内积计算，\(k(x,z)\)为核函数。</p>
</blockquote>
<p>从上式可以看出，<strong>核函数将\(n\)维高维特征空间的内积计算转化为\(k\)维低维输入空间的核函数计算，巧妙地解决了在高维特征空间中计算可能出现的“维数灾难”等问题。从而为高维特征空间解决复杂的学习问题奠定了理论基础。</strong></p>
<p>本章的安排是这样。首先介绍一些对偶优化问题的基本形式，引出核函数并且为后面的SVM做铺垫；然后给出核函数的发展、常用的核函数等方法；最后详细介绍核函数在分类方法中的经典应用－支持向量机。</p>
<p><br></p>
<h3 id="对偶优化问题"><strong>对偶优化问题</strong></h3><hr>
<p>我们都知道，机器学习模型参数学习过程，大多要通过《最优化算法》中的一些方法来求解。其中，带约束条件的最优化问题（极值求解问题）一般是引入拉格朗日乘子法，构建拉格朗日对偶函数，通过求其对偶函数的解，从而得到原始问题的最优解。</p>
<p>其实不仅是带约束的最优化问题，任何一个最优化问题都伴随着一个”影子”最优化问题。其中一个称为原始问题，另一个称为对偶问题。</p>
<p>这里，先以回归模型的目标函数为例，给出其对偶形式；然后在简要描述带约束的最优化问题求解的一般思路，引出核函数。</p>
<p><br></p>
<h4 id="无约束优化问题的对偶形式"><strong>无约束优化问题的对偶形式</strong></h4><hr>
<p>在<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>中，我们给出了回归模型的目标函数：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\min_{w} \quad J(w) &amp;= \frac{1}{2} \sum_{i=0}^{m} \left(h_{w}(x^{(i)}) - y^{(i)} \right)^2 ＋ \frac{\lambda}{2} {\Vert w \Vert}_{2}^{2} \\<br>&amp;= \frac{1}{2} \sum_{i=0}^{m} \left(w^T \phi(x^{(i)}) - y^{(i)} \right)^2 ＋ \frac{\lambda}{2} w^T w<br>\end{align}  \qquad(n.ml.1.4.1)<br>$$</p>
</blockquote>
<p>回归模型通用形式：\(h_w(x) = w^T \phi(x)\)，其中\(\phi(x)\)是一个\(n \times 1\)的向量。对参数向量\(w\)求导，可得：</p>
<p>$$<br>\begin{align}<br>w = -\frac{1}{\lambda} \sum_{i=1}^{m} \underline{ \left(w^T \phi(x^{(i)}) - y^{(i)}\right) } \cdot \phi(x^{(i)}) = \sum_{i=1}^{m} \alpha^{(i)} \cdot \phi(x^{(i)}) = \Phi^T \alpha<br>\end{align} \qquad(ml.1.4.1)<br>$$</p>
<blockquote>
<p>其中<br>\(<br>\alpha^{(i)} = -\frac{1}{\lambda} \left( w^T \phi(x^{(i)}) - y^{(i)} \right) \qquad<br>\)</p>
<p>\(\alpha_{m \times 1} = (\alpha^{(1)}, \alpha^{(2)}, \cdots, \alpha^{(m)})^T\)是一个向量；\(\Phi_{m \times n} = (\phi(x^{(1)}), \phi(x^{(2)}), \cdots, \phi(x^{(m)}))\) 称为<a href="https://en.wikipedia.org/wiki/Design_matrix" target="_blank" rel="external">设计矩阵</a>。</p>
</blockquote>
<p>接下来，按照对偶表示的思路，重新定义目标函数。将\(w = \Phi^T \alpha\)带入公式\((n.ml.1.4.1)\)可得：</p>
<blockquote>
<p>$$<br>J(\alpha) = \frac{1}{2} a^T \Phi \Phi^T \Phi \Phi^T a - \frac{1}{2} a^T \Phi \Phi^T \cdot Y + \frac{1}{2} Y^T Y + \frac{\lambda}{2} a^T \Phi \Phi^T \alpha \qquad(n.ml.1.4.2)<br>$$</p>
</blockquote>
<p>其中真实结果集合\(Y = (y^{(1)}, y^{(2)}, \cdots, y^{(m)})^T\)。定义<a href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="external">格拉姆矩阵(Gramian Matrix)</a> \(K = \Phi \Phi^T\)一个\(m \times m\)的对称矩阵，每一个元素：</p>
<p>$$<br>K_{ij} = \phi(x^{(i)})^T \cdot \phi(x^{(j)}) = k(x^{(i)}, x^{(j)}) \qquad (ml.1.4.2)<br>$$</p>
<blockquote>
<p>格拉姆矩阵形式：</p>
<p>$$<br>K =<br>\begin{bmatrix}<br>(x^{(1)}, x^{(1)}) &amp; (x^{(1)}, x^{(2)}) &amp; \cdots &amp; (x^{(1)}, x^{(m)}) \\<br>(x^{(2)}, x^{(1)}) &amp; (x^{(2)}, x^{(2)}) &amp; \cdots &amp; (x^{(2)}, x^{(m)}) \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>(x^{(m)}, x^{(1)}) &amp; (x^{(m)}, x^{(2)}) &amp; \cdots &amp; (x^{(m)}, x^{(m)})<br>\end{bmatrix} \qquad(n.ml.1.4.3)<br>$$</p>
<p>格拉姆矩阵的一个重要应用是计算线性无关：一组向量线性无关 当且仅当 格拉姆矩阵行列式不等于0。<br>（如果\(\phi(x)\)是随机变量，得到的格拉姆矩阵是协方差矩阵。）</p>
</blockquote>
<p>公式\((ml.1.4.4)\)是一个表示任意两个样本之间关系的函数。后面会提到，每一个玉树值可通过定义核函数来计算。</p>
<p><br></p>
<h4 id="约束优化问题求解一般思路"><strong>约束优化问题求解一般思路</strong></h4><hr>
<ul>
<li><p><strong>拉格朗日乘子（Lagrange Multiplier）</strong></p>
<p>  拉格朗日乘子法通常用于求解带等式约束的极值问题，一个典型的最优化问题形式化表示如下：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \min_{w} \quad f(w) \\<br>  &amp; s.t. \; h_i(w) = 0, \quad i=1,\cdots,k<br>  \end{align}        \qquad\quad(ml.1.4.3)<br>  $$</p>
<p>  目标函数是\(f(w)\)，\(w \in R^n\) （\(n\)表示参数向量个数）；下面是等式约束，其中\(k\)为等式约束的个数。这类问题通常解法是引入拉格朗日乘子（又称算子），这里用\(\beta\)表示乘子，得到的拉格朗日公式为：</p>
<blockquote>
<p>$$<br>  \mathcal{L}(w,\beta) = f(w) + \sum_{i=1}^{k} \beta_i \cdot h_i(w) \qquad(n.ml.1.4.4)<br>  $$</p>
</blockquote>
<p>  然后分别对\(w\)和\(\beta\)求偏导，使得偏导数等于0，进而求解出\(w\)和\(\beta\)。</p>
<blockquote>
<p>为什么引入拉格朗日乘子就可以求解出极值？</p>
<p>主要原因是\(f(w)\)的切线方向（\(dw\)变化方向）受其它等式的约束，\(dw\)的变化方向与\(f(w)\)的梯度方向垂直时才能获得极值。并且<strong>在极值点处，\(f(w)\)的梯度与其它等式梯度的线性组合平行</strong>。因此它们之间存在线性关系。具体可参考《最优化算法》系列。</p>
</blockquote>
</li>
<li><p><strong>拉格朗日对偶（Lagrange Duality）</strong></p>
<p>  对于带有不等式约束的极值问题，形式化表示如下：</p>
<blockquote>
<p>$$<br>  \begin{align}<br>  &amp; \min_{w} \quad f(w) \\<br>  &amp; s.t. \; g_i(w) \leq 0, \quad i = 1, \cdots, l \\<br>  &amp; \quad\;\;\, h_i(w) = 0, \quad i =1, \cdots, k<br>  \end{align}        \qquad(n.ml.1.4.5)<br>  $$</p>
</blockquote>
<p>  定义拉格朗日公式：</p>
<blockquote>
<p>$$<br>  \mathcal{L}(w, \alpha, \beta) = f(w) + \sum_{i=1}^{l} \alpha_i g_i(w) + \sum_{i=1}^{k} \beta_i h_i(w)  \quad(n.ml.1.4.6)<br>  $$</p>
</blockquote>
<p>  公式\((n.ml.1.4.6)\)中的\(\alpha_i\)和\(\beta_i\)都是拉格朗日乘子。但如果按照该公式求解会出现以下问题。</p>
<blockquote>
<p>因为目标函数要求的是最小值，而约束条件\(h_i(w) \le 0\)，如果将\(\alpha_i\)调整为很大的正数，会使得最后的函数结果为负无穷（\(-\infty\)）。</p>
</blockquote>
<p>  因此，我们需要排除上述情况的发生。策略如下，定义函数：</p>
<blockquote>
<p>$$<br>  \theta_{\mathcal{P}}(w) = \max_{\alpha, \beta;\, \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)  \qquad\quad (n.ml.1.4.7)<br>  $$</p>
</blockquote>
<p>  公式\((n.ml.1.4.5)\)中的\(\mathcal{P}\)表示原问题，即primal。_假设\(g_i(w) &gt; 0\)或者\(h_i(w) \neq 0\)，那么我们总可以通过调整\(\alpha_i\)和\(\beta_i\)，使得\(\theta_{\mathcal{P}}(w)\)趋向正无穷。而只有当函数\(g\)和\(h\)满足约束条件时，\(\theta_{\mathcal{P}}(w)\)为\(f(w)\)。_ </p>
<blockquote>
<p>公式\((n.ml.1.4.7)\)精妙之处就在于\(\alpha_i \ge 0\)，而且是求极大值。因此公式\((n.ml.1.4.7)\)可以写为：</p>
<p>$$<br>  \theta_{\mathcal{P}}(w) =<br>  \begin{cases}<br>  f(w), \quad &amp; 如果w满足原问题约束; \\<br>  \; \infty, \quad &amp; otherwise.<br>  \end{cases}        \qquad(n.ml.1.4.8)<br>  $$</p>
<p>公式\((n.ml.1.4.7)\)和\((n.ml.1.4.8)\)是理解拉格朗日对偶的关键。</p>
</blockquote>
<p>  如此，我们原来要求解的\(\min_w f(w)\)可以转化为求解\(\min_w \theta_{\mathcal{P}}(w)\)了。即：</p>
<blockquote>
<p>$$<br>  \min_w f(w) = \min_w \theta_{\mathcal{P}}(w) = \min_{w} \max_{\alpha, \beta;\, \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)  \qquad (n.ml.1.4.9)<br>  $$</p>
</blockquote>
<p>  这里先用\(\mathcal{P}^{\ast}\)表示\(\min_w \theta_{\mathcal{P}}(w)\)。如果直接求解，又会面临如下问题：</p>
<blockquote>
<p>首先面对两个参数\(\alpha、\beta\)，并且参数\(\alpha_i\)也是一个不等式约束；然后再在\(w\)上求极小值。这个过程不容易做，可否有相对容易的解法呢？</p>
</blockquote>
<p>  我们换一个角度考虑该问题，令\(\theta_{\mathcal{D}}(\alpha, \beta) = \min_{w} \mathcal{L}(w, \alpha, \beta)\)。\(\mathcal{D}\)是对偶的意思，\(\theta_{\mathcal{D}}(\alpha, \beta)\)将问题转化为**先求拉格朗日关于\(w\)的最小值，将\(\alpha\)和\(\beta\)看作是固定值。然后再求\(\theta_{\mathcal{D}}(\alpha, \beta)\)的极大值**。即：</p>
<blockquote>
<p>$$<br>  \max_{\alpha, \beta;\, \alpha_i \geq 0} \theta_{\mathcal{D}}(\alpha, \beta) = \max_{\alpha, \beta;\, \alpha_i \geq 0} \min_{w} \mathcal{L}(w, \alpha, \beta)   \qquad (n.ml.1.4.10)<br>  $$</p>
</blockquote>
<p>  问题转化为原问题的对偶问题来求解。其实，相对于原问题来说只是更换了\(max\)和\(min\)的顺序，而一般更换顺序的结果是：\(max \; min \; f(x) \le min \; max \; f(x)\)。用\(\mathcal{D}^{\ast}\)表示对偶问题，与原问题\(\mathcal{P}^{\ast}\)关系如下：</p>
<p>  $$<br>  \mathcal{D}^{\ast} = \max_{\alpha,\beta; \; \alpha_i \ge 0} \min_{w} \mathcal{L}(w, \alpha, \beta) \le \min_{w} \max_{\alpha,\beta; \; \alpha_i \ge 0} \mathcal{L}(w, \alpha, \beta) = \mathcal{P}^{\ast} \quad (ml.1.4.4)<br>  $$</p>
<blockquote>
<p>即将_最小最大问题_转化为_最大最小问题。_ </p>
<p>(这部分可与<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/" target="_blank" rel="external">《第02章：深入浅出ML之Entropy-Based家族》</a>结合起来理解，更容易理解并能建立起优化问题、最大熵以及后面要介绍的SVM之间的公式关联。)</p>
</blockquote>
<p>  这里我们总结下拉格朗日对偶的精髓：</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><strong>将原有参数\(w\)的计算提前并消除\(w\)，使得优化函数变为拉格朗日乘子的单一参数优化问题。</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><br></p>
<h3 id="核方法"><strong>核方法</strong></h3><hr>
<p>在机器学习中，核方法被形式化为特征空间的向量内积。又被称为核技巧（kernel trick），或核置换（kernel substitution）。</p>
<p><br></p>
<h4 id="核函数介绍"><strong>核函数介绍</strong></h4><hr>
<p>早在机器学习学科成立之前，核函数相关理论、核函数技术以及核函数的有效性定理就已经存在。1964年Aizerman等人把势函数（potential function）相关研究引入到模式识别领域。中间过去很多年，直到1992年Boser等人在研究机器学习中最大间隔分类器中再次将核函数引入进来，产生了支持向量机技术。从这开始，核函数在机器学习的理论和应用得到了更多地关注和极大的兴趣。</p>
<p>例如，Scholkopf等人将核函数应用到主成分分析PCA中产生了核主元分析法（kernel PCA），Mika等人将核函数引入到Fisher判别中产生了核Fisher判别分析（kernel Fisher discriminant）等等。这些技术在机器学习、模式识别等不同领域中起到了很重要的作用。</p>
<blockquote>
<p>核函数有效性Mercer定理可追溯至1909年。</p>
</blockquote>
<p><br></p>
<h4 id="核函数引入"><strong>核函数引入</strong></h4><hr>
<p>Andrew Ng男神在《机器学习》课程第一讲的线性回归中提到例子，用回归模型拟合房屋面积（\(x\)表示）与价格（\(y\)表示）之间的关系。示例中的回归模型用\(y=w_0 + w_1 x\)方程拟合。假设从样本的分布中可以看到\(x\)与\(y\)符合3次曲线关系，那么我们希望使用\(x\)的3次多项式来逼近这些样本点。</p>
<p>首先要做的是将一维特征\(x\)映射至三维\((x,x^2,x^3)\)，然后再根据新特征与结果之间的关系模型。我们将这种特征变换称作<strong>特征映射（Feature Mapping）</strong>。映射函数这里用\(\phi(x)\)表示，该例中可表示为：</p>
<blockquote>
<p>$$<br>    \phi(x): \;\; x \longrightarrow<br>    \begin{bmatrix}<br>    x \\<br>    x^2 \\<br>    x^3 \\<br>    \end{bmatrix} \qquad\qquad(exp.ml.1.4.1)<br>$$</p>
</blockquote>
<p>我们希望将映射后的特征应用于后面要介绍的SVM分类中，而不直接用原始特征（Raw Feature）。</p>
<blockquote>
<p>为什么要用映射后的特征，而不是原始特征来计算呢?</p>
<p>为了更好的拟合数据是其中一个原因。另外一个重要原因是本章<strong>写在前面</strong>提到的：样本可能存在在低维空间线性不可分的情况，而将其映射到高维空间中往往就可分。</p>
</blockquote>
<p><br></p>
<h4 id="核函数方法"><strong>核函数方法</strong></h4><hr>
<p>核函数形式化定义：如果原始特征内积\(\langle x,z \rangle\)，映射后为\(\langle \phi(x), \phi(z) \rangle\)，那么核函数定义为：</p>
<p>$$<br>K(x,z) = \phi(x)^T \phi(z)    \qquad (ml.1.4.5)<br>$$</p>
<blockquote>
<p>如果要实现\(\phi(x)^T \phi(z)\)的计算，只需要先计算\(\phi(x)\)，然后再计算\(\phi(z)^T \phi(z)\)即可。然而这种计算方式是非常低效的。如果最初特征是\(n\)维的，现在将其映射到\(n^2\)维，然后再计算，此时时间复杂度从\(O(n)\)上升为\(O(n^2)\)。</p>
</blockquote>
<p><strong>核函数无需知道非线性映射函数\(\phi\)的形式和参数，它会隐式地改变从输入空间到特征空间的映射，进而对特征空间产生影响。</strong> 举例：假设\(x\)和\(z\)都是\(n\)维的，有: </p>
<p>$$<br>K(x,z) = (x^Tz)^2    \qquad\quad(ml.1.4.6)<br>$$</p>
<p>公式\((ml.1.4.6)\)称为<strong>多项式核函数（polynomial kernel）</strong>，它是对原始输入空间到特征空进行多项式映射（非线性变换）。假设\(x,z\)都是\(n\)维向量，对公式\((ml.1.4.6)\)展开后，可得：</p>
<blockquote>
<p>$$<br>\begin{align}<br>K(x,z) &amp;= (x^Tz)^2 = \left( \sum_{i=1}^{n} x_i z_i \right) \cdot \left( \sum_{i=1}^{n} x_i z_i \right) \\<br>&amp; = \sum_{i=1}^{n} \sum_{j=1}^{n} (x_i x_j) (z_i z_j) = \phi(x)^T \phi(z)<br>\end{align}  \qquad(n.ml.1.4.11)<br>$$</p>
</blockquote>
<p>从公式\((n.ml.1.4.11)\)可以看出，只要计算原始特征\(x\)和\(z\)的平方（时间复杂度\(O(n)\)），就等价于映射后特征的内积。</p>
<blockquote>
<p>举例：当\(n\)=2时，根据上面的公式可得映射函数：</p>
<p>$$<br>\phi(x) =<br>\begin{bmatrix}<br>x_1^2 \\<br>\sqrt{2} x_1 x_2 \\<br>x_2^2 \\<br>\end{bmatrix}     \qquad\qquad(exp.ml.1.4.2)<br>$$</p>
<p>令\(x = (x_1, x_2), z = (z_1, z_2)\)</p>
<p>$$<br>\begin{align}<br>K(x,z) &amp;= (x^T z)^2 = (x_1 z_1 + x_2 z_2)^2  \\<br>&amp;= (x_1^2 z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2) \\<br>&amp;= \underline { (x_1^2, \sqrt{2} x_1 x_2, x_2^2) } \cdot \underline { (z_1^2, \sqrt{2} z_1 z_2, z_2^2)^T } \\<br>&amp;= \phi(x)^T \phi(z)<br>\end{align} \qquad(exp.ml.1.4.3)<br>$$</p>
</blockquote>
<p>也就是说，核函数\(K(x,z) = (x^Tz)^2\)只能选择这样的映射函数\(\phi\)时，才能等价于映射后特征的内积。下面再看一个核函数：</p>
<blockquote>
<p>$$<br>\begin{align}<br>K(x,z) &amp; = (x^Tz + c)^2 \\<br>&amp; = \sum_{i=1}^{n} \sum_{j=1}^{n} (x_i x_j)(z_i z_j) + \sum_{i=1}^{n} (\sqrt{2c} x_i) (\sqrt{2c} z_i) + c^2<br>\end{align} \qquad(n.ml.1.4.12)<br> $$</p>
<p>那么其对应的映射函数应该为：</p>
<p>$$<br>\phi(x) =<br>\begin{bmatrix}<br>x_1 x_1 \\<br>x_1 x_2 \\<br>x_1 x_3 \\<br>x_2 x_1 \\<br>x_2 x_2 \\<br>x_2 x_3 \\<br>x_3 x_1 \\<br>x_3 x_2 \\<br>x_3 x_3 \\<br>\sqrt{2c}x_1 \\<br>\sqrt{2c}x_2 \\<br>\sqrt{2c}x_3 \\<br>c \\<br>\end{bmatrix} \qquad\qquad(exp.ml.1.4.4)<br>$$</p>
</blockquote>
<p>由于计算的是内积，我们联想一下余弦相似度：如果\(x\)和\(z\)向量夹角越小，那么么核函数值就越大，反之就越小。因此，核函数值大小与 \(\phi(x)\)和\(\phi(z)\)相似度 成正相关。下面再看一个核函数:</p>
<p>$$<br>K(x,z) = \exp \left( -\frac{|x-z|^2}{2\sigma^2} \right)  \qquad(ml.1.4.7)<br>$$</p>
<p>从公式\((ml.1.4.7)\)可以看出，如果\(x\)和\(z\)很近（\(|x-z| \thickapprox 0\)），那么核函数值为1；如果\(x\)和\(z\)相差很大（\(|x-z| \gg 0\)），那么核函数值约等于0。由于这个函数类似于高斯分布，因此称为<strong>高斯核函数</strong>，也叫做<strong>径向基函数（Radial Basic Function，简称RBF）</strong>。它能够把原始函数映射到无穷维。</p>
<p>下面在看一个核函数：</p>
<p>$$<br>K(x,z) = \tanh(a x^T z + b) \qquad\qquad(ml.1.4.8)<br>$$</p>
<p>上式称为<strong>Sigmoid核函数（sigmoidal kenel）</strong>。Sigmoid核多用于多层感知机神经网络，隐含层节点数目、隐含节点对输入节点的权值都是在训练过程中自动确定。</p>
<blockquote>
<p>注意：sigmoid核对应的核函数矩阵不是正定的。之所以仍作为kernel的一种是因为给了kernel在实际应用中更多的扩展，包括在svm、神经网络等方法中的应用。</p>
</blockquote>
<p><br></p>
<h4 id="核函数有效性判定"><strong>核函数有效性判定</strong></h4><hr>
<p>问题：</p>
<blockquote>
<p>给定一个函数\(K\)，我们能否使用\(K\)来替代计算\(\phi(x)^T \phi(z)\)？也就是说，能否找出一个\(\phi\)，使得对于所有的\(x\)和\(z\)，都有\(K(x,z) = \phi(x)^T \phi(z)\)？</p>
<p>公式\((n.ml.1.4.11)\)描述的核函数\(K(x,z) = (x^T z)^2\)，是否能够认为\(K\)是一个有效的核函数？</p>
</blockquote>
<ul>
<li><p>Mercer定理</p>
<p>  在介绍Mercer定理之前，我们先解决上面的问题：</p>
<blockquote>
<p>给定\(m\)个训练样本\(\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}\)，每一个\(x^{(i)}\)对应一个特征向量。那么，我们可以将任意两个样本\(x^{(i)}\)和\(x^{(j)}\)带入函数\(K\)中，计算得到\( K_{ij} = K(x^{(i)}, x^{(j)})\)，其中\(1 \le i \le m\)，\(1 \le j \le m\)。</p>
<p>这样，我们就可以计算得到一个\(m*m\)的核函数矩阵（Kernel Matrix）。这里为了方便，将和函数矩阵和核函数\(K(x,z)\)都用\(K\)来表示。</p>
</blockquote>
<p>  假设\(K\)是有效的核函数，那么根据核函数定义：</p>
<p>  $$<br>  \begin{align}<br>  K_{ij} &amp; = K(x^{(i)}, x^{(j)}) \\<br>  &amp; = \phi(x^{(i)})^T \phi(x^{(j)}) = \phi(x^{(j)})^T \phi(x^{(i)}) \\<br>  &amp; = K(x^{(j)}, x^{(i)}) = K_{ji}<br>  \end{align}  \qquad(ml.1.4.9)<br>  $$</p>
<p>  可见，核函数矩阵\(K\)应该是个对称阵。下面的公式推导会给出更明确的结论。</p>
<blockquote>
<p>首先，使用符号\(\phi_k(x)\)来表示映射函数\(\phi(x)\)的第\(k\)维属性值。那么，对于任意向量\(z\)，可得：</p>
<p>  $$<br>  \begin{align}<br>  z^T K z &amp; = \sum_{i=1}^{m} \sum_{j=1}^{m} z_i K_{ij} z_j \\<br>  &amp; = \sum_{i=1}^{m} \sum_{j=1}^{m} z_i \phi(x^{(i)})^T \phi(x^{(j)}) z_j \\<br>  &amp; = \sum_{i=1}^{m} \sum_{j=1}^{m} z_i \sum_{k=1}^{n} \phi_k(x^{(i)})\phi_k(x^{(j)}) z_j \\<br>  &amp; = \sum_{k=1}^{n} \sum_{i=1}^{m} \sum_{j=1}^{m} z_i \phi_k(x^{(i)})\phi_k(x^{(j)}) z_j \\<br>  &amp; = \sum_{k=1}^{n} \left( \sum_{i=1}^{m} z_i \phi(x^{(i)}) \right)^2 \\<br>  &amp; \ge 0<br>  \end{align}  \qquad\quad(n.ml.1.4.13)<br>  $$</p>
</blockquote>
<p>  从公式\((n.ml.1.4.13)\)可以看出，如果\(K\)是个有效的核函数（即\(K(x,z)\)与\(\phi(x)^T \phi(z)\)等价），那么，在训练集上得到的核函数矩阵\(K\)应该是半正定的（\(K \ge 0\))。</p>
<p>  如此，我们可以得出核函数的必要条件是：</p>
<blockquote>
<p><strong>\(K\)是有效的核函数 \(\Longrightarrow\) 核函数矩阵\(K\)半正定。</strong></p>
</blockquote>
<p>  比较幸运的是，这个条件也是充分的。具体由<strong>Mercer定理</strong>来表达:</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Mercer定理：</th>
</tr>
</thead>
<tbody>
<tr>
<td>如果函数\(K\)是\(R^n \times R^n \rightarrow R\)上的映射（即从两个\(n\)维向量映射到实数域）并且如果\(K\)是个有效的核函数（也称为Mercer核函数），那么当且仅当对于训练样例\( \{x^{(1)}, \cdots, x^{(m)}\}\)来说，其相应的核函数矩阵是半正定的。</td>
</tr>
</tbody>
</table>
<p>Mercer定理可以说明：为了证明\(K\)是有效的核函数，我们不用直接去寻找\(\phi\)，只需要在训练集上求出各个\(K_{ij}\)，然后判定矩阵\(K\)是否是半正定即可。</p>
<blockquote>
<p>注：使用矩阵左上角主子式 \(\ge 0\)等方法可判定矩阵是否是半正定的。</p>
</blockquote>
<p>当然，Mercer定理证明过程中可以通过\(L2\)范数和再生希尔伯特空间等概念，但在\(n\)维情况下，这里给出的证明是等价的。</p>
<p>Kernel方法不仅用在SVM上，只要在模型的计算过程中出现\(\langle x,z \rangle\)，我们都可以使用\(K(x,z)\)去替换，通过特征空间变换解决本章开头所提到的特征映射问题。</p>
<p><br></p>
<h3 id="支持向量机"><strong>支持向量机</strong></h3><hr>
<p>支持向量机（Support Vector Machine，简称SVM）是建立在统计学习理论之上的学习算法，其主要优势体现在解决线性不可分问题，通过引入核函数，巧妙地解决了在高维空间的内积运算复杂度的问题，从而很好的解决了非线性分类问题。</p>
<p>这里我们先通过LR模型与SVM比较，建立二者之间的基本认知。</p>
<p><br></p>
<h4 id="LR与SVM"><strong>LR与SVM</strong></h4><hr>
<ul>
<li><p>LR模型回顾</p>
<p>  在第1章，我们详细阐述了Logistic Regression模型。我们直接地可理解为，LR的目的是从特征中学习出一个0/1分类或回归模型。</p>
<blockquote>
<p>LR特点：</p>
<p>LR模型将特征的线性组合作为自变量（取值范围\((-\infty, \infty)\)），使用logistic函数（又称sigmoid函数）将自变量映射到\((0,1)\)上，映射后的值被认为是y=1的概率（如果是分类问题），表达式 \(P(y=1|x;w)=h_w(x)\)。（具体细节可参考<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>）</p>
</blockquote>
<p>  当我们要判别一个样本（用一组特征表示）属于哪个类时，只需求出\(h_w(x)\)即可；若\(h_w(x)&gt;0.5\)，则属于y=1的类；反之属于y=0的类。（阈值0.5的前提条件：当正负样本基本均衡时）</p>
<blockquote>
<p>再次审视\(h_w(x)\)：</p>
<p>  $$<br> P(Y=1|X=x; w) = h_w(x) = \frac{1}{1+e^{-w^T \cdot x}}<br> $$</p>
<p>我们可以发现， \(h_w(x)\)只与\(w^Tx\)有关：若\(w^Tx \geq 0\)，则\(h_w(x) \geq 0.5\)；\(w^Tx &lt; 0\)，则\(h_w(x) &lt; 0.5\)。也就是说，一个样本真实类别的决定权还在\(w^Tx\)。</p>
<p>从几何角度可这样理解：当\(w^Tx \gg 0\)时，\(h_w(x)=1\)；反之，\(h_w(x)=0\)。</p>
</blockquote>
<p>  如果我们从\(w^Tx\)出发，希望LR模型最终达到的目标就是：<strong>让训练数据中y=1的特征组合尽可能远大于0（即\(w^Tx \gg 0\)）；让训练数据中y=0的特征组合尽可能远小于0（即\(w^Tx \ll 0\)）</strong>。</p>
<p>  LogReg模型要用<strong>全部训练数据</strong>学习参数\(w\)，学习过程中尽可能使正样本的特征组合远大于0，负样本的特征组合远小于0。</p>
<blockquote>
<p><strong>注意：LR强调的是在全部训练样本上达到这个目标，采用MLE作为求参准则。</strong></p>
</blockquote>
</li>
<li><p>SVM直观引入</p>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_1_svm_introduction.png" width="360" height="300" alt="SVM几何间隔" align="right"></p>
<p>  假设\(w^Tx=0\)是正负样本的分割线（或分割面，SVM中称为超平面），存在a、b、c三个样本点（如右图所示），它们到\(w^Tx=0\)的距离分别表示为\(h(a)、h(b)、h(c)\)（其中\(h(a) &gt; h(b) &gt; h(c)\)），即a点距离分割面的距离最远，c点距离最近。</p>
<p>  相对于c点来说，我们更有把握的确定a点的类别，b点类别基本也可以确定。c点因为距离分割面比较近，没有把握确定其类别。</p>
<blockquote>
<p>因为c点距离\(w^Tx=0\)比较近，而训练出来的模型存在一定的误差。参数\(w\)稍微变动一些，可能直接导致c点类别的判断结果截然不同。</p>
</blockquote>
<p>  因此我们可以得出如下总结：<strong>我们更关心的是靠近中间分割面的点，如果让它们尽可能的远离分割面即可，没必要在所有点上达到最优。</strong> 如果这样的话，就需要使得一部分点（距离中间线较远的点）靠近中间线来换取另一部分点（距离中间线较近的点）更加远离中间线。</p>
<p>  也许这就是SVM与LR不同的学习思想和出发点：<strong>前者考虑局部</strong>（只关心距离较近的点，不太关心已经确定远离的点），<strong>后者考虑全局</strong>（整体上达到最远，会出现已经远离的点调整中间线后更加远离）。    </p>
<blockquote>
<p><strong>其实SVM仅利用支持向量（少部分样本）表示训练样本，非全部样本。</strong></p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="函数间隔与几何间隔"><strong>函数间隔与几何间隔</strong></h4><hr>
<ul>
<li><p>形式化表示</p>
<p>  这里，我们先定义SVM中使用的样本标签为\(\{-1, 1\}\)（与LR中的\(\{0, 1\}\)不同）。同时LR中的参数\(w \in R^{n+1}\)，在这里\(w \in R^{n}, b \in R\)。SVM中要求的分类器可表示为：</p>
<p>  $$<br>  h_{w,b}(x) = g(z) = g(w^Tx + b)        \qquad\qquad(ml.1.4.10)<br>  $$</p>
<blockquote>
<p>由于是分类问题，我们只需要考虑\(w^Tx + b\)的正负问题即可，不用关心\(g(z)\)绝对大小。因此我们这里将\(g(z)\)做一个简化，将其简单映射到\(y=1\)和\(y=-1\)上。映射关系如下：</p>
<p>  $$<br>  g(z) =<br>  \begin{cases}<br>  \quad 1, &amp; z \ge 0 \\<br>  \;\, -1, &amp; z &lt; 0<br>  \end{cases}     \qquad\quad(n.ml.1.4.14)<br>  $$</p>
</blockquote>
</li>
<li><p><strong>函数间隔（Functional Margin）</strong></p>
<p>  任意给定一个训练样本\((x^{(i)}, y^{(i)})\)，用\(x^{(i)}\)表示样本特征（向量），\(y^{(i)}\)表示样本标签，\(i\)表示第\(i\)个样本。定义函数间隔：</p>
<p>  $$<br>  \hat{\gamma}^{(i)} = y^{(i)} \left(w^T x^{(i)} + b\right)  \qquad(ml.1.4.11)<br>  $$</p>
<p>  当\(y^{(i)}=1\)时，在公式\((n.ml.1.4.14)\)定义中，\(w^Tx^{(i)}+b \ge 0\)，\(\hat{\gamma}^{(i)}\)的值实际上就是\(\left|w^Tx^{(i)}+b\right|\)，反之亦然。</p>
<blockquote>
<p>为了使函数间隔最大（如此我们就更有信心确认该样本是正例还是反例），当\(y^{(i)}=1\)时，\(w^Tx^{(i)}+b\)应该是个比较大的正数，反之是个较大的负数。</p>
</blockquote>
<p>  因此，<strong>函数间隔可以表示我们认为样本（一组特征表示）是正例还是反例的确信度。</strong></p>
<p>  公式\((ml.1.4.11)\)定义的是一个样本的函数间隔，现在我们定义在全局样本上的函数间隔：</p>
<p>  $$<br>  \hat{\gamma} = \min_{i=1,2,\cdots, m} \hat{\gamma}^{(i)} \qquad\quad(ml.1.4.12)<br>  $$</p>
<p>  即全局函数间隔就是在所有训练样本上分类正例和负例确信度最小的那个函数间隔。</p>
<p>  继续考虑参数\(w、b\)，如果同时加大这两个值，比如在\(w^Tx+b\)前面乘一个系数\(n\)(\(n&gt;1\))，那么所有点的函数间隔都会增加\(n\)倍，参数倍数缩放对求解问题不应该有影响。</p>
<blockquote>
<p>因为我们要求的是\(w^Tx+b=0\)，参数同比例缩放对结果是无影响的。</p>
</blockquote>
<p>  为了限制\(w和b\)，需要加入归一化条件，毕竟求解的目标是确定唯一一个\(w和b\)，而不是多组线性相关的向量。</p>
</li>
<li><p><strong>几何间隔（Geometric Margin）</strong></p>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_2_svm_geometric_margin.png" width="300" height="250" alt="SVM几何间隔" align="right"></p>
<p>  如右图所示，假设分割面\(w^Tx+b=0\)上有了点B，\(n\)维空间任意一点A\((x^{(i)}, y^{(i)})\)到该分割面的距离用\(\gamma^{(i)}\)表示，假设B点就是A在分割面上的投影。利用中学的几何知识可知，向量\(\overrightarrow{BA}\)的方向是\(w\)（分割面的梯度），单位向量是\(\frac{w}{| w |}\)。</p>
<blockquote>
<p>那么B点的坐标可以求得：</p>
<p>  $$<br>  x = x^{(i)} - \gamma^{(i)} \frac{w}{|w|}  \qquad (n.ml.1.4.15)<br>  $$</p>
<p>将公式\((n.ml.1.4.15)\)带入\(w^Tx+b=0\)得到：</p>
<p>  $$<br>  w^T (x^{(i)} - \gamma^{(i)} \frac{w}{|w|}) + b =0 \qquad(n.ml.1.4.16)<br>  $$</p>
<p>进一步整理得到：</p>
<p>  $$<br>  \gamma^{(i)} = \left(\frac{w}{|w|}\right)^T x^{(i)} + \frac{b}{|w|} \qquad(n.ml.1.4.17)<br>  $$</p>
<p>\(\gamma^{(i)}实际上就是n维空间中点到超平面的距离。\) </p>
</blockquote>
<p>  <strong>主要考查点：</strong></p>
<p>  在我们广告算法组的相关面试中，经常会请求职者推导\(n\)维空间中点到超平面的距离公式（如果求职者知道SVM），但是能完整推导出来的只有十之二三 …</p>
<p>  几何间隔在分类问题中，可表示为：</p>
<p>  $$<br>  \gamma^{(i)} = y^{(i)} \left( \left(\frac{w}{|w|}\right)^T x^{(i)} + \frac{b}{|w|} \right) \qquad(ml.1.4.13)<br>  $$</p>
<p>  可以发现，当\(|w|=1\)时，就是函数间隔。其实<strong>几何间隔是函数间隔归一化的结果</strong>。那么，全局几何间隔定义为：</p>
<p>  $$<br>  \gamma = \min_{i=1,2,\cdots, m} \gamma^{(i)} \qquad(ml.1.4.14)<br>  $$</p>
</li>
</ul>
<p><br></p>
<h4 id="最优间隔分类器"><strong>最优间隔分类器</strong></h4><hr>
<p>在前面也提到，SVM的目标是寻找一个超平面，使得距离超平面最近的点能有更大的间距。不考虑所有的点都必须远离超平面，只关心求得的超平面能够让所有的点中离它最近的点具有最大间距。因此，我们需要数学化表示最优间隔（optimal margin classifier）。</p>
<ul>
<li><p>形式化表示</p>
<p>  最优间隔形式化可表示为：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \gamma \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge \gamma, \; i=1,\cdots,m \\<br>  &amp; \qquad |w|=1<br>  \end{align}  \qquad(ml.1.4.15)<br>  $$</p>
<blockquote>
<p>这里利用\(|w|=1\)约束\(w\)，使得\(w^Tx+b\)是几何间隔。</p>
</blockquote>
<p>  到这里其实已经将SVM的模型定义出来了，求解出来的模型称为最优间隔分类器。如果求得了\(w\)和\(b\)，对于任意一个样本\(x\)，我们就能够分类了。那么，<strong>接下来主要工作就是围绕如何求解参数\(w\)和\(b\)来展开的</strong>。</p>
<p>  由于\(|w|=1\)不是凸函数，那么先利用几何间隔和函数间隔的关系转化一下\(\gamma=\frac{\hat{\gamma}}{|w|}\)，因此公式\((ml.1.4.6)\)可改写为：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \frac {\hat{\gamma}} {|w|} \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge \gamma, \; i=1,\cdots,m<br>  \end{align}  \qquad(ml.1.4.16)<br>  $$</p>
<blockquote>
<p>上式所求的极大值仍然是几何间隔，只不过此时的\(w\)不再受\(|w|=1\)的约束了。</p>
</blockquote>
<p>  然而此时的目标函数仍然不是凸函数，无法方便求解。继续改写目标函数…</p>
<p>  前面说过同时缩放\(w\)和\(b\)对结果没有影响，但最后希望求得的是确定值，不是一组倍数值。为了达到这个目的，需要对\(\hat{\gamma}\)做一些限制，以保证最终解是唯一的。为了简便，取\(\hat{\gamma}=1\)。</p>
<blockquote>
<p><strong>将全局函数间隔定义为1</strong>，即将离超平面最近的点的距离定义为\(\frac{1}{|w|}\)（当然定义为其它非0常数也可以，不影响最终的参数求解）。</p>
</blockquote>
<p>  由于求\(\frac{1}{|w|}\)的极大值等价于求\(\frac{1}{2} |w|^2\)的极小值，因此目标函数可进一步改写为：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \frac{1}{2} |w|^2 \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge 1, \; i=1,\cdots,m<br>  \end{align}  \qquad(ml.1.4.17)<br>  $$</p>
<p>  公式\((ml.1.4.17)\)是一个典型的带不等式约束的二次规划求解问题（目标函数是自变量的二次函数）。</p>
<p>  到此为止，我们完成了目标函数由非凸到凸的转变.</p>
</li>
</ul>
<p><br></p>
<h4 id="最优间隔分类器学习过程">最优间隔分类器学习过程</h4><hr>
<ul>
<li><p>KKT条件</p>
<p>  第一节对偶优化问题中，公式\((ml.1.4.4)\)有一个问题：在什么条件下\(\mathcal{P}^{\ast}\)与\(\mathcal{D}^{\ast}\)    两者等价？</p>
<p>  暂时先不回答这个问题，我们假设函数\(f(w)\)和\(g(w)\)是凸函数，\(h(w)\)是仿射的（affine，仿射的含义是指存在\(a_i、b_i\)，能够使得\(h_i(w)=a_i^T w + b_i成立\)）。并且存在\(w\)使得对于所有的\(i\)，\(g_i(w)&lt;0\)。在这种假设下，一定存在\(w^{\ast}、\alpha^{\ast}、 \beta^{\ast}\)使得\(w^{\ast}\)是原问题的解，\(\alpha^{\ast}、\beta^{\ast}\)是对偶问题的解。此时也满足\(\mathcal{P}=\mathcal{D}=\mathcal{L}(w^{\ast},\alpha^{\ast},\beta^{\ast})\)。</p>
<p>  另外，解\(w^{\ast}、\alpha^{\ast}、 \beta^{\ast}\)满足<strong>库恩-塔克条件(Karush-Kuhn-Tucker, 简称KKT条件)</strong>，该条件表示如下：</p>
<p>  $$<br>  \begin{align}<br>  \frac{\partial} {\partial w_i} \mathcal{L}(w^{\ast}, \alpha^{\ast}, \beta^{\ast}) &amp; = 0, \quad i=1,\cdots,n \qquad(1)\\<br>  \frac{\partial}{\partial \beta_i} \mathcal{L}(w^{\ast}, \alpha^{\ast}, \beta^{\ast}) &amp; = 0, \quad i =1, \cdots, k \qquad(2)\\<br>  a^{\ast} g_i(w^{\ast}) &amp; = 0, \quad i=1,\cdots, l \qquad\;(3)\\<br>  g_i(w^{\ast}) &amp; \le 0, \quad i = 1,\cdots, l \qquad\;(4)\\<br>  a^{\ast} &amp; \geq 0, \quad i=1,\cdots, l        \qquad\;(5)<br>  \end{align}  \qquad (ml.1.4.18)<br>  $$</p>
<p>  所以，如果\(w^{\ast}、\alpha^{\ast}、\beta^{\ast}\)满足了库恩-塔克条件，那么它们就是原问题与对偶问题的解。</p>
<p>  在这里我重点关注公式\((ml.1.4.18)-(3)\)，这个条件称作是KKT Dual Complementarity条件。这个条件隐含了<strong>如果\(\alpha^{\ast} &gt; 0\)</strong>，那么\(g_i(w) = 0\)。</p>
<blockquote>
<p>从集合的角度可这样理解：\(g_i(w^{\ast})=0\)时，\(w\)处在可行域的边界上，这时才是真正起作用的约束。而位于可行域内部\(（即g_i(w^{\ast})&lt;0）\)的点都是不起作用的约束，对应的\(\alpha^{\ast}=0\)。</p>
</blockquote>
<p>  这个KKT双重补足条件会用来解释SVM中的<strong>支持向量和SMO的收敛测试</strong>。KKT思想可总结如下：</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><strong>KKT总体思想是认为极值会在可行域边界上取得，也就是不等式为0或等式约束的条件下取得，而最优下降（或上升）方向一般是这些等式的线性组合，其中每个元素要么是不等式为0的约束，要么是等式约束。对于在可行域边界内的点，对目标函数最优解不起作用，因此前面的系数为0。</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li><p>优化目标</p>
<p>  重新前面的SVM的优化问题：</p>
<blockquote>
<p>$$<br>  \begin{align}<br>  &amp; \max_{w, b} \; \frac{1}{2} |w|^2 \\<br>  &amp; s.t. \; y^{(i)}(w^Tx+b) \ge 1, \; i=1,\cdots,m<br>  \end{align}  \qquad(n.ml.1.4.18)<br>  $$</p>
<p>首先将约束条件改写为：</p>
<p>  $$<br>  g_i(w) = -y^{(i)} (w^T x^{(i)} + b) + 1 \le 0   \qquad\quad(n.ml.1.4.19)<br>  $$</p>
</blockquote>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_5_optimal_margin_classifier.png" width="360" height="250" alt="最优间隔分类器" align="right"></p>
<p>  根据上一节中的KKT条件可知，只有函数间隔是1（离超平面最近的点）的线性约束式前面的系数\(\alpha_i &gt; 0\)，也就是说这些对应的约束式\(g_i(w) = 0\)。对于其它的不在线上的点\(g_i(w) &lt; 0\)，极值不会在它们所在的范围内取得，因此前面的系数\(\alpha_i = 0\)。</p>
<blockquote>
<p>这里每一个约束式对应一个训练样本。</p>
</blockquote>
<p>  如右图所示，实线是最大间隔超平面，用”×”表示正样本，”o”表示负样本。在虚线上的点（两个”×”和一个”o”）就是函数间隔是1的点（又称支持向量），那么它们前面的系数\(\alpha_i &gt; 0\)，其它点都是\(\alpha_i = 0\)。</p>
<p>  因此，可构造拉格朗日函数如下：</p>
<p>  $$<br>  \mathcal{L}(w,b,\alpha) = \frac{1}{2} |w|^2 - \sum_{i=1}^{m} \alpha_i \cdot  \left(y^{(i)} (w^Tx + b) - 1\right)  \qquad(ml.1.4.19)<br>  $$</p>
<blockquote>
<p>说明：这里面只有\(\alpha_i\)而没有\(\beta_i\)，是因为SVM原问题中没有等式约束，只有不等式约束。</p>
</blockquote>
</li>
<li><p>SVM对偶问题</p>
<p>  下面我们看着拉格朗日的对偶问题来求解，对偶函数表达式如下：</p>
<p>  $$<br>  \mathcal{D} = \max_{\alpha;\; \alpha \ge 0} \min_{w,b} \mathcal{L}(w,b,\alpha)  \qquad\qquad(ml.1.4.20)<br>  $$</p>
<ul>
<li><p>首先，极小化过程。固定参数\(\alpha\)，求\(\mathcal{L}(w,b,\alpha)\)关于\(w\)和\(b\)的最小值。分别对\(w\)和\(b\)求偏导：</p>
<p>$$<br>\begin{align}<br>\nabla_w \mathcal{L}(w,b,\alpha) = w - \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} = 0 \qquad(1)\\<br>\frac{\partial}{\partial b} \mathcal{L}(w,b,\alpha) = \sum_{i=1}^{m} \alpha_i y^{(i)} = 0  \qquad\qquad\;\;\;(2)<br>\end{align}        \qquad(ml.1.4.21)<br>$$</p>
<blockquote>
<p>根据公式\((ml.1.4.21)\)-\((1)\)可得到：</p>
<p>\(<br>\qquad\qquad w = \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} \qquad(n.ml.1.4.20)<br>\)</p>
</blockquote>
<p>将公式\((n.ml.1.4.20)\)带入\((ml.1.4.18)\)，此时得到的是该目标函数的最小部分（因为目标函数是凸函数）。公式推导如下：</p>
<blockquote>
<p>$$<br>\begin{align}<br>\mathcal{L}(w,b,\alpha) &amp; = \frac{1}{2} w^T w - \sum_{i=1}^{m} \alpha_i y^{(i)} w^T x^{(i)} - b \cdot \sum_{i=1}^{m} \alpha_i y^{(i)}  + \sum_{i=1}^{m} \alpha_i \qquad\quad (1)\\<br>&amp; = \frac{1}{2} w^T \cdot \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} - w^T \cdot \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} + \sum_{i=1}^{m} \alpha_i - b \sum_{i=1}^{m} \alpha_i y^{(i)} \quad(2) \\<br>&amp; = -\frac{1}{2} w^T \sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} + \sum_{i=1}^{m} \alpha_i - b \sum_{i=1}^{m} \alpha_i y^{(i)} \qquad\quad(3) \\<br>&amp; = -\frac{1}{2} \left(\sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} \right)^T \cdot\sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)} + \sum_{i=1}^{m} \alpha_i - b\sum_{i=1}^{m} \alpha_i y^{(i)} \quad(4) \\<br>&amp; = -\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \cdot (x^{(i)})^T x^{(j)} + \sum_{i=1}^{m} \alpha_i - b \sum_{i=1}^{m} \alpha_i y^{(i)} \quad(5)<br>\end{align} \;(n.ml.1.4.21)<br>$$</p>
<p>说明：第\((3)\)步到第\((4)\)步使用了线性代数中的转置运算，由于\(\alpha_i\)和\(y^{(i)}\)都是实数，因此转置后一样。第\((4)\)步到第\((5)\)步使用了乘法运算规则: \((a+b)(a+b) = aa + ab + ba + bb\)。</p>
<p>公式\((n.ml.1.4.21)\)主要目的是将\(\mathcal{L}(w,b,\alpha)\)转化为关于拉格朗日乘子\(\alpha\)的函数。</p>
</blockquote>
<p>又因为有\((ml.1.4.21)-(2)\)成立，所以公式\((n.ml.1.4.21)-(5)\)最后一项为0。因此，目标函数最后整理为：</p>
<p>$$<br>\mathcal{L}(w,b,\alpha) = -\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} + \sum_{i=1}^{m} \alpha_i \qquad(ml.1.4.22)<br>$$</p>
<blockquote>
<p>这里我们将向量内积\((x^{(i)})^T x^{(j)}\)表示为\(\langle x^{(i)}, x^{(j)} \rangle\)。<strong>正因为有向量内积的（复杂）计算，才有后面Kernal的出现</strong>。</p>
</blockquote>
</li>
<li><p>接着，求极大化过程。此时极值问题可表示为：</p>
<p>$$<br>\begin{align}<br>&amp; \max_{\alpha} \quad -\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} + \sum_{i=1}^{m} \alpha_i \\<br>&amp; s.t. \quad \alpha_i \ge 0, \quad i=1,\cdots, m \\<br>&amp; \qquad \sum_{i=1}^{m} \alpha_i y^{(i)} = 0.<br>\end{align} \qquad\quad(ml.1.4.23)<br>$$</p>
<blockquote>
<p>从公式\((ml.1.4.23)\)中可以看出，目标函数和线性约束都是凸函数，并且这里不存在不等式约束。根据4.1.4节中介绍的KKT条件可知：存在\(w\)使得对于所有的\(i，g_i(w) &lt; 0\)。因此，一定存在\(w^{\ast}、\alpha^{\ast}\)使得\(w^{\ast}\)是原问题的解，\(\alpha^{\ast}\)是对偶问题的解。（在这里，求\(\alpha_i\)就是求\(\alpha^{\ast}\)了。）</p>
</blockquote>
</li>
</ul>
</li>
<li><p>对偶问题求解思路</p>
<p>  如果求出\(\alpha_i\)，根据\((n.ml.1.4.20)\)可求出\(w\)（即\(w^{\ast}\)，原问题的解）。参数\(b\)的求解可利用下面的公式：</p>
<blockquote>
<p>$$<br>  b^{\ast} = - \frac{\max_{i:y^{(i)}=-1} {w^{\ast}}^T x^{(i)} + \min_{i:y^{(i)}=1} {w^{\ast}}^T x^{(i)}} {2}  \qquad\quad (n.ml.1.4.22)<br>  $$</p>
</blockquote>
<p>  即<strong>离超平面最近的正的函数间隔要等于离超平面最近的负的函数间隔</strong>。这样，我们就可以求出最优间隔分类器\(w^T x + b = 0\)。</p>
<blockquote>
<p>此外，我们根据公式\((n.ml.1.4.20)\)得到了参数\(w\)的表达式。但别忘了，我们SVM通篇考虑的是最优间隔分类器\(w^x+b=0\)的求解。根据\(w\)的表达式，我们带入方程可得：</p>
<p>  $$<br>  w^T x + b = \left(\sum_{i=1}^{m} \alpha_i y^{(i)} x^{(i)}\right)^T x + b = \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x \rangle + b  \qquad(n.ml.1.4.23)<br>  $$</p>
<p>也就是说，如果按照\(w^T x + b\)为新来的样本做分类，首先根据\(w\)和\(b\)做一次线性运算，然后看求出来的结果是大于0还是小于0，以此来判断样本是正例还是负例。<strong>现在有了\(\alpha_i\)，我们不需要求\(w\)，只需要将新来的样本与训练数据中的所有样本做内积即可</strong>。</p>
<p><strong>或许存在这样一个疑问：与所有样本做内积是否太耗时了？</strong> 其实不然，根据KKT条件可知，只有支持向量的参数\(\alpha_i\)是大于0的，其它情况都等于0。因此，我们只需要求新样本与支持向量的内积，然后运算即可。</p>
</blockquote>
<p>  上面的讨论都是建立在样本线性可分的假设前提下，当样本不可分时，我们可以尝试利用核函数将特征映射到高维空间，这样很可能就可分了。然而，映射后也不能保证样本100%线性可分，那又该如何？这就是下面通过引入松弛变量的软间隔模型和正则化要解决的问题。</p>
</li>
</ul>
<p><br></p>
<h4 id="软间隔与正则化"><strong>软间隔与正则化</strong></h4><hr>
<p>为了解决上述问题，这里需要将模型做一个调整，以保证在不可分的情况下，也能够尽可能地找出分隔超平面。</p>
<ul>
<li><p>软间隔与惩罚项</p>
<p>  <img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_4_1_6_soft_margin_and_non_seperate.png" width="270" height="225" alt="soft-margin" align="right"></p>
<p>  从右图可以看到，如果多一个离群点（也许是噪声，黑圆圈表示）可能造成超平面的移动，求出来的最优间隔在缩小（实线部分），那么之前的模型对噪声非常敏感。更有甚者，如果离群点在另一个类中，此时就线性不可分了。</p>
<p>  如何解决这个问题呢？</p>
<p>  我们可以考虑允许一些点游离并在模型求解中违背约束条件（函数间隔\(\ge 1\)）。那么新的模型就可以定义如下：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \min_{w,b,\gamma} \quad \frac{1}{2} |w|^2 + C \sum_{i=1}^{m} \xi_i \\<br>  &amp; s.t \quad y^{(i)} (w^T x^{(i)} + b) \ge 1-\xi_i, \; i=1,2,\cdots,m \\<br>  &amp; \qquad \xi_i \ge 0, \quad i=1,2,\cdots, m<br>  \end{align}    \qquad(ml.1.4.24)<br>  $$</p>
<p>  公式\((ml.1.4.24)\)被称为SVM的<strong>软间隔</strong>模型，非负参数\(\xi_i\)称为松弛变量。</p>
<blockquote>
<p>引入松弛变量就是允许样本点的函数间隔小于1，即在最大间隔区间里面，或者函数间隔可以为负数，即样本点在对方的区域中。</p>
</blockquote>
<p>  而放松限制条件后，我们需要重新调整目标函数，以对离群点进行惩罚。目标函数后面的\(C\sum_{i=1}^{m} \xi_i\)就表示离群点越多，目标函数值就越大，而我们要求的事尽可能小的目标函数值。</p>
<blockquote>
<p>这里的参数\(C\)表现离群点的权重（在《深入浅出ML》系列第7章正则化部分有详细介绍，那里称为惩罚因子，与这里的离群点权重是同一个意思）。\(C\)越大，表明离群点对目标函数影响越大，也就越不希望看到离群点。</p>
</blockquote>
<p>  我们可以给出以下总结：</p>
<p>  <strong>目标函数控制了离群点的数据和程度，使大部分样本点仍然遵守限制条件。同时，考虑了离群点对模型的影响。</strong></p>
<p>  模型修改后，同时也需要对\((ml.1.4.19)\)拉格朗日公式进行修改如下：</p>
<p>  $$<br>  \mathcal{L}(w,b,\alpha) = \frac{1}{2} |w|^2 + C\sum_{i=1}^{m} \xi_i - \sum_{i=1}^{m} \alpha_i \cdot  \left(y^{(i)} (w^Tx + b) - 1 + \xi_i \right) - \sum_{i=1}^{m} \gamma_i \xi_i \quad(ml.1.4.25)<br>  $$</p>
<p>  这里的\(\alpha_i、\gamma_i\)都是拉格朗日乘子。</p>
<blockquote>
<p>回想4.1.4节中介绍的拉格朗日对偶中提到的求法。<br></p>
<ol>
<li>首先，写出拉格朗日公式（如\((ml.1.4.25)\)）；<br></li>
<li>然后，将其看作是变量\(w\)和\(b\)的函数，分别对其求偏导，得到\(w\)和\(b\)的表达式；<br></li>
<li>最后，把表达式带入拉格朗日公式中，求带入公式后的极大值。</li>
</ol>
</blockquote>
<p>  整体推导过程如4.1.5节类似，这里只给出最后结果：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{\alpha} \quad  W(\alpha) = - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} y^{(i)} y^{(j)} \alpha_i \alpha_j \langle x^{(i)}, x^{(j)}\rangle + \sum_{i=1}^{m} \alpha_i \\<br>  &amp; s.t. \quad 0 \le \alpha_i \le C, \quad i=1,\cdots,m \\<br>  &amp; \qquad \sum_{i=1}^{m} \alpha_i y^{(i)} = 0<br>  \end{align} \qquad(ml.1.4.26)<br>  $$</p>
<p>  此时我们会发现，新的目标函数中没有了参数\(\xi_i\)，与之前的模型（公式\((ml.1.4.23)\)）唯一的不同在于\(\alpha_i\)又多了\(\alpha_i \le C\)的限制条件。</p>
<blockquote>
<p>需要注意的是：参数\(b\)的求值公式也发生了变化，改变结果在SMO算法里面介绍。</p>
</blockquote>
<p>  软间隔模型对应的KKT条件，变化如下：</p>
<p>  $$<br>  \begin{align}<br>  \alpha_i = 0 &amp; \Rightarrow y^{(i)} (w^T x^{(i)} + b ) \ge 1 \qquad (1) \\<br>  \alpha_i = C &amp; \Rightarrow y^{(i)} (w^T x^{(i)} + b ) \le 1 \qquad (2) \\<br>  0 &lt; \alpha_i &lt; C &amp; \Rightarrow y^{(i)} (w^T x^{(i)} + b ) = 1 \qquad (3) \\<br>  \end{align} \qquad(ml.1.4.27)<br>  $$</p>
<p>  公式\((ml.1.4.27)\)第\((1)\)式子表明，在两条间隔线外的样本点前面的系数为0；第\((2)\)个式子表明，在间隔线内的样本点（离群点）前面的系数为\(C\)；第\((3)\)个式子表明，支持向量（即在超平面两边的最大间隔线上）的样本想前面系数在\((0, C)\)上。</p>
<blockquote>
<p>通过KKT条件克制，某些在最大间隔线上的样本点也不是支持向量，相反可能是离群点（\((1)、(2)中等号成立时\)）。</p>
</blockquote>
<p>  SMO算法（Sequential Minimal Optimization）用于求解目标函数\(W(\alpha)\)的极大值。</p>
<blockquote>
<p>SMO算法将在《最优化算法》系列给出详细介绍。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="总结"><strong>总结</strong></h4><hr>
<p>这里中带你介绍了SVM的基本公式推导，并就其中的关键点给予了介绍。</p>
<p>首先，通过LR与SVM两个经典模型的比较，前者全局，后者局部（从对参数起作用的训练样本角度），引入SVM；通过函数间隔和几何间隔的介绍，明确了最优间隔的公式表达以及带优化的目标函数；并且将非凸函数转化为凸函数，以便于后续求解；</p>
<p>其次，我们详细地给出了求解带约束的极值优化问题的求解方法：拉格朗日乘子法与拉格朗日对偶法。并且详细推导了最优间隔分类器的优化学习过程，发现参数\(w\)可以用特征向量内积表示，进而引入核函数。此时，我们通过调整核函数就可以完成特征从低维到高维的映射，在低维上计算，实则表现在高位空间上。</p>
<blockquote>
<p>核函数主要用来解决特征空间变换（线性不可分、特征映射等）以及在高维空间计算效率问题。</p>
</blockquote>
<p>由于在实际场景中，训练样本不一定都线性可分。为了提升SVM算法的通用性，引入了松弛变量对优化模型进行软间隔处理，导致的结果就是优化问题变得更加复杂。然而，拉格朗日对偶求解结果中，松弛变量并没有出现在带优化的目标函数中。最后的优化求解问题，也通过SMO算法得到了解决。</p>
<p>最后，值得一提的是：支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不容易出现过拟合现象。</p>
<ul>
<li>参考资料：<ul>
<li>斯坦福大学《机器学习》课程与讲义（Andrew Ng男神）；</li>
<li>台湾大学《机器学习技法》课程与讲义 （林老师）；</li>
<li>《Pattern Recognition and Machine Learning》</li>
<li>技术博客：<a href="http://www.cnblogs.com/jerrylead" target="_blank" rel="external">http://www.cnblogs.com/jerrylead</a></li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/KKT/">KKT</a><a href="/tags/Kernel/">Kernel</a><a href="/tags/SVM/">SVM</a><a href="/tags/VC维/">VC维</a><a href="/tags/对偶优化/">对偶优化</a><a href="/tags/支持向量机/">支持向量机</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter10-clustering-family/" title="第10章：深入浅出ML之Clustering家族" itemprop="url">第10章：深入浅出ML之Clustering家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-10-20T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-10-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-10-15</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>期望最大值算法</li>
<li>高斯混合模型</li>
<li>K-Means聚类</li>
</ul>
<h3 id="写在前面">写在前面</h3><p>聚类属于无监督学习体系下的一类算法，数据样本通常表示为\(D=\{(x^{(1)}, ?), (x^{(2)}, ?), \cdots, (x^{(m)}, ?)\}\)。与分类相比，最明显的特征就是标签\(y\)未知（这里用\(?\)表示）。</p>
<p>那么如何把相近的样本点聚合在一起，同样不相近的样本尽可能不在同一个簇中？一个思路就是假设每个样本有标签，只是“隐藏”起来了，把它当作<strong>隐变量（latent variable）</strong>。然后用监督学习的思路去求解，把相同标签的样本聚合在一起即可。</p>
<p>如此一来，会发现整个过程出现两类变量：</p>
<ul>
<li>样本类别变量</li>
<li>模型参数变量</li>
</ul>
<p>传统的参数学习算法无法解决该类问题，How to do it? 下面要介绍的期望最大值算法可以很好的解决该类问题。</p>
<p><br></p>
<h3 id="期望最大值算法"><strong>期望最大值算法</strong></h3><p>期望最大值（Expectation Maximization，简称EM算法）是<strong>在概率模型中寻找参数最大似然估计或者最大后验估计的算法</strong>，其中概率模型依赖于无法观测的隐藏变量。</p>
<p>其主要思想就是<strong>通过迭代来建立完整数据的对数似然函数的期望界限，然后最大化不完整数据的对数似然函数</strong>。</p>
<p>本节将尽可能详尽地描述EM算法的原理。并结合下一节的高斯混合模型介绍EM算法是如何求解的。</p>
<p><br></p>
<h4 id="数学铺垫"><strong>数学铺垫</strong></h4><ul>
<li><p><strong>Jensen不等式</strong></p>
<p>  在《最优化》相关的资料中，通常会提到凸函数与凹函数的概念。假设\(f\)是定义域为实数的函数，如果对于所有的实数\(x\)，函数的二阶导数\(f^{\prime\prime}(x) \ge 0\)（如果存在的话），那么\(f\)就是凸函数。</p>
<blockquote>
<p> 当\(x\)是向量时，如果其对应的Hessian矩阵H是半正定的（\(H \ge 0\)），那么\(f\)是凸函数。如果\(f^{\prime\prime}(x) &gt; 0\)或\(H&gt;0\)，那么称\(f\)是严格凸函数。</p>
<p>(将\(\ge 0\)换成\(\le 0\)（或者\(&gt;0\)换成\(&lt;0\)）得到的是凹函数的概念。)</p>
</blockquote>
<p>  Jensen不等式表述如下：</p>
<p>  <strong>如果\(f\)是凸函数，\(X\)是随机变量，那么有 \(E[f(X)] \ge f (E[X])\).</strong></p>
<p>  特别的，如果\(f\)是凸函数，那么 \(E[f(X)] = f[E(X)]\)当且仅当\(p(x=E[X])=1\)，此时随机变量\(X\)就是常量。</p>
<p>  </p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_10_1_1_jensen_inequality.png" width="550" height="440" alt="Jensen不等式与凸函数"></p>
<p>  在图中，实现\(f\)是凸函数，\(X\)是随机变量，假设其取值为\(x\)和\(y\)（概率分别为0.5）。那么\(X\)的期望值就是\([x,y]\)的中值，从图中可以看到\(E[f(x)] \ge f(E[x])\)成立。</p>
<blockquote>
<p>Jensen不等式同样可应用于凹函数，不等号方向相反即可，即\(E[f(X)] \le f(E[X])\)。</p>
</blockquote>
</li>
</ul>
<p><br>    </p>
<h4 id="EM算法描述"><strong>EM算法描述</strong></h4><ul>
<li><p><strong>EM初探</strong></p>
<p>  假设给定训练样本\(\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}\)，且样本间独立。我们想寻找每个样本隐变量（类别\(z\)），使得\(P(x,z)\)最大。\(P(x,z)\)的最大似然估计（取对数）如下： </p>
<p>  $$<br>  \ell(w) = \sum_{i=1}^{m} \log P(x^{(i)}; w) = \sum_{i=1}^{m} \log \left( \sum_{z} P(x^{(i)}, z; w) \right) \qquad(ml.1.10.1)<br>  $$</p>
<blockquote>
<p>解释公式\((ml.1.10.1)\)，首先对极大似然函数取对数，然后对每个样本的每一种可能类别\(z\)求<strong>联合分布概率之和</strong>。</p>
</blockquote>
<p>  直接求参数\(w\)会比较困难，因为有隐变量\(z\)的存在，如果确定了\(z\)之后，再求解就容易了（不同于监督学习过程，这里存在两个变量：模型参数变量\(w\)和模型类别变量\(z\)）。</p>
<p>  <strong>EM算法是一种求解存在隐变量的参数优化问题的有效方法</strong>。既然不能直接最大化\(\ell(w)\)，我们可以不断的建立\(\ell\)的下界（E步），然后优化下界（M步）。</p>
</li>
<li><p><strong>公式推导</strong></p>
<p>  对于每一个样本\(x^{(i)}\)，这里用\(Q_i\)表示该样本隐变量\(z\)的某种分布，\(Q_i\)满足的条件\(\sum_{z} Q_i(z) = 1\)且\(Q_i(z)\ge 0\)。</p>
<blockquote>
<p>如果\(z\)是连续型的，那么\(Q_i\)就是概率密度函数，需要将求和符号换成积分符号。比如要将班上学生聚类，假设隐含变量\(z\)表示身高，那么可以认为该变量服从高斯分布。如果隐含变量表示性别，那么\(z\)就服从伯努利分布了。</p>
</blockquote>
<p>  根据前面的数学铺垫和公式\((ml.1.10.1)\)，可得下面的公式：</p>
<p>  $$<br>  \begin{align}<br>  \sum_{i=1}^{m} \log P(x^{(i)}; w) &amp; = \sum_{i=1}^{m} \log \left( \sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; w) \right) \qquad\qquad(1)\\<br>  &amp; = \sum_{i=1}^{m} \underline{ \log \left( \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{P(x^{(i)}, z^{(i)}; w)}{Q_i(z^{(i)})}\right) } \;\,\quad(2)\\<br>  &amp; \ge \sum_{i=1}^{m} \underline{\sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{P(x^{(i)}, z^{(i)}; w)}{Q_i(z^{(i)})} } \;\;\,\qquad(3)<br>   \end{align} \quad(ml.1.10.2)<br>  $$</p>
<blockquote>
<p>公式\((ml.1.10.2)\)解释：</p>
<p>(1)到(2)比较直接，分子分母同乘以一个相等的函数；(2)到(3)利用了Jensen不等式。这里面函数\(log(x)\)是凹函数（二阶导数小于0），并且</p>
<p>  $$<br>  \sum_{z^{(i)}} Q_i(z^{(i)}) \left[\frac{p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right] \qquad(n.ml.1.10.1)<br>  $$</p>
<p>就是\(\left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right]\)的期望。</p>
<p>设\(Y\)是随机变量\(X\)的函数，\(Y=g(X)\)（\(g\)是连续函数），那么有：</p>
<p>①. \(X\)是离散型随机变量，它的分布律为\(P(X=x_k) = p_k\)（\(k=1,2,\cdots\)）。若\(\sum_{k=1}^{\infty} g(x_k)p_k\)绝对收敛，则有</p>
<p>  $$<br>  E[Y] = E[g(X)] = \sum_{i=1}^{\infty} g(x_k) p_k  \qquad(n.ml.1.10.2)<br>  $$</p>
<p>②. \(X\)是连续型随机变量，其对应的概率密度为\(f(x)\)，若\(\int_{-\infty}^{\infty} g(x) f(x) dx\)绝对收敛，则有</p>
<p>  $$<br>  E[Y] = E[g(X)] = \int_{-\infty}^{\infty} g(x) f(x) dx \qquad(n.ml.1.10.3)<br>  $$</p>
<p>回到公式\((ml.1.10.2)\)，\(Y\)相当于\(\left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right]\)；\(X\)是\(z^{(i)}\)；\(Q_i(z^{(i)})\)是\(p_k\)； \(g\)是\(z^{(i)}\)到\(\left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right]\)的映射。</p>
<p>这样就解释了\((2)\)的期望，然后根据Jensen不等式，得到：</p>
<p>  $$<br>  f \left(E_{z^{(i)} \sim Q_i} \left[ \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right] \right) \ge E_{z^{(i)} \sim Q_i} \left[ f\left( \frac {p(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} \right)\right]  \quad(n.ml.1.10.4)<br>  $$</p>
</blockquote>
<p>  公式\((ml.1.10.2)-(3)\)可以看作是对对数似然函数\(\ell(w)\)求了下界。对于\(Q_i\)的选择，有很多种可能，那么哪种更好呢？</p>
</li>
<li><p><strong>E步</strong></p>
<p>  为了回答上面的问题，我们<strong>先假设参数\(w\)已经给定</strong>，那么\(\ell(w)\)的值就决定于\(Q_i(z^{(i)})\)和\(P(x^{(i)}, z^{(i)})\)了。这样我们就可以通过调整这两个概率使下界不断上升，以逼近\(\ell(w)\)的真实值。那么，什么时候算是调整好了呢？当公式\((ml.1.10.2)\)不等式变成等式时，说明我们调整后的结果等价于\(\ell(w)\)。</p>
<p>  按照上述思路，我们需要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，即：</p>
<p>  $$<br>  \frac {P(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} = c \quad（等式成立条件）\quad(ml.1.10.3)<br>  $$</p>
<p>  这里\(c\)是常数，不依赖\(z^{(i)}\)。对上式进一步推导，我们知道有\(\sum_{z} Q_i(z^{(i)})=1\)，那么就有\(\sum_{z} P(x^{(i)}, z^{(i)}; w) = c\)。进而可以得到：</p>
<p>  $$<br>  \begin{align}<br>  Q_i(z^{(i)}) &amp; = \frac {P(x^{(i)}, z^{(i)}; w)} {\sum_z P(x^{(i)}, z^{(i)}; w)} = \frac {P(x^{(i)}, z^{(i)}; w)} {P(x^{(i)};w)} \\<br>  &amp; = P(z^{(i)}|x^{(i)};w)<br>  \end{align}    \qquad(ml.1.10.4)<br>  $$</p>
<blockquote>
<p>这里，\(Q_i\)是第\(i\)个样本隐变量\(z\)的某种分布假设。通过建立\(\ell(w)\)的下界，并根据Jensen不等式的等式成立条件，推导出\(Q_i\)对应的计算公式就是后验概率\(P(z|x;w)\)。</p>
</blockquote>
<p>  到此，我么推出了在固定参数\(w\)后，\(Q_i(z^{(i)})\)的计算公式就是后验概率，解决了\(Q_i(z^{(i)})\)如何选择的问题。这一步称为E步：建立对数似然函数\(\ell(w)\)的下界。</p>
</li>
<li><p><strong>M步</strong></p>
<p>  接下来，在给定\(Q_i(z^{(i)})\)后，调整参数\(w\)，极大化\(\ell(w)\)的下界（固定\(Q_i\)后，下界还可以调整的更大）。因此，EM算法的步骤可描述如下（伪代码）：</p>
<blockquote>
<p>\(<br>  \begin{align}<br>  &amp; Loop \; until \; convergence \; \{ \\<br>  &amp; \qquad E－Step: for \; each \; i, \; calculate: \\<br>  &amp; \qquad\qquad\qquad\qquad \underline{ Q_i(z^{(i)}) := P(z^{i}|x^{(i)};w)} \qquad(n.ml.1.10.5)\\<br>  &amp; \qquad M－Step: calculate: \\<br>  &amp; \qquad\qquad\qquad \underline{ w := \arg \max_w \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{P(x^{(i)}, z^{(i)}; w)} {Q_i(z^{(i)})} } \quad(n.ml.1.10.6)\\<br>  &amp; \}<br>  \end{align}<br>  \)</p>
</blockquote>
<p>  伪代码中提到，直到收敛循环才退出。那么如何保证收敛？这是下一节要介绍的内容。</p>
</li>
</ul>
<p><br></p>
<h4 id="EM算法收敛性证明"><strong>EM算法收敛性证明</strong></h4><p>假定\(w^{(t)}\)和\(w^{(t+1)}\)是EM算法在第\(t\)次和第\(t+1\)次得到的结果。如果能够证明\(\ell(w^{(t)}) \le \ell(w^{(t+1)})\)，即极大似然估计单调递增，那么最终我们会达到最大似然估计的最大值。</p>
<ul>
<li><p>收敛性证明</p>
<blockquote>
<p>［第\(t\)次：E步］选定 \(w^{(t)}\)后，可以得到E步：</p>
<p>  $$<br>  Q_i^{(t)}(z^{(i)}) = P(z^{(i)} | x^{(i)}; w^{(t)}) \qquad(n.ml.1.10.7)<br>  $$</p>
<p>这一步保证了再给定\(w^{(t)}\)时，Jensen不等式中的等式成立，也就是</p>
<p>  $$<br>  \ell(w^{(t)}) = \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t)})} {Q_i^{(t)} (z^{(i)})} \qquad(n.ml.1.10.8)<br>  $$</p>
<p>［第\(t\)次：M步］然后进行M步，固定\(Q_i^{(t)}(z^{(i)}) \)，并将\(w^{(t)}\)视为变量，对上面的\(\ell(w^{(t)})\)求导后，得到\(w^{(t+1)}\)。经过一些推导，整理后会得到如下式子：</p>
<p>  $$<br>  \begin{align}<br>  \ell(w^{(t+1)}) &amp; \ge \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (x^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t+1)})} {Q_i^{(t)} (x^{(i)})} \quad(1)\\<br>  &amp; \ge \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (x^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t)})} {Q_i^{(t)} (x^{(i)})} \;\;\,\quad(2)\\<br>  &amp; = \ell(w^{(t)})<br>  \end{align} \qquad(n.ml.1.10.9)<br>  $$</p>
<p>公式\((n.ml.1.10.9)\)的\((1)\)，得到\(w^{(t+1)}\)时，只是最大化了\(\ell(w^{(t)})\)，也就是\(\ell(w^{(t+1)})\)的下界，而没有使等式成立。(在第\(t+1\)次的E步时等式才成立，即固定\(w^{(t+1)}\)，并按照Jensen不等式得到\(Q_i\)时。)</p>
<p>第\((1)\)步的成立可以根据公式\((ml.1.10.2)\)来论证。这里的第\((2)\)步直接利用了M步的定义，<strong>M步就是将\(w^{(t)}\)调整为\(w^{(t+1)}\)，使得下界最大化</strong>。</p>
</blockquote>
</li>
</ul>
<table>
<thead>
<tr>
<th>EM算法思想总结：</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>E步会将下界拉到与\(\ell(w)\)一个特定值(这里是\(w^{(t)}\))一样的高度，而此时发现下界仍然可以上升，因此经过M步后，下界又被拉升；但达不到与\(\ell(w)\)另外一个特定值(这里是\(w^{(t+1)}\))一样的高度，然后又进行E步…，如此重复下去，直到最大值。</strong></td>
</tr>
</tbody>
</table>
<p>  这样我们就证明了\(\ell(w)\)会单调增加。收敛方式可以用两种：一种是\(\ell(w)\)不再变化，另一种是\(\ell(w)\)变化幅度很小。</p>
<p>  为了更好的理解EM与其它优化算法的关系，下面我们定义一个公式：</p>
<p> $$<br>J(Q,w) = \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \cdot \log \frac{P(x^{(i)}, z^{(i)}; w^{(t)})} {Q_i^{(t)} (z^{(i)})} \qquad(ml.1.10.5)<br>$$</p>
<p>从前面的推导中我们知道了\(\ell(w) \ge J(Q,w)\)，那么EM算法可以看作是函数\(J\)的坐标上升法：<strong>E步固定w,优化Q；M步固定Q，优化w</strong>。</p>
<p><br></p>
<h3 id="高斯混合模型"><strong>高斯混合模型</strong></h3><p>密度估计(Density Estimation）是无监督学习非常重要的一个应用方向。本节主要介绍高斯混合模型（Gaussian Mixture Model，简称GMM）如何求解，进而做密度估计的？。</p>
<ul>
<li><p><strong>GMM介绍</strong></p>
<p>  假设给定训练样本\(\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}\)，且样本间独立，将隐含类别标签用\(z^{(i)}\)表示（即EM中的隐变量）。我们首先认为\(z^{(i)}\)是满足一定概率分布的（不是绝对的一个固定值），这里假设满足多项式分布：\(z^{(i)} \sim Miltinomial(\phi)\)，其中\(P(z^{(i)} = j) = \phi_j, \phi_j \ge 0, \sum_{j=1}^{k} \phi_j = 1，z^{(j)}\)有\(k\)个值\(\{1,2,\cdots,k\}\)可选。</p>
<p>  并且我们认为在给定\(z^{(i)}\)后，<strong>\(x^{(i)}\)满足多变量高斯分布</strong>，即\(x^{(i)}|z^{(i)}=j \sim N(\mu_j, \Sigma)\)。由此可以得到联合分布\(P(x^{(i)}, z^{(i)}) = P(x^{(i)}|z^{(i)}) \cdot P(z^{(i)})\)。</p>
<blockquote>
<p>多变量高斯分布介绍可参考《深入浅出ML》系列第05章高斯判别分析模型部分。</p>
</blockquote>
<p>  高斯混合模型可以描述为：</p>
<p>  <strong>对每个样本\(x^{(i)}\)，我们先从\(k\)个类别中按照多项式分布抽取一个\(z^{(i)}\)，然后根据\(z^{(i)}\)所对应的<strong>_\(k\)个多变量高斯分布中的一个_</strong>生成样本\(x^{(i)}\)。整个建模过程得到的模型称为高斯混合模型。</strong></p>
<p>  需要注意的是，这里的\(z^{(i)}\)仍然是隐随机变量（不同于高斯判别分析中的类别已知，监督学习）。模型中还有三个变量\(\phi, \mu\)和\(\Sigma\)。最大似然估计为联合概率分布\(P(x,z)\)，取对数后：</p>
<p>  $$<br>  \begin{align}<br>  \ell(\phi, \mu, \Sigma) &amp; = \sum_{i=1}^{m} \log \underline{ P(x^{(i)}; \phi, \mu, \Sigma) } \\<br>  &amp; = \sum_{i=1}^{m} \log \underline{ \left(\sum_{z^{(i)}=1}^{k} P(x^{(i)}|z^{(i)}; \mu, \Sigma) \cdot P(z^{(i)}; \phi)\right)}<br>  \end{align}    \qquad(ml.1.10.6)<br>  $$</p>
<blockquote>
<p>\(x^{(i)}\)是观测值，已经存在的。那么它是如何生成的呢？是以最大概率生成的… (MLE的思想)。</p>
</blockquote>
<p>  公式\((ml.1.10.6)\)的最大值不能通过高斯判别分析中直接求参数偏导并置为0的方法解决。但是我们假设已经知道了每个样本的标签\(z^{(i)}\)，那么公式\((ml.1.10.6)\)可以简化为：</p>
<p>  $$<br>  \ell(\phi, \mu, \Sigma) = \sum_{i=1}^{m} \left( \log P(x^{(i)}|z^{(i)}; \mu, \Sigma) + \log P(z^{(i)}; \phi) \right)  \qquad(ml.1.10.7)<br>  $$</p>
<p>  此时，我们再对\(\phi, \mu\)和\(\Sigma\)求导，可得：</p>
<p>  $$<br>  \begin{align}<br>  \phi_j &amp; = \frac{1}{m} \sum_{i=1}^{m} 1\{z^{(i)}=j\} \qquad\qquad(1)\\<br>  \mu_j &amp; = \frac{\sum_{i=1}^{m} 1\{z^{(i)}=j\}x^{(i)}} {\sum_{i=1}^{m} 1\{z^{(i)}=j\}} \quad\qquad(2) \\<br>  \Sigma_j &amp; = \frac {\sum_{i=1}^{m} 1\{z^{(i)}=j\} \cdot (x^{(i)} -\mu_j) (x^{(i)} -\mu_j)^T} {\sum_{i=1}^{m} 1\{z^{(i)}=j\}}  \quad(3)<br>  \end{align} \qquad(ml.1.10.8)<br>  $$</p>
<p>  \(\phi_j\)就是样本类别中\(z^{(i)}=j\)的占比。\(\mu_j\)是类别为\(j\)的样本特征均值，\(\Sigma_j\)是类别为\(j\)的样本的特征对应的协方差矩阵。</p>
<blockquote>
<p>公式\((2)\)中的\(\mu_j\)是一个\(n\)维向量：类标号为\(j\)的样本的特征累加和对应的均值。因此\(\mu = (\mu_1, \mu_2, \cdots, \mu_k)^T\)是一个\(k_n\)的矩阵。\(\Sigma_j\)是一个\(n_n\)的方阵，<strong>所有类标号为\(j\)的样本（特征集合）都服从相同参数\((\mu_j, \Sigma_j)\)的多变量高斯分布。</strong></p>
</blockquote>
</li>
<li><p><strong>高斯混合模型与高斯判别分析</strong></p>
<p>  实际上，当知道样本的隐含变量\(z^{(i)}\)（类标号）后，极大似然估计就等同于高斯判别分析模型了。从表面上看，二者不同的是GDA中的类别\(y\)服从伯努利分布，而这里的\(z\)服从多项式分布；此外，这里的每个样本都有不同的协方差矩阵，而GDA中只有一个。</p>
<p>  公式\((ml.1.10.6)\)与第5章中的\((ml.1.5.6)\)表达的是同一个意思。包括\((ml.1.10.8)与(ml.1.5.7)\)的求解结果，在物理意义也是等价的。 </p>
</li>
</ul>
<table>
<thead>
<tr>
<th>GMM与GDA相同点：</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>模型的概率假设相同</strong>：类别\(z^{(i)}\)服从多项式分布，样本特征服从多变量高斯分布；</td>
</tr>
<tr>
<td>2. <strong>目标函数一致</strong>：都是联合概率分布（在训练集上）的极大似然估计</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>GMM与GDA不同点：</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>学习方式不同</strong>：GDA是有监督学习，而GMM用于无监督学习；</td>
</tr>
<tr>
<td>2. <strong>求解算法不同</strong>：GDA直接通过MLE求参数偏导即可得到闭式解；GMM因为有隐变量，需要用EM算法不断迭代，得到结果。</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>GMM参数学习过程</strong></p>
<p>  公式\((ml.1.10.7)\)假设给定了\(z^{(i)}\)，而实际上\(z^{(i)}\)是不知道的，那怎么求解参数呢？</p>
<p>  回到EM算法思想：<strong>用E步猜测隐含变量\(z^{(i)}\)的分布；用M步更新其它参数，以获得更大的似然函数估计值。</strong></p>
<p>  因此用EM算法求解高斯混合模型的步骤如下（伪代码）：</p>
<blockquote>
<p>\(<br>  \begin{align}<br>  &amp; Loop \; until \; convergence \; \{ \\<br>  &amp; \qquad E－Step: for \; each \; i \; and \; j, \; calculate: \\<br>  &amp; \qquad\qquad\qquad\qquad w_j^{(i)} := P(z^{(i)}=j|x^{(i)}; \phi, \mu, \Sigma) \\<br>  &amp; \qquad M－Step: \; update \; parameters: \\<br>  &amp; \qquad\qquad\qquad\qquad \phi_j : = \frac{1}{m} \sum_{i=1}^{m} w_j^{(i)} \\<br>  &amp; \qquad\qquad\qquad\qquad \mu_j := \frac {\sum_{i=1}^{m} w_j^{(i)} x^{(i)}} {\sum_{i=1}^{m} w_j^{(i)}}  \\<br>  &amp; \qquad\qquad\qquad\qquad \Sigma_j := \frac {\sum_{i=1}^{m} w_j^{(i)} \cdot (x^{(i)} -\mu_j) (x^{(i)} -\mu_j)^T} {\sum_{i=1}^{m} w_j^{(i)}}\\<br>   &amp; \}<br>  \end{align}<br>  \)</p>
<p>这里，在E步中，我们将其它参数\(\phi, \mu, \Sigma\)看作常量，计算\(z^{(i)}\)的后验概率，也就是估计隐变量，即为观测样本”划分”类标号，更新每一个样本\(i\)可能的每一个类标号\(j\)的概率分布。</p>
<p><strong>M步是最大似然估计，通过更新参数实现</strong>。这里根据估计出来的属于每一个类的样本，重新计算属于这个类的参数。计算好后发现最大化似然估计时，\(w_j^{(i)}\)值又不对了，需要重新计算，如此循环，直至收敛。</p>
</blockquote>
<p>  参数\(w_j^{(i)}\)的计算公式如下：</p>
<p>  $$<br>  P(z^{(i)}=j|x^{(i)}; \phi, \mu, \Sigma) = \frac {P(x^{(i)}|z^{(i)}=j; \mu, \Sigma) \cdot P(z^{(i)}=j; \phi)} {\sum_{l=1}^{k} P(x^{(i)}|z^{(i)}=l; \mu, \Sigma) \cdot P(z^{(i)}=l; \phi)} \quad(ml.1.10.9)<br>  $$    </p>
<p>  \((ml.1.10.9)\)利用了贝叶斯公式。之所以是条件概率，是因为该式由Jensen不等式推导而来，通过建立似然函数的下界。</p>
<p>  在伪代码中使用了\(w_j^{(i)}\)代替了公式\((ml.1.10.8)\)中的\(1\{z^{(i)}=j\}\)，由0/1值变成了概率值。</p>
<p>  跟下面要介绍的经典聚类算法K-menas相比，<strong>这里使用类别“软”指定</strong>，为每个样例分配的类别\(z^{(i)}\)是有概率分布的，同时计算量也变大了，每个样例\(i\)都要计算属于每一个类别\(j\)的概率。与K-means相同的是，结果仍然是局部最优解。对其它参数取不同的初始值进行多次计算不失为一种好方法。</p>
</li>
</ul>
<p><br></p>
<h3 id="K-means聚类"><strong>K-means聚类</strong></h3><p>K-menas是聚类算法中最经典的一个。这里首先介绍K-means算法的流程，然后阐述其背后包含的EM思想。</p>
<ul>
<li><p><strong>关于聚类</strong></p>
<p>  聚类属于无监督学习，之前章节中的Regression，Naive Bayes，SVM等都是有类别标签的，也就是样本中已经给出了真实分类标签。而聚类样本中没有给定标签，只有特征（向量）。</p>
<blockquote>
<p>比如假设宇宙中的星星可以表示成三维空间中的点集\((x_1,x_2,x_3)\)。<strong>聚类的目的就是找个到每个样本\(\vec{x}\)潜在的类别\(y\)，并将同类别\(y\)的样本\(\vec{x}\)放在一起</strong>。比如上面的星星，聚类后结果是一个个星团，星团里面的点相互距离比较近，星团间的星星距离就比较远了。</p>
</blockquote>
<p>  在聚类问题中，训练样本是\(x^{(1)}, x^{(2)}, \cdots, x^{(m)}\)，每个\(x^{(i)} \in R^n\)，没有类别\(y^{(i)}\)。</p>
</li>
<li><p><strong>K-means算法流程</strong></p>
<p>  K-means算法是将样本聚类成k个簇，算法描述如下：</p>
<blockquote>
<p>\(\{ \\<br>  \qquad\, Step1: 随机选区k个聚类质心点(cluster centroids)，为\mu_1, \mu_2, \cdots, \mu_k \in R^n. \\<br>  \qquad Step2: Loop \; Until \; Convergence \{ \\<br>  \qquad\qquad 对于每一个样例i，计算其应该属于的类。计算公式 \\<br>  \qquad\qquad\qquad c^{(i)} := \arg \min_j |x^{(i)} - \mu_j|^2 \\<br>  \qquad\qquad 对于每一个类j，重新计算该类的质心 \\<br>  \qquad\qquad\qquad \mu_j := \frac{\sum_{i=1}^{m} 1\{c^{(i)}=j\}x^{(i)}} {\sum_{i=1}^{m} 1\{c^{(i)}=j\}}  \\<br>  \qquad \} \\<br>  \}<br>  \)</p>
</blockquote>
</li>
<li><p>\(Step1\)中的k是事先给定的聚类数，\(c^{(i)}\)代表样例\(i\)和\(k\)个类中距离较近的那个类。</p>
<p>  \(c^{(i)}\)的值为\({1,2,\cdots,k}\)。质心\(\mu_j\)表示我们对属于同一个类的样本中心点的猜测。以星团模型为例，就是要将所有的星星聚成\(k\)个星团，首先随机选取\(k\)个宇宙中的点（即\(k\)个星星）作为\(k\)个星团的质心。代码示例如下：</p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;iostream&gt;</span></span><br><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;cmath&gt;</span></span><br><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;vector&gt;</span></span><br><span class="line">	<span class="preprocessor">#<span class="keyword">include</span>&lt;ctime&gt;</span></span><br><span class="line">	<span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">typedef</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> uint;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">struct</span> Cluster</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; centroid;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; samples;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">vector</span>&lt;Cluster&gt; k_means(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; &gt; trainX, <span class="keyword">int</span> k, <span class="keyword">int</span> maxepoches)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">const</span> <span class="keyword">int</span> row_num = trainX.size();</span><br><span class="line">		<span class="keyword">const</span> <span class="keyword">int</span> col_num = trainX[<span class="number">0</span>].size();</span><br><span class="line"></span><br><span class="line">		<span class="comment">/*初始化聚类中心*/</span></span><br><span class="line">		<span class="built_in">vector</span>&lt;Cluster&gt; clusters(k);</span><br><span class="line">		<span class="keyword">int</span> seed = (<span class="keyword">int</span>)time(NULL);</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			srand(seed);</span><br><span class="line">			<span class="keyword">int</span> c = rand() % row_num;</span><br><span class="line">			clusters[i].centroid = trainX[c];</span><br><span class="line">			seed = rand();</span><br><span class="line">		&#125;</span><br><span class="line">		......</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">+ 然后，迭代中的每一步计算每个星星到k个质心中的每一个的距离，选取距离最近的那个星团作为\\(c^&#123;(i)&#125;\\)。</span><br><span class="line"></span><br><span class="line">	这样经过第一步每个星星都有了所属的星团；第二步对于每个星团，重新计算它的质心\\(\mu_j\\)（对里面所有的星星坐标求平均）。重复迭代第一步和第二步直至质心不变或变动很小。示例代码如下：</span><br></pre></td></tr></table></figure>
<p>  vector<cluster> k_means(vector<vector<double> &gt; trainX, int k, int maxepoches)<br>  {</vector<double></cluster></p>
<pre><code><span class="keyword">const</span> size_t row_num = trainX.size();
<span class="keyword">const</span> size_t col_num = trainX[<span class="number">0</span>].size();

<span class="comment">/*初始化聚类中心*/</span>
(在此省略)

<span class="comment">/*多次迭代直至收敛，这里固定迭代次数（100次）*/</span>
<span class="keyword">for</span> (<span class="keyword">int</span> it = <span class="number">0</span>; it &lt; maxepoches; it++)
{
    <span class="comment">/*每一次重新计算样本点所属类别之前，清空原来样本点信息*/</span>
    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)
    {
        clusters[i].samples.clear();
    }
    <span class="comment">/*求出每个样本点距应该属于哪一个聚类*/</span>
    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; row_num; j++)
    {
        <span class="comment">/*都初始化属于第0个聚类*/</span>
        <span class="keyword">int</span> c = <span class="number">0</span>;
        <span class="keyword">double</span> min_distance = cal_distance(trainX[j], clusters[c].<span class="keyword">centroid</span>);
        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; k; i++)
        {
            <span class="keyword">double</span> <span class="built_in">distance</span> = cal_distance(trainX[j], clusters[i].<span class="keyword">centroid</span>);
            <span class="keyword">if</span> (<span class="built_in">distance</span> &lt; min_distance)
            {
                min_distance = <span class="built_in">distance</span>;
                c = i;
            }
        }
        clusters[c].samples.push_back(j);
    }

    <span class="comment">/*更新聚类中心*/</span>
    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)
    {
        vector&lt;<span class="keyword">double</span>&gt; val(col_num, <span class="number">0.0</span>);
        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; clusters[i].samples.size(); j++)
        {
            <span class="keyword">int</span> <span class="keyword">sample</span> = clusters[i].samples[j];
            <span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; col_num; d++)
            {
                val[d] += trainX[<span class="keyword">sample</span>][d];
                <span class="keyword">if</span> (j == clusters[i].samples.size() - <span class="number">1</span>)
                {
                    clusters[i].<span class="keyword">centroid</span>[d] = val[d] / clusters[i].samples.size();
                }
            }
        }
    }
} <span class="comment">// 多次迭代，直至收敛</span>
<span class="keyword">return</span> clusters;
</code></pre><p>  }</p>
<p>  double cal_distance(vector<double> a, vector<double> b)<br>  {</double></double></p>
<pre><code>size_t da = a.size();
size_t db = b.size();

<span class="keyword">if</span> (da != db)
{
    cerr &lt;&lt; <span class="string">"Dimensions of two vectors must be same!\n"</span>;
}

double <span class="function"><span class="keyword">val</span> =</span> <span class="number">0.0</span>;
<span class="keyword">for</span> (size_t i = <span class="number">0</span>; i &lt; da; i++)
{
    <span class="function"><span class="keyword">val</span> <span class="title">+=</span> <span class="title">pow</span>(</span>(a[i] - b[i]), <span class="number">2</span>);
}
<span class="keyword">return</span> pow(<span class="function"><span class="keyword">val</span>, 0.5);</span>
</code></pre><p>  }</p>
</li>
</ul>
<ul>
<li><p><strong>K-means收敛性</strong></p>
<p>  k-means面对的第一个问题是如何保证收敛，算法描述中抢到结束条件就是收敛，可以证明的是k-means完全可以保证收敛。下面是定性的描述，先定义畸变函数（Distortion Function）如下：</p>
<p>  $$<br>  J(c,\mu) = \sum_{i=1}^{m} |x^{(i)} - \mu_{c^{(i)}}|^2 \qquad(ml.1.10.10)<br>  $$</p>
<p>  \(J\)函数表示每个样本点到其质心\(\mu\)的距离平方和。k-means的任务是将\(J\)调整到最小。</p>
<blockquote>
<p>假设当前\(J\)没有达到最小值，此时可以先固定每个类的质心\(\mu_j\)，调整每个样例所属的类别\(c^{(i)}\)来让\(J\)函数减小；同样的，固定\(c^{(i)}\)，调整每个类的质心\(\mu_j\)也可以使\(J\)减小。这两个过程就是<strong>内循环中使\(J\)单调递减的过程</strong>。当\(J\)递减到最小时，\(\mu\)和\(c\)也同时收敛。（理论上，可以存在多组不同的\(\mu\)和\(c\)值能够使得\(J\)取得最小值，但这种情况比较少见）。</p>
</blockquote>
<p>  由于畸变函数\(J\)是非凸函数，意味着我们不能保证取得的最小值是全局最小值，也就是k－menas对质心初始位置的选取比较敏感（其实在多数情况下k-means达到的局部最优已经满足需求，如果担心陷入局部最优，可以选区不同的初始值跑多遍k-means，然后去骑中最小的\(J\)对应的\(\mu\)和\(c\)输出）。</p>
</li>
<li><p><strong>K-means与EM算法</strong></p>
<p>  这里主要是想阐述清楚k-means和EM之间的关系。首先回到初始问题：我们的目标是将样本划分为\(k\)个类。其实说白了就是求每个样本\(x^{(i)}\)的隐含类别\(y^{(i)}\)，然后利用隐含类别\(y^{(i)}\)（有k个取值）将\(x^{(i)}\)归类。</p>
<p>  由于我们事先不知道类别\(y^{(i)}\)，那就先对每个样本假定一个\(y^{(i)}\)吧。此时新的问题来了：怎么知道假定的对不对？如何评价假定的好不好？我们可以使用极大似然估计来度量，这里就是\(x\)和\(y\)的联合分布\(P(x,y)\)。</p>
<blockquote>
<p>如果我们找到的\(y\)能够使\(P(x,y)\)最大，那么可以说\(y\)是样本\(x\)的最佳类别了，\(x\)归类为\(y\)就顺理成章了。但是我们第一次指定的\(y\)不一定会让\(P(x,y)\)最大。而且\(P(x,y)\)还依赖于其它未知参数，当然在给定\(y\)的情况下，我们可以调整其它参数让\(P(x,y)\)最大。但是调整参数后，我们发现有更好的\(y\)可以指定。那么我们重新指定\(y\)，然后再计算\(P(x,y)\)最大时的参数，反复迭代知道没有更好的\(y\)可以指定。</p>
</blockquote>
<p>  联合概率分布\(P(x,y)\)最大所对应的\(y\)就是\(x\)的最佳类别。</p>
</li>
</ul>
<p><br></p>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/EM算法/">EM算法</a><a href="/tags/K-means/">K-means</a><a href="/tags/高斯混合模型/">高斯混合模型</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter3-tree-based-family/" title="第03章：深入浅出ML之Tree-Based家族" itemprop="url">第03章：深入浅出ML之Tree-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-09-28T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-09-28</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-09-15</li>
<li>sina weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>决策树学习过程</li>
<li>特征选择方法</li>
<li>决策树解读</li>
<li>分类与回归树</li>
<li>随机森林</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><hr>
<p>本章我想以一个例子作为直观引入，来介绍决策树的结构、学习过程以及具体方法在学习过程中的差异。（注：构造下面的成绩示例数据，来说明决策树的构造过程）</p>
<blockquote>
<p>假设某次学生的考试成绩，第一列表示学生编号，第2列表示成绩，第3、4列分别划分两个不同的等级。数据如下表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">Score</th>
<th style="text-align:center">等级1</th>
<th style="text-align:center">等级2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">82</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">74</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">68</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">91</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">88</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">53</td>
<td style="text-align:center">较差</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">76</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">过关</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">62</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">58</td>
<td style="text-align:center">较差</td>
<td style="text-align:center">不过关</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">97</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">过关</td>
</tr>
</tbody>
</table>
<p>定义划分等级的标准：</p>
<p>“等级1”把数据划分为4个区间：</p>
<table>
<thead>
<tr>
<th style="text-align:center">分数区间</th>
<th style="text-align:center">[90, 100]</th>
<th style="text-align:center">[75, 90)</th>
<th style="text-align:center">[60, 75)</th>
<th style="text-align:center">[0, 60)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">等级1</td>
<td style="text-align:center">优秀</td>
<td style="text-align:center">良好</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">较差</td>
</tr>
</tbody>
</table>
<p>“等级2”的划分 假设这次考试，成绩超过75分算过关；小于75分不过关。得到划分标准如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">分数区间</th>
<th style="text-align:center">\(score \ge 75\)</th>
<th style="text-align:center">\(0 \le score \lt 75\)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">等级2</td>
<td style="text-align:center">过关</td>
<td style="text-align:center">不过关</td>
</tr>
</tbody>
</table>
</blockquote>
<p>我们按照树结构展示出来，如下图所示：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/head_first_ml/ml_3_0_1_decision_tree_graph.png" width="600" height="500" alt="Decision Tree"></p>
<p>如果按照“等级1”作为划分标准，取值为<code>“优秀”，“良好”，“中等”和“较差”</code>分别对应4个分支，如图4.1所示。由于只有一个划分特征，它对应的是一个单层决策树，亦称作“决策树桩”（Decision Stump）。</p>
<blockquote>
<p>决策树桩的特点是：只有一个非叶节点，或者说它的根节点等于内部节点（我们在下面介绍决策树多层结构时再介绍）。</p>
</blockquote>
<p>“等级1”取值类型是category，而在实际数据中，一些特征取值可能是连续值（如这里的score特征）。如果用决策树模型解决一些回归或分类问题的话，在学习的过程中就需要有将连续值转化为离散值的方法在里面，在特征工程中称为特征离散化。</p>
<blockquote>
<p>在图4.2中，我们把<strong>连续值划分为两个区域</strong>，分别是\(score \ge 75\) 和 \(0 \le score \lt 75\) </p>
</blockquote>
<p>图4.3和图4.4属于CART（Classification and Regression Tree，分类与回归树）模型。<strong>CART假设决策树是二叉树</strong>，根节点和内部节点的特征取值为”是”或”否”，节点的左分支对应”是”，右分支对应“否”，<strong>每一次划分特征选择都会把当前特征对应的样本子集划分到两个区域。</strong></p>
<blockquote>
<p>在CART学习过程中，不论特征原有取值是连续值（如图4.2）或离散值（图4.3，图4.4），也要转化为离散二值形式。</p>
</blockquote>
<p>直观上看，回归树与分类树的区别取决于实际的应用场景（回归问题还是分类问题）以及对应的“Label”取值类型。</p>
<blockquote>
<p><code>Label</code>是连续值，通常对应的是回归树；当<code>Label</code>是category时，对应分类树模型；</p>
</blockquote>
<p>后面会提到，CART学习的过程中最核心的是<strong>通过遍历选择最优划分特征及对应的特征值</strong>。那么二者的区别也体现在具体的最优划分特征的方法上。</p>
<p>同样，为了直观了解本章要介绍的内容，这里用下述表格来说明：</p>
<table>
<thead>
<tr>
<th style="text-align:center">决策树算法</th>
<th style="text-align:center">特征选择方法</th>
<th style="text-align:center">作者信息</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ID3</td>
<td style="text-align:center">信息增益</td>
<td style="text-align:center">Quinlan. 1986. <br>(Iterative Dichotomiser 迭代二分器)</td>
</tr>
<tr>
<td style="text-align:center">C4.5</td>
<td style="text-align:center">增益率</td>
<td style="text-align:center">Quinlan. 1993. </td>
</tr>
<tr>
<td style="text-align:center">CART</td>
<td style="text-align:center">回归树： 最小二乘<br> 分类树： 基尼指数</td>
<td style="text-align:center">Breiman. 1984. <br> (Classification and Regression Tree 分类与回归树)</td>
</tr>
</tbody>
</table>
<p>除了介绍这3个具体算法对应的特征选择方法外，还会简要地介绍决策树学习过程出现的<strong>模型以及数据问题</strong>，如过拟合问题，连续值和缺失值问题等。</p>
<p><br></p>
<h3 id="决策树学习过程">决策树学习过程</h3><hr>
<p><code>图4.1~图4.4</code>给出的仅仅是单层决策树，只有一个非叶节点（对应一个特征）。那么对于含有多个特征的分类问题来说，决策树的学习过程通常是_一个通过递归选择最优划分特征，并根据该特征的取值情况对训练数据进行分割，使得切割后对应的**_数据子集有一个较好的分类_**的过程_。</p>
<blockquote>
<p>为了更直观的解释决策树的学习过程，这里参考《数据挖掘－实用机器学习技术》一书中P69页提供的天气数据，根据天气情况决定是否出去玩，数据信息如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">ID</th>
<th>阴晴</th>
<th>温度</th>
<th>湿度</th>
<th>刮风</th>
<th style="text-align:center">玩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td>sunny</td>
<td>hot</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td>overcast</td>
<td>hot</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td>rainy</td>
<td>mild</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td>rainy</td>
<td>cool</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td>overcast</td>
<td>cool</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td>sunny</td>
<td>mild</td>
<td>high</td>
<td>false</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td>sunny</td>
<td>cool</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td>rainy</td>
<td>mild</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td>sunny</td>
<td>mild</td>
<td>normal</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td>overcast</td>
<td>mild</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td>overcast</td>
<td>hot</td>
<td>normal</td>
<td>false</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td>rainy</td>
<td>mild</td>
<td>high</td>
<td>true</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
</blockquote>
<p>利用ID3算法中的<strong>信息增益</strong>特征选择方法，递归的学习一棵决策树，得到树结构，如图4.5所示：</p>
<p></p><p style="text-align:center"><img src="https://raw.githubusercontent.com/ComputationalAdvertising/note-materials/master/img/head_first_ml/ml_3_1_1_decision_tree_info_gain.png" width="550" height="400" alt="ID3-决策树示意图"></p>
<p>假设 训练数据集\(D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(m)},y^{(m)}) \} \) (特征用离散值表示)，候选特征集合\(F=\{f^1, f^2, \cdots, f^n\} \)。开始，建立根节点，将所有训练数据都置于根节点（\(m\)条样本）。从特征集合\(F\)中选择一个最优特征\(f^{\ast}\)，按照\(f^{\ast}\)取值将训练数据集切分成若干子集，使得各个数据子集有一个在当前条件下最好的分类。</p>
<p>如果子集中样本类别基本相同，那么构建叶节点，并将数据子集划分给对应的叶节点；如果子集中样本类别差异较大，不能被基本正确分类，需要在剩下的特征集合（\(F-\{f^{\ast}\}\)）中选择新的最优特征，构建相应的内部节点，继续对数据子集进行切分。如此递归地进行下去，直至所有数据子集都能被基本正确分类，或者没有合适的最优特征为止。</p>
<p>这样，最终结果是每个子集都被分到叶节点上，对应着一个明确的类别。那么，递归生成的层级结构即为一棵决策树。我们将上面的文字描述用伪代码形式表达出来，如下：</p>
<p>\(<br>\{ \\<br>\quad输入: 训练数据集D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(m)},y^{(m)}) \} \; (特征用离散值表示); \\<br>\qquad\quad\; 候选特征集F=\{f^1, f^2, \cdots, f^n\} \\<br>\quad输出：一颗决策树T(D,F) \\<br>\quad学习过程：\\<br>\qquad 01. \;\; 创建节点node; \\<br>\quad\;\;\;02. \;\; if \; D中样本全属于同一类别C； \; then \\<br>\qquad 03. \qquad 将node作为叶节点，用类别C标记，返回； \\<br>\qquad 04. \; endif \\<br>\qquad 05. \;\; if \; F为空（F=\emptyset）or \; D中样本在F的取值相同；\; then \\<br>\qquad 06. \qquad 将node作为叶节点，其类别标记为D中样本数最多的类（多数表决），返回； \\<br>\qquad 07. \;\underline{ 选择F中最优特征，得到f^{\ast}(f^{\ast} \in F) }； \\<br>\qquad 08. \;标记节点node为f^{\ast} \\<br>\qquad 09. \;\; for \; f^{\ast} \;中的每一个已知值f_{i}^{\ast}; \; do \\<br>\qquad 10. \quad\;\; 为节点node生成一个分支；令D_i表示D中在特征f^{\ast}上取值为f_i^{\ast}的样本子集； \; //划分子集 \\<br>\qquad 11. \quad\;\; if \; D_i为空；\; then \\<br>\qquad 12. \qquad\quad 将分支节点标记为叶节点，其类别标记为D_i中样本最多的类；\; then \\<br>\qquad 13. \quad\;\; else \\<br>\qquad 14. \qquad\quad 以T(D_i, F-\{f^{\ast}\})为分支节点；\quad // 递归过程 \\<br>\qquad 15. \quad\;\; endif \\<br>\qquad 16. \; done<br>\\\}<br>\)</p>
<p>决策树学习过程中递归的每一步，在选择最优特征后，根据特征取值切割当前节点的数据集，得到若干数据子集。由于决策树学习过程是递归地选择最优特征，因此可以理解为这是一个<strong>特征空间划分</strong>的过程。每一个特征子空间对应决策树中的一个叶子节点，特征子空间相应的类别就是叶子节点对应数据子集中样本数最多的类别。</p>
<blockquote>
<p>决策树学习过程可以简要的概括为：通过递归地选择最优特征，实现在整个特征空间维度对样本的划分（每个空间对应一个类别），进而完成了整个分类过程。</p>
</blockquote>
<p><br></p>
<h3 id="特征选择方法"><strong>特征选择方法</strong></h3><hr>
<p>上面多次提到递归地选择最优特征，根据特征取值切割数据集，使得对应的数据子集有一个较好的分类。从伪代码中也可以看出，在决策树学习过程中，最重要的是第07行，即如何选择最优特征？也就是我们常说的特征选择问题。</p>
<p>顾名思义，<strong>特征选择就是将特征的重要程度进行量化后，得到的结果再进行选择</strong>，而如何量化特征的重要性，就成了各种方法间最大的区别。</p>
<blockquote>
<p>例如卡方检验、斯皮尔曼法（Spearman）、<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/#互信息" target="_blank" rel="external">互信息</a>等使用<code>&lt;feature, label&gt;</code>之间的<strong>关联性</strong>来进行量化<code>feature</code>的重要程度。关联性越强，特征得分越高，该特征越应该被优先选择。</p>
</blockquote>
<p>决策树中，我们希望随着特征选择过程地不断进行，决策树的分支节点所包含的样本尽可能属于<br>同一类别，即<strong>希望节点的”纯度（purity）”越来越高</strong>。</p>
<blockquote>
<p>如果子集中的样本都属于同一个类别，当然是最好的结果；如果说大多数的样本类型相同，只有少部分样本不同，也可以接受。</p>
</blockquote>
<p>那么如何才能做到选择的特征对应的样本子集纯度最高呢？</p>
<p>ID3算法用<strong>信息增益</strong>来刻画样例集的纯度; C4.5算法采用<strong>增益率</strong>; CART算法采用<strong>基尼指数</strong>来刻画样例集纯度。</p>
<p><br></p>
<h4 id="信息增益"><strong>信息增益</strong></h4><hr>
<p>信息增益（Information Gain，简称IG）衡量特征的重要性是根据<strong>当前特征为划分带来多少信息量，带来的信息越多，该特征就越重要，此时节点的”纯度”也就越高。</strong></p>
<p>分类系统的信息熵，我们在<a href="http://www.52caml.com/head_first_ml/ml-chapter2-entropy-based-family/#熵与信息熵" target="_blank" rel="external">第02章：熵与信息熵</a>部分已经给出计算公式，这里再复习一下：</p>
<blockquote>
<p>对一个分类系统来说，假设类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别总数），每一个类别出现的概率分别是\(p(c_1),p(c_2), \cdots, p(c_k)\)。此时，分类系统的熵可以表示为:</p>
<p>$$<br>H(C ) = - \sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) \qquad (n.ml.1.3.1)<br>$$</p>
<p>分类系统的作用就是输出一个特征向量（文本特征、ID特征、特征特征等）属于哪个类别的值，而这个值可能是\(c_1, c_2, \cdots, c_k\)，因此这个值所携带的信息量就是公式\((n.ml.1.3.1)\)这么多。</p>
</blockquote>
<p>假设离散特征\(t\)的取值有\(I\)个，\(H(C|t=t_i)\) 表示特征\(t\)被取值为\(t_i\)时的条件熵；\(H(C|t)\)是指特征\(t\)被固定时的条件熵。二者之间的关系是：</p>
<blockquote>
<p>$$<br>\begin{align}<br>H(C|t) &amp; = p_1 \cdot H(C|t=t_1) + p_2 \cdot H(C|t=t_2) + \cdots + p_k \cdot H(C|t=t_{n}) \\<br>&amp; = \sum_{i=1}^{I} p_i \cdot H(C|t=t_i)<br>\end{align}        \quad (n.ml.1.3.2)<br>$$</p>
<p>假设总样本数有\(m\)条，特征\(t=t_i\)时的样本数\(m_i\)，\(p_i=\frac{m_i}{m}\).</p>
</blockquote>
<p>接下来，如何求\(P(C|T=t_i)？\)</p>
<blockquote>
<p>以二分类为例（正例为1，负例为0），总样本数为\(m\)条，特征\(t\)的取值为\(I\)个，其中特征\(t=t_i\)对应的样本数为\(m_i\)条，其中正例\(m_{i1}\)条，负例\(m_{i0}\)条（即\(m_i=m_{i0} + m_{i1}\)）。那么有：</p>
<p>$$<br>\begin{align}<br>P(C|T=t_i) &amp; = - \frac{m_{i1}}{m_i} \cdot log_{2} \frac{m_{i1}}{m_i} - \frac{m_{i0}}{m_i} \cdot log_{2} \frac{m_{i0}}{m_i} \\<br>&amp; = -\sum_{j=0}^{k-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i}<br>\end{align} \qquad (n.ml.1.3.3)<br>$$</p>
<p>这里\(k=2\)表示分类的类别数，公式\(\frac{m_{ij}}{m_i}\)物理含义是当\(t=t_i\)且\(C=c_j\)的概率，即条件概率\(p(c_j|t_i)\)。</p>
<p>因此，条件熵计算公式为：</p>
<p>$$<br>\begin{align}<br>H(C|t) &amp; = \sum_{i=1}^{I} p(t_i) \cdot H(C|t=t_i) \\<br>&amp; = - \sum_{i=1}^{I} p(t_i) \cdot \underline { \sum_{j=0}^{k-1} p(c_j|t_i) \cdot log_2 p(c_j|t_i) } \\<br>&amp; = - \sum_{i=1}^{I} \sum_{j=o}^{k-1} p(c_j,t_i) \cdot log_2 p(c_j|t_i)<br>\end{align} \qquad (n.ml.1.3.4)<br>    $$</p>
</blockquote>
<p>特征\(t\)给系统带来的信息增益等于<strong>系统原有的熵与固定特征\(t\)后的条件熵之差</strong>，公式表示如下:</p>
<p>$$<br>\begin{align}<br>IG(T) &amp; = H(C) - H(C|T) \\<br>&amp; = -\sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) + \sum_{i=1}^{n} \sum_{j=1}^{k} p(c_j,t_i) \cdot \log_2 p(c_j|t_i)<br>\end{align}  \qquad(ml.1.3.1)<br>    $$</p>
<p>\(n表示特征t取值个数，k表示类别C个数，\sum_{j=0}^{n-1} \frac{m_{ij}}{m_i} \cdot log_{2} \frac{m_{ij}}{m_i}表示每一个类别对应的熵。\)</p>
<p>下面以天气数据为例，介绍通过信息增益选择最优特征的工作过程：</p>
<blockquote>
<p>根据阴晴、温度、湿度和刮风来决定是否出去玩。样本中总共有14条记录，取值为“是”和“否”的yangebnshu分别是9和5，即9个正样本、5个负样本，用\(S(9+,5-)\)表示，S表示样本(sample)的意思。<br> <br></p>
<p>(1). 分类系统的熵:</p>
<p>$$<br>Entropy(S) = info(9,5) = -\frac{9}{14} _ log_2 (\frac{9}{14}) - \frac{5}{14} _ log_2 (\frac{5}{14}) = 0.940位    \quad(exp.1.3.1)<br>$$</p>
<p>(2). 如果以特征”阴晴”作为根节点。“阴晴”取值为{sunny, overcast, rainy}, 分别对应的正负样本数分别为(2+,3-), (4+,0-), (3+,2-)，那么在这三个节点上的信息熵分别为：</p>
<p>$$<br>\begin{align}<br>&amp; Entropy(S| “阴晴”=sunny) = info(2,3) = 0.971位  \quad(exp.1.3.2.1) \\<br>&amp; Entropy(S| “阴晴”=overcast) = info(4,0) = 0位  \;\;\quad(exp.1.3.2.2) \\<br>&amp; Entropy(S| “阴晴”=rainy) = info(3,2) = 0.971位  \;\quad(exp.1.3.2.3)<br>\end{align}<br>$$</p>
<p>以特“阴晴”为根节点，平均信息值（即条件熵）为：<br></p>
<p>$$<br>Entropy(S| “阴晴”) = \frac{5}{14} _ 0.971 + \frac{4}{14} _ 0 + \frac{5}{14} * 0.971 = 0.693位 \quad (exp.1.3.2)<br>$$</p>
<p>以特征\( “阴晴”\)为条件，计算得到的条件熵代表了期望的信息总量，即对于一个新样本判定其属于哪个类别所必需的信息量。<br><br><br>(3). 计算特征\( “阴晴”\)对应的信息增益:</p>
<p>$$<br>IG( “阴晴”) = Entropy(S) - Entropy(S| “阴晴”) = 0.247位 \quad(exp.1.3.3.1)<br>$$</p>
<p>同样的计算方法，可得每个特征对应的信息增益，即</p>
<p>$$<br>IG(“刮风”) = Entropy(S) - Entropy(S|“刮风”) = 0.048位 \qquad\qquad(exp.1.3.3.2) \\<br>IG(“湿度”) = Entropy(S) - Entropy(S|“湿度”) = 0.152位 \qquad\qquad(exp.1.3.3.3) \\<br>IG(“温度”) = Entropy(S) - Entropy(S|“温度”) = 0.029位 \qquad\qquad(exp.1.3.3.4)<br>$$</p>
</blockquote>
<p>显然，特征“阴晴”的信息增益最大，于是把它作为划分特征。基于“阴晴”对根节点进行划分的结果，如图4.5所示（决策树学习过程部分）。决策树学习算法对子节点进一步划分，重复上面的计算步骤。</p>
<p>用信息增益选择最优特征，并不是完美的，存在问题或缺点主要有以下两个：</p>
<ul>
<li><p><strong>倾向于选择拥有较多取值的特征</strong></p>
<blockquote>
<p>尤其特征集中包含ID类特征时，ID类特征会最先被选择为分裂特征，但在该类特征上的分支对预测未知样本的类别并无意义，降低了决策树模型的泛化能力，也容易使模型易发生过拟合。</p>
</blockquote>
</li>
<li><p><strong>只能考察特征对整个系统的贡献，而不能具体到某个类别上</strong></p>
<blockquote>
<p>信息增益只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而无法做“本地”的特征选择（对于文本分类来讲，每个类别有自己的特征集合，因为有的词项（word item）对一个类别很有区分度，对另一个类别则无足轻重）。</p>
</blockquote>
</li>
</ul>
<p>为了弥补信息增益这一缺点，一个被称为<strong>增益率（Gain Ratio）</strong>的修正方法被用来做最优特征选择。</p>
<p><br>     </p>
<h4 id="增益率"><strong>增益率</strong></h4><hr>
<p>与信息增益不同，信息增益率的计算考虑了<strong>特征分裂数据集后所产生的子节点的数量和规模，而忽略任何有关类别的信息</strong>。</p>
<blockquote>
<p>以信息增益示例为例，按照特征“阴晴”将数据集分裂成3个子集，规模分别为5、4和5，因此不考虑子集中所包含的类别，产生一个分裂信息为：</p>
<p>$$<br>SplitInfo(“阴晴”) = info(5,4,5) = 1.577位 \qquad(exp.1.3.4)<br>$$</p>
<p>分裂信息熵（Split Information）可简单地理解为<strong>表示信息分支所需要的信息量</strong>。 <br><br>那么信息增益率：</p>
<p>$$<br>IG_{ratio}(T) = \frac{IG(T)}{SplitInfo(T)} \qquad(n.ml.1.3.5)<br>$$</p>
<p>在这里，特征 “阴晴”的信息增益率为\(IG_{ratio}( “阴晴”)=\frac{0.247}{1.577} = 0.157\)。减少信息增益方法对取值数较多的特征的影响。</p>
</blockquote>
<p>基尼指数（Gini Index）是CART中分类树的特征选择方法。这部分会在下面的“分类与回归树－二叉分类树”一节中介绍。</p>
<p><br></p>
<h3 id="分类与回归树">分类与回归树</h3><hr>
<p>分类与回归树（Classification And Regression Tree, 简称CART）模型在Tree-Based家族中是应用最广泛的学习方法之一。它既可以用于分类也可以用于回归，在<a href="http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/" target="_blank" rel="external">第06章：Boosting家族</a>的核心成员－<a href="http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/#Gradient_Boosting" target="_blank" rel="external">Gradient Boosting</a>就是以该模型作为基本学习器(base learner)。</p>
<p>一句话概括CART模型：</p>
<table>
<thead>
<tr>
<th>CART模型是在给定输入随机变量\(X\)条件下求得输出随机变量\(Y\)的<strong>条件概率分布</strong>的学习方法。</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p>在“写在前面”也提到，CART假设决策树时二叉树结构，内部节点特征取值为“是”和“否”，左分支对应取值为“是”的分支，右分支对应为否的分支，如图4.3所示。这样CART学习过程等价于递归地二分每个特征，将输入空间（在这里等价特征空间）划分为有限个字空间（单元），并在这些字空间上确定预测的概率分布，也就是在输入给定的条件下输出对应的条件概率分布。</p>
<blockquote>
<p>可以看出CART算法在叶节点表示上不同于ID3、C4.5方法，后二者叶节点对应数据子集通过“多数表决”的方式来确定一个类别（固定一个值）；而CART算法的叶节点对应类别的概率分布。如此看来，我们可以很容易地用CART来学习一个<code>multi-label / multi-class / multi-task</code>的分类任务。</p>
</blockquote>
<p>与其它决策树算法学习过程类别，CART算法也主要由两步组成：</p>
<ul>
<li>决策树的生成：基于训练数据集生成一棵<strong>二分决策树</strong>；</li>
<li>决策树的剪枝：用验证集对已生成的二叉决策树进行剪枝，剪枝的标准为损失函数最小化。</li>
</ul>
<p>由于分类树与回归树在递归地构建二叉决策树的过程中，选择特征划分的准则不同。二叉分类树构建过程中采用<strong>基尼指数（Gini Index）</strong>为特征选择标准；二叉回归树采用<strong>平方误差最小化</strong>作为特征选择标准。</p>
<p><br></p>
<h4 id="二叉分类树">二叉分类树</h4><hr>
<p>二叉分类树中用基尼指数（Gini Index）作为最优特征选择的度量标准。基尼指数定义如下：</p>
<p>同样以分类系统为例，数据集\(D\)中类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别数），一个样本属于类别\(c_i\)的概率为\(p(i)\)。那么<strong>概率分布的基尼指数</strong>公式表示为：</p>
<p>$$<br>Gini(D) = 1 - \sum_{i=1}^{k} {p_i}^2    \qquad(ml.1.3.2)<br>$$</p>
<blockquote>
<p>其中\(p_i = \frac{类别属于c_i的样本数}{总样本数}\)。如果所有的样本类别相同，则\(p_1 = 1, p_2 = p_3 = \cdots = p_k = 0\)，则有\( Gini(C)=0\)，此时数据不纯度最低。\(Gini(D)\)的物理含义是表示数据集\(D\)的不确定性。数值越大，表明其不确定性越大（这一点与信息熵相似）。</p>
<p>如果\(k=2\)（二分类问题，类别命名为正类和负类），若样本属于正类的概率是\(p\)，那么对应基尼指数为：</p>
<p>$$<br>Gini(D) = 2 p (1-p) \qquad\qquad (n.ml.1.3.6)<br>$$</p>
</blockquote>
<p>如果数据集\(D\)根据特征\(f\)是否取某一可能值\(f_{\ast}\)，将\(D\)划分为\(D_1\)和\(D_2\)两部分，即\(D_1=\{(x, y) \in D | f(x) = f_{\ast}\}, D_2=D-D_1\)。那么特征\(f\)在数据集\(D\)基尼指数定义为：</p>
<p>$$<br>Gini(D, f=f_{\ast}) = \frac{\vert D_1 \vert}{\vert D \vert} Gini(D_1) + \frac{\vert D_2 \vert}{\vert D \vert} Gini(D_2) \qquad\qquad (ml.1.3.3)<br>$$</p>
<p>在实际操作中，<strong>通过遍历所有特征（如果是连续值，需做离散化）及其取值</strong>，选择基尼指数最小所对应的特征和特征值。</p>
<p>这里仍然以天气数据为例，给出特征“阴晴”的基尼指数计算过程。</p>
<blockquote>
<p>(1). 当特征“阴晴”取值为”sunny”时，\(D_1 = \{1,2,8,9,11\}, |D_1|=5\); \(D_2=\{3,4,5,6,7,10,12,13,14\}, |D_2|=9\)。\(D_1、D_2\)数据自己对应的类别数分别为\((+2,-3)、(+7,-2)\)。因此\(Gini(D_1) = 2 \cdot \frac{3}{5} \cdot \frac{2}{5} = \frac{12}{25} \)；\(Gini(D_2) = 2 \cdot \frac{7}{9} \cdot \frac{2}{9} = \frac{28}{81}\). 对应的基尼指数为：</p>
<p>$$<br>Gini(C, “阴晴”=”sunny”) = \frac{5}{14} Gini(D_1) + \frac{9}{14} Gini(D_2) = \frac{5}{14} \frac{12}{25} + \frac{9}{14} \frac{28}{81} = 0.394 \quad(exp.1.3.5)<br>$$</p>
<p>(2). 当特征“阴晴”取值为”overcast”时，\(D_1 = \{2,7,12,13\}, |D_1|=4\); \(D_2=\{1,2,4,5,6,8,9,10,11,14\}, |D_2|=10\)。\(D_1、D_2\)数据自己对应的类别数分别为\((+4,-0)、(+5,-5)\)。因此\(Gini(D_1) = 2 \cdot 1 \cdot 0 = 0；Gini(D_2) = 2 \cdot \frac{5}{10} \cdot \frac{5}{10} = \frac{1}{2}\). 对应的基尼指数为：</p>
<p>$$<br>Gini(C, “阴晴”=”sunny”) = \frac{4}{14} Gini(D_1) + \frac{10}{14} Gini(D_2) = 0 + \frac{10}{14} \cdot \frac{1}{2} = \frac{5}{14} = 0.357 \quad(exp.1.3.6)<br>$$</p>
<p>(3). 当特征“阴晴”取值为”rainy”时，\(D_1 = \{4,5,6,10,14\}, |D_1|=5\); \(D_2=\{1,2,3,7,8,9,11,12,13\}, |D_2|=9\)。\(D_1、D_2\)数据自己对应的类别数分别为\((+3,-2)、(+6,-3)\)。因此\(Gini(D_1) = 2 \cdot \frac{3}{5} \cdot \frac{2}{5} = \frac{12}{25} \)；\(Gini(D_2) = 2 \cdot \frac{6}{9} \cdot \frac{3}{9} = \frac{4}{9}\). 对应的基尼指数为：</p>
<p>$$<br>Gini(C, “阴晴”=”sunny”) = \frac{5}{14} Gini(D_1) + \frac{9}{14} Gini(D_2) = \frac{5}{14} \frac{12}{25} + \frac{9}{14} \frac{4}{9} = \frac{4}{7} = 0.457 \quad(exp.1.3.7)<br>$$</p>
</blockquote>
<p>如果特征”阴晴”是最优特征的话，那么特征取值为”overcast”应作为划分节点。</p>
<p><br></p>
<h4 id="二叉回归树">二叉回归树</h4><hr>
<p>二叉回归树采用<strong>平方误差最小化</strong>作为特征选择和切分点选择的依据。一棵回归树对应着特征空间的若干个划分及其在划分单元上的输出值。假设将特征空间划分为\(J\)个单元（子空间），分别是\(\{R_1, R_2, \cdots, R_J\}\)，在每个单元\(R_j\)（对应回归树的一个叶子节点）上有一个固定的输出值\(v_j\)（连续变量）。给定训练数据集\(D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})\}\)，二叉回归树模型可表示为：</p>
<p>$$<br>f(x) = \sum_{j=1}^{J} v_j \cdot I(x \in R_j) \qquad (ml.1.3.4)<br>$$</p>
<blockquote>
<p>\(I(x \in R_j)\)为指示函数，如果输入样本\(x\)属于 \(R_j\)字空间，指示函数为1，对应的输出结果为\(v_j\)。<strong>每个输入\(x\)仅对应一个划分单元（字空间）</strong>。</p>
</blockquote>
<p>当特征空间的划分确定时，用平方误差来表示二叉回归树模型对于训练数据的预测误差，即表示如下：</p>
<p>$$<br>\hat{v_j} := \arg \min \sum_{x^{(i)} \in R_j} \left(y^{(i)} - f(x^{(i)}) \right)^2 \qquad(ml.1.3.5)<br>$$</p>
<p>每个单元的最优输出值可通过最小化平方误差求得。易知，划分单元\(R_j\)上的\(v_j\)的最优值\(\hat{v_j}\)是\(R_j\)上的所有输入样本\(x^{(i)}\)对应的输出\(y^{(i)}\)的均值，即\(\hat{c_j} = avg(y^{(i)} | x^{(i)} \in R_j)\)。</p>
<p>下面要解决的问题是：如何划分特征空间？</p>
<p>一个启发式的方式就是选择特征空间中第\(k\)个特征\(f_k\)和它的取值\(s\),作为划分特征和划分点。定义两个区域（对应内部节点两个分支）：</p>
<blockquote>
<p>$$<br>R_1(k, s) = \{x \,|\, f_k \le s\}, \quad R_2(k, s) = \{x \,|\, f_k \gt s\} \qquad(n.ml.1.3.7)<br>$$</p>
</blockquote>
<p>然后寻找最优划分特征\(f_k\)和最优划分点\(s\)。具体操作就是<strong>遍历所有未划分过的特征集合和对应的取值（集合）</strong>，求解：</p>
<p>$$<br>\min_{f_k, \, s} \left[ \min_{v_1} \sum_{x^{(i)} \in R_1(f_k, \, s)} (y^{(i)} - v_1)^2 + \min_{v_2} \sum_{x^{(i)} \in R_2(f_k, \, s)} (y^{(i)} - v_2)^2\right] \qquad(ml.1.3.6)<br>$$</p>
<p>第1步：固定特征\(f_k\)，最小化<code>[ ... ]</code>里的式子可得最优划分点\(s\)。即：</p>
<blockquote>
<p>$$<br>\hat{c_1} = avg (y^{(i)} \,|\, x^{(i)} \in R_1(f_k, s)), \quad \hat{c_2} = avg (y^{(i)} \,|\, x^{(i)} \in R_2(f_k, s)) \qquad(n.ml.1.3.8)<br>$$</p>
</blockquote>
<p>第1步：遍历所有特征集合，寻找最优的划分特征，构成\((f_k, s)\)对。这样就可生成一个内部节点，对应的特征\(f_k\)和划分值\(s\)。依此将特征空间划分为两个区域（对应两个数据子集）。</p>
<p>对每个子区域重复1、2两步的划分过程，直到满足划分停止条件为止（一般的，特征集合\(F = \emptyset\)时）。如此就得到一颗回归树，亦称为最小二乘回归树（Least Squares Regression Tree）。</p>
<p><br></p>
<h3 id="NEXT_\(\cdots\)">NEXT \(\cdots\)</h3><hr>
<ul>
<li>决策树解读</li>
<li>随机森林</li>
</ul>
<p><br></p>
<h3 id="参考资料">参考资料</h3><hr>
<ul>
<li>《机器学习导论》</li>
<li>《机器学习》－周志华</li>
<li>《统计学习方法》</li>
<li>《数据挖掘－实用机器学习技术》</li>
</ul>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/C4-5/">C4.5</a><a href="/tags/CART/">CART</a><a href="/tags/Decision-Stump/">Decision Stump</a><a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/ID3/">ID3</a><a href="/tags/IG/">IG</a><a href="/tags/Random-Forest/">Random Forest</a><a href="/tags/信息增益/">信息增益</a><a href="/tags/决策树/">决策树</a><a href="/tags/决策树桩/">决策树桩</a><a href="/tags/分类与回归树/">分类与回归树</a><a href="/tags/随机森林/">随机森林</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/head_first_ml/ml-chapter2-entropy-based-family/" title="第02章：深入浅出ML之Entropy-Based家族" itemprop="url">第02章：深入浅出ML之Entropy-Based家族</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-09-05T13:45:02.000Z" itemprop="datePublished"> 发表于 2015-09-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-08-04</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>内容列表</strong></p>
<ul>
<li>写在前面</li>
<li>基本概念<ul>
<li>熵与信息熵</li>
<li>条件熵</li>
<li>联合熵</li>
<li>相对熵、KL距离</li>
<li>互信息</li>
</ul>
</li>
<li>最大熵模型（Maximum Entropy Model） <ul>
<li>最大熵原理</li>
<li>最大熵模型定义</li>
<li>最大熵模型参数学习</li>
<li>对偶函数极大化与极大似然估计等价</li>
</ul>
</li>
<li>参数学习的最优化算法</li>
</ul>
<p><br></p>
<h3 id="写在前面">写在前面</h3><p>记得在《Pattern Recognition And Machine Learning》一书中的开头有讲到：“概率论、决策论、信息论3个重要工具贯穿着《PRML》整本书，虽然看起来令人生畏…”。确实如此，其实这3大理论在机器学习的每一种技法中，或多或少都会出现其身影（不局限在概率模型）。</p>
<blockquote>
<p>《PRML》书中原话：”This chapter also provides a self-contained introduction to <strong>three important tools that will be used throughout the book, namely probability theory, decision theory, and information theory</strong>. Although these might sound like daunting topics, they are in fact straightforward, and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications.”</p>
<p>怀念好学生时代：<a href="http://photo.weibo.com/1707438033/albums/detail/album_id/3942924873266495#!/mode/2/page/1" target="_blank" rel="external">那些年－书本啃过的印记</a></p>
</blockquote>
<p>本章主要讨论《信息论》(Information Theory)中一个非常重要的概念：<strong>信息熵</strong>，以及概率模型的一个学习准则：<strong>最大熵理论</strong>。</p>
<p><br></p>
<h3 id="基本概念">基本概念</h3><p><br></p>
<h4 id="熵与信息熵">熵与信息熵</h4><ul>
<li><p>如何理解熵的含义？</p>
<p>  自然界的事物，如果任其自身发展，最终都会达到尽可能的平衡或互补状态。举例：</p>
<blockquote>
<p>一盒火柴，（人为或外力）有序地将其摆放在一个小盒子里，如果不小心火柴盒打翻了，火柴会“散乱”地洒在地板上。此时火柴虽然很乱，但这是它自身发展的结果。</p>
</blockquote>
<p>  上面描述的其实是自然界的熵。在自然界中，熵可以这样表述：</p>
<p>  <strong>熵是描述事物无序性的参数，熵越大则无序性越强。</strong></p>
<p>  那么，在信息论中，我们用熵表示一个随机变量的不确定性，那么如何量化信息的不确定性呢？</p>
</li>
<li><p>信息熵公式定义</p>
<p>  设一次随机事件（用随机变量\(X\)表示），它可能会有\(x_1, x_2, x_3, \cdots ,x_m\)共\(m\)个不同的结果，每个结果出现的概率分别为\(p_1, p_2, p_3, \cdots, p_m\)，那么\(X\)的不确定度，即信息熵为：</p>
<p>  $$<br>H(X) =\sum_{i=1}^{m} p_i \cdot \log_{2} \frac{1}{p_i} = - \sum_{i=1}^{m} p_i \cdot \log_{2} p_i         \qquad (ml.1.2.1)<br>  $$</p>
<blockquote>
<p>①. 信息熵的物理意义：<br><br>一个事件（用随机变量\(X\)表示）可能的变化越多，那么它携带的信息量就越大（与变量具体取值无关，只跟值的种类多少以及发生概率有关）。</p>
<p>②. 系统熵举例：<br><br>对于一个分类系统来说，假设类别\(C\)可能的取值为\(c_1, c_2, \cdots, c_k\)（\(k\)是类别总数），每一个类别出现的概率分别是\(p(c_1),p(c_2), \cdots, p(c_k)\)。此时，分类系统的熵可以表示为:</p>
<p>  $$<br>  H(C) = - \sum_{i=1}^{k} p(c_i) \cdot \log_{2} p(c_i) \qquad (n.ml.1.2.1)<br>  $$</p>
<p>分类系统的作用就是输出一个特征向量（文本特征、ID特征、属性特征等）属于哪个类别的值，而这个值可能是\(c_1, c_2, \cdots, c_k\)，因此这个值所携带的信息量就是公式\((n.ml.1.2.1)\)这么多。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="条件熵">条件熵</h4><p>设\(X,Y\)为两个随机变量，在\(X\)发生的前提下，\(Y\)发生所新带来的熵 定义为\(Y\)的条件熵（Conditional Entropy），用\(H(Y|X)\)表示，计算公式如下：</p>
<p>$$<br>H(Y|X) = - \sum_{x_i,y_j}^{m,n} p(x_i,y_j) \cdot log_2 p(y_j|x_i) \qquad(ml.1.2.2)<br>$$</p>
<p>其物理含义是当变量\(X\)已知时，变量\(Y\)的平均不确定性是多少。公式\((ml.1.2.2)\)推导如下：</p>
<blockquote>
<p>假设变量\(X\)取值有\(m\)个，那么\(H(Y|X=x_i)\)是指变量\(X\)被固定为值\(x_i\)时的条件熵；\(H(Y|X)\)时指变量\(X\)被固定时的条件熵。那么二者之间的关系时：</p>
<p>$$<br>\begin{align}<br>H(Y|X) &amp; = p(x_1) \cdot H(Y|X=x_1) +  \cdots + p(x_m) \cdot H(Y|X=x_m) \\<br>&amp; = \sum_{i=1}^{m} p(x_i) \cdot H(Y|X=x_i)<br>\end{align}    \quad(n.ml.1.2.2)<br>$$</p>
<p>根据公式\((n.ml.1.2.2)\)继续推导\(Y\)的条件熵：</p>
<p>$$<br>\begin{align}<br>H(Y|X) &amp; = \sum_{i=1}^{m} p(x_i) \cdot H(Y|X=x_i) \\<br>&amp; = -\sum_{i=1}^{m} p(x_i) \cdot \left( \sum_{j=i}^{n} p(y_j|x_i) \cdot log_2 p(y_j|x_i) \right) \\<br>&amp; = -\sum_{i=1}^{m} \sum_{j=1}^{n} p(y_j,x_i) \cdot log_2 p(y_j|x_i) \\<br>&amp; = - \sum_{x_i,y_j}^{m,n} p(x_i,y_j) \cdot log_2 p(y_j|x_i)<br>\end{align}    \qquad\qquad (n.ml.1.2.3)<br>$$</p>
<p>注：<strong>条件熵里面是联合概率分布累加</strong>，公式\((n.ml.1.2.3)\)推导过程可参考《第3章：深入浅出ML之Based-Tree Classification Family》中3.1.2节条件熵部分。</p>
</blockquote>
<p><br></p>
<h4 id="联合熵">联合熵</h4><p>一个随机变量的不确定性可以用熵来表示，这一概念可以直接推广到多个随机变量。</p>
<ul>
<li><p>联合熵计算（Joint Entropy）</p>
<p>  设\(X,Y\)为两个随机变量，\(p(x_i,y_j)\)表示其联合概率，用\(H(XY)\)表示联合熵，计算公式为：</p>
<p>  $$<br>  H(XY) = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_{2} p(x_i,y_j) \qquad(ml.1.2.3)<br>  $$</p>
<blockquote>
<p>条件熵、联合熵、熵之间的关系：</p>
<p>  $$<br>  H(Y|X) = H(X,Y) - H(X)         \qquad\qquad(n.ml.1.2.4)<br>  $$</p>
<p>公式推导如下：</p>
<p>$$<br>\begin{align}<br>H(X,Y) - H(X) &amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 p(x_i,y_j) + \sum_{i=1}^{m} \underline{p(x_i)} \cdot log_2 p(x_i) \\<br>&amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 p(x_i,y_j) + \sum_{i=1}^{m} \underline{ \left( \sum_{j=1}^{n} p(x_i,y_j) \right) } \cdot log_2 p(x_i) \\<br>&amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot \left(log_2 p(x_i,y_j) - log_2 p(x_i) \right) \\<br>&amp; = - \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 p(y_j|x_i) \\<br>&amp; = H(Y|X)    \qquad\qquad\qquad\qquad\qquad\qquad (n.ml.1.2.5)<br>\end{align}<br>$$</p>
</blockquote>
</li>
<li><p>联合熵特点</p>
<ul>
<li>\(H(XY) \geq H(X)\)<ul>
<li>联合系统的熵不小于子系统的熵，即增加一个新系统不会减少不确定性。</li>
</ul>
</li>
<li>\(H(XY) \leq H(X)+H(Y)\)<ul>
<li>子系统可加性 </li>
</ul>
</li>
<li>\(H(XY) \geq 0\): 非负性。</li>
</ul>
</li>
</ul>
<p><br>    </p>
<h4 id="相对熵、KL距离">相对熵、KL距离</h4><ul>
<li><p>相对熵概念</p>
<p>  相对熵，又称为交叉熵或<strong>KL距离</strong>，是Kullback-Leibler散度（Kullback-Leibler Divergence）的简称。它主要用于衡量<strong>相同事件空间里</strong>的两个概率分布的差异。简单介绍其背景：</p>
<blockquote>
<p>根据香农的信息论，给定一个字符集的概率分布，我们可以设计一种编码，使得表示该字符集组成的（每个）字符串平均需要的比特数最少（比如Huffman编码）。假设字符集是\(X\)，对\(x \in X\)，其出现概率为\(P(x)\)，那么其最优编码平均需要的比特数（即每一个字符需要的比特数）等于这个字符集的熵（公式见\((ml.1.2.1)\)），即最优编码时，字符\(x\)的编码长度等于\(log_2{\frac{1}{P(x)}}\)。</p>
<p>在同样的字符集上，假设存在另一个概率分布\(Q(x)\)。如果根据\(Q(x)\)分布进行编码，那么表示这些字符就会比理想情况多用一些比特数。而<strong>KL距离</strong>就是用来衡量这种情况下平均每个字符多用的比特数，可用来度量两个分布的距离。</p>
</blockquote>
</li>
<li><p>KL距离计算公式</p>
<p>  这里用\(D(P||Q)\)表示KL距离，计算公式如下：</p>
<p>  $$<br>  D(P||Q) = \sum_{x \in X} P(x) \cdot log_2 \frac{P(x)}{Q(x)}  \qquad\qquad(ml.1.2.4)<br>  $$</p>
<p>  从公式\((ml.1.2.4)\)可以看出，当两个概率分布完全相同时，KL距离为0。概率分布\(P(x)\)的信息熵如公式\((ml.1.2.1)\)所示，说的是如果按照概率分布\(P(x)\)编码时，描述这个随机事件至少需要多少比特编码。</p>
<p>  因此，KL距离的物理意义可以这样表达：</p>
<blockquote>
<p>在相同的事件空间里，概率分布为\(P(x)\)的事件空间，若用概率分布\(Q(x)\)编码时，平均每个基本事件（符号）编码长度<strong>增加了</strong>多少比特数。</p>
</blockquote>
<p>  通过信息熵可知，不存在其它比按照随机事件本身概率分布更好的编码方式了，所以<strong>\(D(P||Q)\)始终是大于等于0的</strong>。</p>
<blockquote>
<p>虽然KL被称为距离，但是其不满足距离定义的3个条件：1) 非负性；2) 对称性(不满足)；3) 三角不等式(不满足)。</p>
</blockquote>
</li>
<li><p>KL距离示例</p>
<blockquote>
<p>假设有一个字符发射器，随机发出0和1两种字符，真实发出的概率分布为\(A\)。现在通过样本观察，得到概率分布\(B\)和\(C\)。各个分布的具体情况如下：</p>
<p>(1). \(A(0) = 1/2, A(1) = 1/2\); <br><br>(2). \(B(0) = 1/4, B(1) = 3/4\); <br><br>(3). \(C(0) = 1/8, C(1) = 7/8\); <br></p>
<p>那么可以计算出相对熵如下：<br></p>
<p>\(D(A||B) = 1/2 \cdot log_2 (\frac{1/2}{1/4}) + 1/2 \cdot log_2 (\frac{1/2}{3/4}) = 1/2 \cdot log_2 (4/3)\)  <br></p>
<p>\(D(A||C) = 1/2 \cdot log_2 (\frac{1/2}{1/8}) + 1/2 \cdot log_2 (\frac{1/2}{7/8}) = 1/2 \cdot log_2 (16/7)\)</p>
<p>可以看到，用\(B和C\)两种方式进行编码，其结果都是的平均编码长度增加了。同时也能发现，按照概率分布\(B\)进行编码，要比按照\(C\)进行编码，平均每个符号增加的比特数目要少。从分布熵也可以看出，实际上\(B\)要比\(C\)更接近实际分布。<br><br><br>如果实际分布为\(C\)，而用\(A\)分布来编码这个字符发射器的每个字符，同样可以得到：</p>
<p>\(D(C||A) = 1/8 \cdot log_2 (\frac{1/8}{1/2}) + 7/8 \cdot log_2 (\frac{7/8}{1/2}) = 7/8 \log_2{7} - 2 &gt; 0\)</p>
</blockquote>
<p> 从示例中，我们可以得出结论：<strong>对于一个信息源进行编码，按照其本身的概率分布进行编码，每个字符的平均比特数最少。</strong> 这也是信息熵的概念，用于衡量信息源本身的不确定性。</p>
<p> 此外可以看出，KL距离不满足对称性，即\(D(P||Q)\)不一定等于\(D(Q||P)\)。</p>
</li>
<li><p>相对熵应用场景</p>
<ul>
<li><p>推荐系统－物品之间相似度</p>
<blockquote>
<p>在使用LDA(Latent Dirichlet Allocation)<strong>计算物品之间的内容相似度</strong>时，我们可以先计算出物品在Topic上的分布，然后利用两个物品的Topic（话题）分布计算物品的相似度。比如，如果两个物品的Topic分布相似（<strong>处在同一个事件空间</strong>），则认为两个物品具有较高的相似度，反之则认为两个物品的相似度较低。<br><br><br>这种Topic分布的相似度可以利用KL散度来计算：</p>
<p>   $$<br>   D(P||Q) = \sum_{i \in X} p(x_i) \cdot log_2 {\frac{p(x_i)}{q(x_i)}}  \qquad(n.ml.1.2.6)<br>   $$</p>
<p>其中\(p\)和\(q\)是两个分布，\(X\)为话题集合，\(x_i\)表示第\(i\)个话题。<strong>KL散度越大说明分布的相似度越低</strong>。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p><br></p>
<h4 id="互信息">互信息</h4><p>如果说相对熵（KL）距离衡量的是相同事件空间里的两个事件的相似度大小，那么，互信息通常用来衡量不同事件空间里的两个信息（随机事件、变量）的相关性大小。</p>
<ul>
<li><p>互信息计算公式</p>
<p>  设\(X\)和\(Y\)为两个离散随机变量，事件\(Y=y_j\)的出现对于事件\(X=x_i\)的出现的互信息量\(I(x_i,y_j)\)定义为：</p>
<p>  $$<br>  I(x_i;y_j) = log_2 {\frac{p(x_i|y_j)}{p(x_i)}} = log_2 {\frac {p(x_i,y_j)}{p(x_i)p(y_j)}} \qquad(ml.1.2.5)<br>  $$</p>
<p>  对于事件\(X\)和\(Y\)来说，它们之间的互信息用\(I(X;Y)\)表示，公式为：</p>
<p>  $$<br>  I(X;Y) = \sum_{i=1}^{m} \sum_{j=1}^{n} p(x_i,y_j) \cdot log_2 {\frac{p(x_i,y_j)}{p(x_i)p(y_j)}}     \qquad(ml.1.2.6)<br>  $$</p>
<blockquote>
<p>公式解释：<br><br><br>互信息就是随机事件\(X\)的不确定性（即熵\(H(X)\)），以及在给定随机变量\(Y\)条件下的不确定性（即条件熵\(H(X|Y)\)）<strong>之间的差异</strong>，即</p>
<p>  $$<br>  I(X;Y) = H(X) - H(X|Y) \qquad(n.ml.1.2.7)<br>  $$</p>
<p><strong>互信息与决策树中的信息增益等价: 互信息  \(\Longleftrightarrow\) 信息增益.</strong></p>
</blockquote>
<p>  所谓两个事件相关性的量化度量，就是在了解了其中一个事件\(Y\)的前提下，对消除另一个事件\(X\)不确定性所提供的信息量。</p>
</li>
<li><p>互信息与其它熵之间的关系</p>
<ul>
<li>\(H(X|Y) = H(X,Y) - H(Y)\) </li>
<li>\(I(X;Y) = H(X) + H(Y) - H(X,Y)\) </li>
<li>\(I(X;Y) = H(X) - H(X|Y)\)</li>
<li>\(I(X;X) = H(X)\)</li>
</ul>
</li>
<li>互信息应用场景<ul>
<li>机器学习－<code>&lt;feature，label&gt;</code>之间相关性<ul>
<li>计算随机事件之间（不同的事件空间）的<strong>相关性</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p><br></p>
<h3 id="最大熵模型（Maximum_Entropy_Model）">最大熵模型（Maximum Entropy Model）</h3><p><br></p>
<h4 id="最大熵原理">最大熵原理</h4><p>在介绍最大熵模型之前，我们先了解一下最大熵原理，因为<strong>最大熵原理是选择最优概率模型的一个准则</strong>。</p>
<ul>
<li><strong>最大熵原理</strong></li>
</ul>
<table>
<thead>
<tr>
<th><strong>在概率模型空间集合中，在满足给定约束条件的前提下，使信息熵最大化得到的概率模型，就是最优的模型。</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>通常用约束条件来确定概率模型的集合。</p>
</blockquote>
<ul>
<li><p><strong>理解最大熵原理</strong></p>
<p>  假设离散随机变量\(X\)的概率分布是\(P(X)\)，其信息熵可用公式\((ml.1.2.1)\) 表示，并且熵满足以下不等式：</p>
<p>  $$<br>  0 \leq H(X) \leq log_2 |X|         \qquad\quad(ml.1.2.7)<br>  $$</p>
<p>  其中，\(|X|\)是\(X\)的取值个数，当且仅当\(X\)的分布是均匀分布时右边的等号才成立。也就是说，当\(X\)服从均匀分布时，熵最大。</p>
<blockquote>
<p>根据最大熵原理学习概率模型坚持的原则：<strong>首先必须满足已有的事实，即约束条件；但对不确定的部分不做任何假设，坚持无偏原则</strong>。最大熵原理通过熵的最大化来表示等可能性。</p>
</blockquote>
</li>
<li><p><strong>最大熵原理举例</strong>（本示例来自《统计学习方法》第6章－李航老师）</p>
<blockquote>
<p>问题：假设随机变量\(X\)有5个取值\(\{A,B,C,D,E\}\), 要估计各个取值的概率\(P(A),P(B),P(C),P(D),P(E)\)。</p>
<p>首先这些概率只满足以下约束条件：</p>
<p>  $$<br>  P(A) + P(B) + P(C) + P(D) + P(E) = 1 \qquad(exp.ml.1.2.1)<br>  $$</p>
<p>满足这个约束条件的概率分布有无穷多个，但是在没有任何其它信息的情况下，根据最大熵原理和无偏原则，选择熵最大时对应的概率分布，即各个取值概率相等是一个不错的概率估计方法。即有：</p>
<p>  $$<br>  P(A) = P(B) = P(C) = P(D) = P(E) = \frac{1}{5} \qquad(exp.ml.1.2.2)<br>  $$</p>
<p>等概率坚持了最大熵的无偏原则，因为没有更多信息，此种判断是合理的。</p>
<p>现在从先验知识中得到一些<code>信息</code>：\(A和B\)的概率值之和满足以下条件：</p>
<p>  $$<br>  P(A) + P(B) = \frac{3}{10}        \qquad(exp.ml.1.2.3)<br>  $$</p>
<p>  同样的，满足公式\((exp.ml.1.2.1)和(exp.ml.1.2.3)\)两个约束条件的概率分布仍有无穷多个。在缺少其它信息的情况下，坚持无偏原则，得到：</p>
<p>  $$<br>  \begin{align}<br>  P(A) = P(B) = \frac{3}{20} \qquad (exp.ml.1.2.4) \\<br>  P(C) = P(D) = P(E) = \frac{7}{30} \qquad (exp.ml.1.2.5)<br>  \end{align}<br>  $$</p>
<p>…</p>
<p>还可以继续按照满足约束条件下的求等概率的方法估计概率分布。以上概率模型学习的方法正是遵循了最大熵原理。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="最大熵模型定义">最大熵模型定义</h4><p>最大熵原理是统计学习的一般原理，将它应用到分类问题中，即得到最大熵模型。</p>
<ul>
<li><p>最大熵模型引入</p>
<p>  训练数据集：\(D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(N)},y^{(N)})\}\)，学习的目标是：<strong>用最大熵原理选择最优的分类模型。</strong></p>
<blockquote>
<p>假设分类模型是一个条件概率分布\(P(y|x), x \in X \subseteq R^n\)表示输入（特征向量），\(y \in Y\), \(X\)和\(Y\)分别是输入（特征向量）和输出（标签）的集合。这个模型表示的是对于给定的输入\(x\)，以<strong>条件概率</strong>\(P(y|x)\)计算得到标签\(y\)。</p>
</blockquote>
<ul>
<li><p>首先，考虑模型应满足的条件</p>
<p>  给定训练集，可以计算得到经验联合分布\(P(x,y)\)和边缘分布\(P(x)\)的经验分布，分别以\(\tilde{P}(x,y)\)和\(\tilde{P}(x)\)表示，即：</p>
<p>  $$<br>  \begin{align}<br>  \tilde{P}(x=\tilde{x}, y = \tilde{y}) &amp;= \frac{freq(x=\tilde{x}, y = \tilde{y})}{N} \qquad(1)\\<br>  \tilde{P}(x=\tilde{x}) &amp;= \frac{freq(x=\tilde{x})}{N} \qquad\qquad\;(2)<br>  \end{align}  \qquad(ml.1.2.8)<br>  $$</p>
<p>  其中，\(freq(x=\tilde{x}, y=\tilde{y})\)表示训练集中样本\((\tilde{x}, \tilde{y})\)出现的频数，\(freq(\tilde{x})\)表示训练集中输入\(\tilde{x}\)（向量）出现的频数，\(N\)表示训练集容量。</p>
</li>
<li><p>特征函数（Feature Function）</p>
<p>  定义<strong>特征函数</strong> \(f(x,y)\)用于描述输入\(x\)和输出\(y\)之间满足的某一种事实：</p>
<p>  $$<br>  f(x,y) = \begin{cases}<br>  \displaystyle 1, &amp;x与y满足某一事实; \\<br>  0, &amp; 其它<br>  \end{cases}     \qquad\qquad(ml.1.2.9)<br>  $$</p>
<p>  这是一个二值函数（也可以是任意实值函数），当\(x\)与\(y\)满足这个事实时取值为1，否则为0.</p>
<blockquote>
<p>①. 特征函数\(f(x,y)\)关于经验分布\(\tilde{P}(x,y)\)的期望值，用\(E_{\tilde{P}}(f)\)表示如下：</p>
<p>  $$<br>  E_{\tilde{P}} = \sum_{x,y} \tilde{P}(x,y) \cdot f(x,y)    \qquad\qquad(n.ml.1.2.8)<br>  $$</p>
<p>②. 特征函数\(f(x,y)\)关于模型\(P(y|x)\)与经验分布\(\tilde{P}(x)\)的期望值，用\(E_P(f)\)表示如下：</p>
<p>  $$<br>  E_P(f) = \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot f(x,y)    \qquad(n.ml.1.2.9)<br>  $$</p>
<p>③. 如果模型能够获取训练数据中足够的信息，那么就可以假设这两个期望值相等。即：</p>
<p>  $$<br>  \sum_{x,y} \tilde{P}(x,y) \cdot f(x,y) ＝ \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot f(x,y)    \qquad(n.ml.1.2.10)<br>  $$</p>
<p>注：公式\((n.ml.1.2.10)\)是频率学派－点估计求参数套路，之所以假设相等，是因为有\(p(x,y)=p(y|x) \cdot p(x)\)</p>
</blockquote>
<p>  我们将公式\((n.ml.1.2.10)\)作为概率模型学习的约束条件。假如有\(n\)个特征函数\(f_{i} (x,y), i=1,2, \cdots, n\)，那么就有\(n\)个约束条件。</p>
</li>
</ul>
</li>
<li><p>最大熵模型定义</p>
<p>  假设满足所有约束条件的模型集合为：</p>
<p>  $$<br>  \mathcal{C} = \{P \in \mathcal{P} | E_{P}(f_i) = E_{\tilde{P}}(f_i), i=1,2, \cdots, n\} \qquad (ml.1.2.10)<br>  $$ </p>
<p>  定义在条件概率分布\(P(y|x)\)上的条件熵为：</p>
<p>  $$<br>  H(P) = - \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot \log {P(y|x)}     \qquad (ml.1.2.11)<br>  $$</p>
<p>  模型集合\(\mathcal{C}\)中条件熵\(H(P)\)最大的模型称为最大熵模型。 </p>
<blockquote>
<p>注：<strong>最大熵模型中\(\log\)是指以\(e\)为底的对数</strong>，与信息熵公式中以2为底不同。本文如无特殊说明，\(\log\)均指自然对数。</p>
</blockquote>
</li>
</ul>
<p><br></p>
<h4 id="最大熵模型参数学习">最大熵模型参数学习</h4><p>最大熵模型学习过程即为求解最大熵模型的过程，<strong>最大熵模型的学习问题可以表示为带有约束的最优化问题</strong>。</p>
<ul>
<li><p>示例：学习《最大熵原理》示例中的最大熵模型</p>
<p>   为了简便，这里分别以\(y_1,y_2,y_3,y_4,y_5\)表示\(A,B,C,D和E\)，最大熵模型学习的最优化问题可以表示为：</p>
<p>   $$<br>   \begin{align}<br>   &amp; min \quad -H(P) = \sum_{i=1}^{5} P(y_i) \cdot log{P(y_i)} \\<br>   &amp; s.t. \quad P(y_1) + P(y_2) = \tilde{P}(y_1) + \tilde{P}(y_2) = \frac{3}{10} \\<br>   &amp; \qquad \sum_{i=1}^{5} P(y_i) = \sum_{i=1}^{5} \tilde{P}(y_i) = 1<br>   \end{align} \qquad\quad (exp.ml.1.2.5)<br>   $$</p>
<blockquote>
<p>提示：这里面没有特征\(x\)和特征函数\(f_i(x,y)\)的约束。</p>
</blockquote>
<p>   将带约束优化问题转化为无约束优化问题：引入拉格朗日乘子\(w_0,w_1\)，定义朗格朗日函数：</p>
<p>   $$<br>   L(P,w) = \sum_{i=1}^{5} P(y_i) log{P(y_i)} + w_1 \left( P(y_1) + P(y_2) - \frac{3}{10} \right) + w_0 \left(\sum_{i=1}^{5} P(y_i) - 1 \right) \;(exp.ml.1.2.6)<br>   $$</p>
<p>   根据拉格朗日对偶性，可以通过求解对偶最优化问题得到原始最优化问题的解，所以求解（对偶问题）：\(\max_{w} \min_{P} L(P,w) \)。求解过程如下：</p>
<blockquote>
<p>首先求解\(L(P,w)\)关于\(P\)的极小化问题。为此，固定\(w_0,w_1\)，求偏导数：</p>
<p>   $$<br>   \begin{align}<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_1)} = 1 + log_2 P(y_1) + w_1 + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_2)} = 1 + log_2 P(y_2) + w_1 + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_3)} = 1 + log_2 P(y_3) + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_4)} = 1 + log_2 P(y_4) + w_0 \\<br>   &amp; \frac{\partial L(P,w)}{\partial P(y_5)} = 1 + log_2 P(y_5) + w_0 \\<br>   \end{align}<br>   $$</p>
<p>令各偏导数等于0，可解得：</p>
<p>   $$<br>   \begin{align}<br>   &amp; P(y_1) = P(y_2) = e^{-w_1 - w_0 - 1} \\<br>   &amp; P(y_3) = P(y_4) = P(y_5) = e^{-w_0 -1}<br>   \end{align}<br>   $$</p>
<p>于是，极小化结果为：</p>
<p>   $$<br>   \min_{P} \; L(P,w) = L(P_w, w) = -2 e^{-w_1 - w_0 - 1} -3 e^{-w_0 - 1} - \frac{3}{10} w_1 - w_0<br>   $$</p>
<p>下面再求解对偶函数\(L(P_w,w)\)关于\(w\)的极大化问题：</p>
<p>   $$<br>   \max_{w} \; L(P_w, w) = -2 e^{-w_1 - w_0 - 1} -3 e^{-w_0 - 1} - \frac{3}{10} w_1 - w_0        \qquad(exp.ml.1.2.7)<br>   $$</p>
<p>分别求\(L(P_w,w)\)对\(w_0,w_1\)的偏导数，并令其为0，得到：</p>
<p>   $$<br>   \begin{align}<br>   &amp; e^{-w_1 - w_0 - 1} = \frac{3}{20} \\<br>   &amp; e^{-w_0 - 1} = \frac{7}{30}<br>   \end{align}<br>   $$</p>
<p>于是得到所求的概率分布为：</p>
<p>   $$<br>   \begin{align}<br>   &amp; P(y_1) = P(y_2) = \frac{3}{20} \\<br>   &amp; P(y_3) = P(y_4) = P(y_5) = \frac{7}{30}<br>   \end{align}<br>   $$</p>
</blockquote>
</li>
<li><p>最大熵模型学习一般流程</p>
<p>  对于给定的训练\(D=\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(N)},y^{(N)})\}\)以及特征函数\(f_i(x,y),i=1,2,\cdots,n\)，最大熵模型的学习等价于带约束的最优化问题：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \max_{P \in \mathcal{C}} \quad H(P) = -\sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot log P(y|x) \\<br>  &amp; s.t. \quad E_P(f_i) = E_{\tilde{P}} (f_i), \; i=1,2,\cdots,n \\<br>  &amp; \qquad \sum_{y} P(y|x) = 1<br>  \end{align}        \qquad\quad(ml.1.2.12)<br>  $$</p>
<p>  按照最优化问题的习惯思路，将求最大值问题改写为求等价的最小值问题，即：</p>
<p>  $$<br>  \begin{align}<br>  &amp; \min_{P \in \mathcal{C}} \quad -H(P) = \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot log P(y|x) \\<br>  &amp; s.t. \quad E_P(f_i) - E_{\tilde{P}} (f_i) = 0, \; i=1,2,\cdots,n \\<br>  &amp; \qquad \sum_{y} P(y|x) = 1<br>  \end{align}        \qquad\quad(ml.1.2.13)<br>  $$</p>
<p>  求解约束最优化问题\((ml.1.2.13)\)所得出的解，就是最大熵模型学习的解。</p>
<p>  <strong>将约束最优化的原始问题转换为无约束最优化的对偶问题</strong>。具体推导过程如下：</p>
<ul>
<li><p><strong>首先，引入拉格朗日乘子\(w_0,w_1,\cdots,w_n\)，定义拉格朗日函数\(L(P,w)\)</strong></p>
<p>  表达式为：</p>
<p>  $$<br>\begin{align}<br>L(P,w) &amp; = -H(P) + w_0 \cdot \left( 1- \sum_{y} P(y|x) \right) + \sum_{i=1}^{n} w_i \cdot \left( E_{\tilde{P}}(f_i) - E_P (f_i) \right) \\<br>&amp; = \sum_{x,y} \tilde{P}(x) \cdot P(y|x) \cdot log {P(y|x)} + w_0 \cdot \left( 1- \sum_{y} P(y|x) \right) \\<br>&amp; \qquad + \sum_{i=1}^{n} w_i \cdot \left(\sum_{x,y} \tilde{P}(x,y) \cdot f_i(x,y) -  \sum_{x.y} \tilde{P}(x) \cdot P(y|x) \cdot f_i(x,y) \right)<br>\end{align} \quad(ml.1.2.14)<br>$$</p>
<p>  最优化的原始问题是：</p>
<p>  $$<br>\min_{P \in \mathcal{C}} \max_{w} L(P,w) \qquad\qquad(ml.1.2.15)<br>  $$</p>
<p>  对偶问题是：</p>
<p>  $$<br>  \max_{w} \min_{P \in \mathcal{C}} L(P,w) \qquad\qquad(ml.1.2.16)<br>  $$</p>
<blockquote>
<p>通俗的讲，由_最小最大问题_转化为_最大最小问题_。</p>
</blockquote>
<p>  <strong>由于最大熵模型对应的朗格朗日函数\(L(P,w)\)是参数\(P\)的凸函数，所以原始问题\((ml.1.2.15)\)的解与对偶问题\((ml.1.2.16)\)的解是等价的</strong>。因此，可以通过求解对偶问题来得到原始问题的解。</p>
</li>
<li><p><strong>其次，求对偶问题\((ml.1.2.16)\)内部的极小化问题\(\min_{P \in \mathcal{C}} L(P,w)\)</strong></p>
<p>  \(\min_{P \in \mathcal{C}} L(P,w)\)是乘子\(w\)的函数，将其记作：</p>
<p>  $$<br>\Psi(w) = \min_{P \in \mathcal{C}} L(P,w) = L(P_w, w) \qquad(ml.1.2.17)<br>$$</p>
<blockquote>
<p>\(\Psi(w)\)称为对偶函数（\(Latex: \Psi\) = <code>\Psi</code>）。将其解记作：</p>
<p>  $$<br>  P_w = arg \min_{P \in \mathcal{C}} L(P,w) = P_w (y|x) \qquad(n.ml.1.2.11)<br>  $$</p>
<p>具体地，固定\(w_i\)，求\(L(P,w)\)对\(P(y|x)\)的偏导数：</p>
<p>  $$<br>  \begin{align}<br>\frac{\partial L(P,w)} {\partial P(y|x)} &amp; = \sum_{x,y} \tilde{P}(x) \cdot \left(logP(y|x) + 1 \right) - \sum_{y} w_0 - \sum_{x,y} \left( \tilde{P}(x) \cdot \sum_{i=1}^{n} w_i \cdot f_i(x,y) \right) \\<br>&amp; = \sum_{x,y} \tilde{P}(x) \cdot \left(logP(y|x) + 1 - w_0 - \sum_{i=1}^{n} w_i \cdot f_i(x,y) \right)     \qquad(n.ml.1.2.12)<br>  \end{align}<br>  $$</p>
<p>令偏导数等于0，在\(\tilde{P}(x) &gt; 0\)的情况下，求得：</p>
<p>  $$<br>  P(y|x) = \exp {\left( \sum_{i=1}^{n} w_i \cdot f_i(x,y) + w_0 - 1 \right)} = \frac  {\exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y) \right)} {\exp(1-w_0)}  \quad(n.ml.1.2.13)<br>  $$</p>
<p>由于 \(\sum_{y} P(y|x) = 1\)，可得：</p>
<p>  $$<br>P_w (y|x) = \frac{1}{Z_w(x)} \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y) \right) \qquad\quad(n.ml.1.2.14)<br>  $$</p>
<p>其中，</p>
<p>  $$<br>Z_w(x) = \sum_{y} \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y) \right) \qquad\quad(n.ml.1.2.15)<br>  $$</p>
</blockquote>
<p>  \(Z_w(x)\)称为归一化因子；\(f_i(x,y)\)是特征函数；\(w_i\)是第\(i\)个参数（特征权值）。公式\((n.ml.1.2.14)\)、\((n.ml.1.2.15)\) 表示的模型\(P_w = P_w(y|x)\)就是最大熵模型（\(w\)是最大熵模型中的参数向量）。</p>
</li>
<li><p><strong>最后，求解对偶问题外部的极大化问题</strong></p>
<p>  对偶问题外部极大化表达式：</p>
<p>  $$<br>  \max_{w} \Psi(w)        \qquad\qquad(ml.1.2.18)<br>   $$    </p>
<p>   将其解记作\(w^@\)，即: \(w^@ = arg \max_{w} \Psi(w)\)。</p>
<p>   也就是说，可以应用最优化算法求对偶函数\(\Psi(w)\)的极大化，得到\(w^@\)，用其表示\(P^@ = P_{w^@} = P_{w^@}(y|x)\)是学习到的最优模型（最大熵模型）。</p>
<p>   <strong>最大熵模型的学习归结为对偶函数\(\Psi(w)\)的极大化。</strong></p>
</li>
</ul>
</li>
</ul>
<p><br>     </p>
<h4 id="对偶函数极大化与极大似然估计等价">对偶函数极大化与极大似然估计等价</h4><p>从最大熵模型的学习过程可以看出，最大熵模型是由\(n.ml.1.2.14\)和\(n.ml.1.2.15\)表示的条件概率分布。下面证明：<strong>对偶函数的极大化等价于最大熵模型的极大似然估计</strong>。</p>
<ul>
<li><p><strong><code>对偶函数极大化＝极大似然估计</code></strong></p>
<p>  已知训练数据的经验概率分布\(\tilde{P}(x,y)\)，条件概率分布分布\(P(y|x)\)的对数似然函数表示为：</p>
<p>  $$<br>  L_{\tilde{P}}(P_w) = \log \prod_{x,y} P(y|x)^{\tilde{P}(x,y)} = \sum_{x,y} \tilde{P}(x,y) \cdot \log P(y|x)  \qquad(ml.1.2.19)<br>  $$</p>
<p>  当<strong>条件概率分布\(P(y|x)\)是最大熵模型</strong>(公式\((n.ml.1.2.14)和n(.ml.1.2.15)\))时，对数似然函数\(L_{\tilde{P}}(P_w)\)为：</p>
<p>  $$<br>  \begin{align}<br>  L_{\tilde{P}}(P_w) &amp; = \sum_{x,y} \tilde{P}(x,y) \cdot \log P(y|x) \\<br>  &amp; = \sum_{x,y} \left (\tilde{P}(x,y) \cdot \sum_{i=1}^{n} w_i f_i(x,y)\right) - \sum_{x,y} \tilde{P}(x,y) \cdot log Z_w(x) \\<br>  &amp; = \sum_{x,y} \left (\tilde{P}(x,y) \cdot \sum_{i=1}^{n} w_i f_i(x,y)\right) - \sum_{x} \tilde{P}(x) \cdot log Z_w(x)<br>  \end{align} \quad(ml.1.2.20)<br>  $$</p>
<p>  再看对偶函数\(\Psi(w)\)，由公式\((ml.1.2.14)\)和公式\((ml.1.2.17)\)可得：</p>
<p>  $$<br>  \begin{align}<br>  \Psi(w) &amp; = \sum_{x,y} \tilde{P}(x) \cdot P_w(y|x) \cdot \log P_w(y|x) \\<br>  &amp; \qquad\quad + \sum_{i=1}^{n} w_i \cdot \left(\sum_{x,y} \tilde{P}(x,y) f_i(x,y) - \sum_{x,y} \tilde{P}(x) P_w(y|x)f_i(x,y) \right) \\<br>  &amp; = \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^{n} w_i f_i(x,y) + \sum_{x,y} \tilde{P}(x)P_w(y|x) \left(\underline{log P_w(y|x) - \sum_{i=1}^{n} w_i f_i (x,y)}\right) \\<br>  &amp; = \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^{n} w_i f_i(x,y) - \sum_{x,y} \tilde{P}(x) P_w(y|x) \cdot \underline{\log Z_w(x)} \\<br>  &amp; = \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^{n} w_i f_i(x,y) - \sum_{x} \tilde{P}(x) \log Z_w(x)<br>  \end{align} \quad(ml.1.2.21)<br>  $$</p>
<blockquote>
<p>其中， 第二步推导第三步中用到了:</p>
<p>  $$<br>  \sum_{i=1}^{n} w_i \cdot f_i(x,y) = \log P_w(y|x) \cdot Z_w(x) \qquad(n.ml.1.2.16)<br>  $$</p>
<p>根据公式\((n.ml.1.2.14)\)得到。在最后一步用到了\(\sum_{y} P(y|x) = 1\)的性质。即：</p>
<p>  $$<br>  \begin{align}<br>  \sum_{x,y} \tilde{P}(x) P_w(y|x) \log Z_w(x) &amp; = \sum_{x} \tilde{P}(x) \left( \sum_{y} P_w(y|x) \right) \log Z_w(x) \\<br>  &amp; = \sum_{x} \tilde{P}(x) \log Z_w(x)<br>  \end{align}    \qquad(n.ml.1.2.17)<br>  $$</p>
</blockquote>
<p>  比较公式\((ml.1.2.20)\)和\((ml.1.2.21)\)，可以发现：</p>
<p>  $$<br>  \Psi(w) = L_{\tilde{P}}(P_w)        \qquad\qquad(ml.1.2.22)<br>  $$</p>
<p>  <strong>即对偶函数\(\Psi(w)\)等价于对数似然函数\(L_{\tilde{P}}(P_w)\)，于是最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计</strong>的结论得以证明。</p>
<blockquote>
<p>总结：<strong>最大熵模型的学习问题就转化为具体求解对数似然函数极大化或对偶函数极大化</strong>的问题。</p>
</blockquote>
<p>  可以将最大熵模型写成更为一般的形式：</p>
<p>  $$<br>  \begin{align}<br>  P_w(y|x) &amp;= \frac{1}{Z_w(x)} \cdot \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y)\right) \\<br>  Z_w(x) &amp;= \sum_{y} \exp \left(\sum_{i=1}^{n} w_i \cdot f_i(x,y)\right)<br>  \end{align} \qquad(ml.1.2.23)<br>  $$</p>
<p>  这里，\(x \in R^n\)为输入（向量），\(y \in \{1,2, \cdots, K\}\)为输出，\(w \in R^n\)为权值向量，\(f_i(x,y), i=1,2, \cdots, n\)为任意实值特征函数。</p>
<blockquote>
<p>小结：<br><br>①. <strong>最大熵模型与LR模型有类似的形式，它们又称为对数线性模型（Log Linear Model）。</strong></p>
<p>②. <strong>模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计。</strong></p>
</blockquote>
</li>
</ul>
<p><br></p>
<h3 id="参数学习的最优化问题">参数学习的最优化问题</h3><p>已知偶函数极大化与极大似然估计等价，那么LR模型、最大熵模型的学习问题可以归结为<strong>以似然函数为目标函数的最优化问题，通常通过迭代算法求解（非闭式解）。</strong></p>
<blockquote>
<p>该部分在<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="external">《第01章：深入浅出ML之Regression家族》</a>中有详细介绍。</p>
</blockquote>
<p>从最优化的角度来，此时的目标函数具有良好的性质：光滑的凸函数。因此多种最优化方法都适用，并且能保证找到全局最优解。常用的方法有改进的迭代尺度法（Improved Iterative Scaling, IIS）、梯度下降法（SGD、mini-batch GD等）、共轭梯度法、拟牛顿法等。</p>
<blockquote>
<p><strong>该部分最优化求解方法会在《最优化算法》系列中详细阐述。</strong></p>
</blockquote>
<p><br></p>
<hr>
<p>更多信息请关注：<a href="http://www.52caml.com/" target="_blank" rel="external">计算广告与机器学习－CAML 技术共享平台</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深入浅出机器学习/">深入浅出机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/KL距离/">KL距离</a><a href="/tags/ME/">ME</a><a href="/tags/Maximum-Entropy/">Maximum Entropy</a><a href="/tags/互信息/">互信息</a><a href="/tags/信息熵/">信息熵</a><a href="/tags/信息论/">信息论</a><a href="/tags/最大熵原理/">最大熵原理</a><a href="/tags/最大熵模型/">最大熵模型</a><a href="/tags/条件熵/">条件熵</a><a href="/tags/相对熵/">相对熵</a><a href="/tags/联合熵/">联合熵</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/stats/beta-gamma-dirichlet-function/" title="概率与统计-chapter0-三个重要函数" itemprop="url">概率与统计-chapter0-三个重要函数</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-07-04T04:21:53.000Z" itemprop="datePublished"> 发表于 2015-07-04</time>
    
  </p>
</header>
    <div class="article-content">
        
        <ul>
<li>Author: zhouyongsdzh@foxmail.com</li>
<li>Date: 2015-07-04</li>
<li>Sina Weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<h3 id="多产的数学家－欧拉"><strong>多产的数学家－欧拉</strong></h3><p>他是公认的数学史上4位最伟大的数学家之一，他的一生没有戏剧性的故事，但给后人留下宝贵的科学财富。</p>
<p>18世纪，数学家辈出的年代，他仍属于佼佼者，被认为是18世纪数学界最杰出的人物之一。</p>
<p>同时，他在数学领域最多产的一位，不仅在数学界做出伟大贡献，而且把数学应用到了几乎整个物理领域。他就是<strong>18世纪数学界之星-欧拉</strong>。</p>
<p><a href="http://baike.baidu.com/link?url=OFDCUSh42Bfc_tOI70iV9Zh3hRkxZww91RL_2zlyGNUccKlXAGZRZzLA47ZtTx_XwROYpptHjgsbIJmlAnGPCoRcRi_UIhtRWMQcoqb8zTP-5rLDaLwqvGxU_s2J7Qu596UmWqAJ-43oM5-70om12q" target="_blank" rel="external">莱昂哈德·欧拉</a>)（Leonhard Euler ，1707年4月15日～1783年9月18日），瑞士数学家、自然科学家。1707年出生于瑞士－巴塞尔的牧师家庭，15岁从巴塞尔大学毕业，翌年获得硕士学位。</p>
<p>简述其成就：</p>
<ol>
<li><strong>_微分方程_</strong>: 18世纪中叶，欧拉与其它数学家在解决物理问题过程中，创立了<strong>微分方程</strong>学科。偏微分方程的纯数学研究的第一篇论文是欧拉写的《方程的积分法研究》。此外，还提出了<strong>函数用三角级数表示</strong>的方法和<strong>解微分方程的级数法</strong>等。<br><br></li>
<li><strong>_微分几何学_</strong>：引入了空间曲线的参数方程，给出了空间曲线曲率半径的解析表达式；1766年出版了《关于曲面上曲线的研究》，建立了<strong>曲面理论</strong>，是微分几何发展史上的里程碑。<br><br></li>
<li><strong>_分析学_</strong>：1729年引入了<strong>Gamma函数和Beta函数</strong>，证明了椭圆积分的加法定理，最早引入了<strong>二重积分</strong>。<br><br></li>
<li><strong>_数论_</strong>：欧拉的一系列成果奠定了该数学分支。<br><br></li>
</ol>
<blockquote>
<p>注：数学史上公认的4名最伟大的数学家分别是：<strong>阿基米德、牛顿、欧拉和高斯</strong>。<br></p>
<p>阿基米德有“翘起地球”的豪言壮语，牛顿因为苹果闻名世界，高斯少年时就显露出计算天赋，唯独欧拉没有戏剧性的故事让人印象深刻。</p>
</blockquote>
<p>这里仅总结欧拉在分析学领域中的一个成就－<strong>欧拉积分</strong>及其对应的分布。欧拉积分是一种含参变量的积分，主要包括</p>
<ul>
<li>第一类欧拉积分：又称为Beta函数（简称B函数）</li>
<li>第二类欧拉积分：又称为Gamma函数</li>
</ul>
<h3 id="Gamma函数－第二类欧拉积分">Gamma函数－第二类欧拉积分</h3><blockquote>
<p>因为Beta函数和Dirichlet函数可以表示用Gamma函数表示，所以先介绍Gamma函数。</p>
</blockquote>
<h4 id="历史由来"><br><strong>历史由来</strong></h4><p>1728年，<a href="http://baike.baidu.com/view/7386144.htm?fromtitle=%E5%93%A5%E5%BE%B7%E5%B7%B4%E8%B5%AB&amp;fromid=1632207&amp;type=syn" target="_blank" rel="external">哥德巴赫</a>（Goldbach C. 德国数学家，1690-1764）在考虑<strong>数列插值</strong>的问题，通俗的说就是把数列的通项公式定义<strong>从整数集合延伸至实数集合</strong>，例如数列1,4,9,16…..可以用通项公式\(n^2\)自然地表达，即便\(n\)为实数时，此通项公式也具有良好的定义。另一个自然的解释就是可以找到一条平滑的曲线\(f(x)=x^2\)通过所有的整数点\([x, x^2]\)，从而可以把定义在整数集上的公式延伸至实数集合。</p>
<p>有一天，哥德巴赫开始处理<strong>阶乘序列</strong>\(1,2,6,24,120,720,\cdots\)，我们可以计算整数的阶乘（如\(2!,3!\)），那么是否可以计算实数的阶乘（如\(2.5!\)）呢？</p>
<p>遗憾的是，哥德巴赫当时无法解决<strong>阶乘的计算从整数集合延伸至实数集合上</strong>的问题，于是写信请教<a href="https://en.wikipedia.org/wiki/Nicolaus_II_Bernoulli" target="_blank" rel="external">尼古拉斯.伯努利</a>和其弟<a href="https://en.wikipedia.org/wiki/Daniel_Bernoulli" target="_blank" rel="external">丹尼尔.伯努利</a>，由于当时欧拉与丹尼尔.伯努利在一起，因此也得知了该问题。而欧拉与1729年完美的解决了这个问题，由此导致Gamma函数的诞生，当时的欧拉只有22岁。</p>
<h4 id="Gamma发现之旅"><br><strong>Gamma发现之旅</strong></h4><p>其实最早发现\(n!\)差值计算的是丹尼尔.伯努利。他发现：如果\(m,n\)都是正整数，且\(m\to\infty\)时，有下面公式成立：</p>
<p>$$<br>\frac {1 \cdot 2 \cdot 3 \cdots m} {(1+n)(2+n) \cdots (m-1+n)} \left(m+ \frac{n}{2}\right)^{n-1} \to n!<br>$$</p>
<p>用此<strong>无穷乘积</strong>的方式可以把\(n!\)定义延伸至实数集合。如\(n=2.5,m\)足够大时，上式基本可近似计算出\(2.5!\)的值。</p>
<p>欧拉偶然地发现\(n!\)可用如下无穷乘积表达：</p>
<p>$$<br>\left[\left(\frac{2}{1}\right)^n \frac{1}{n+1}\right] \left[\left(\frac{3}{2}\right)^n \frac{2}{n+2}\right]<br>\left[\left(\frac{4}{3}\right)^n \frac{3}{n+3}\right]<br>\cdots = n!<br>$$</p>
<p>于是他用一些简单的例子做一些计算，寻找其规律。当\(n=1/2\)时，整理上式可得：</p>
<p>$$<br>\left(\frac{1}{2}\right)!=\sqrt{\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots}<br>$$</p>
<p>此式恰好与著名的<a href="http://baike.baidu.com/link?url=FWbPfOA5BqiQxrCJ4KJnedzJRf6blsxbh2qn1XyGlPvUbGXufoVXf4xie6UMnWNUUAH2ewNaziQug2YzHQ9jpK" target="_blank" rel="external">Wallis公式</a>有关。</p>
<blockquote>
<p>关于Wallis公式：</p>
<p>\( \qquad \lim\limits_{k\to\infty} \left(\frac{2^{2k}(k!)^2}{2k!}\right)^2 \frac{1}{2k+1} = \frac{\pi}{2} \)</p>
<p>Wallis公式是关于圆周率的无穷乘积的公式，Wallis本人在1665年使用<strong>插值方法</strong>计算<strong>半圆曲线</strong>\(y=\sqrt{x(1-x)}\)下的面积（即直径为1的半圆面积）时，发现\(\pi/4\)等于下式：</p>
<p>\(<br>\qquad\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots = \frac{\pi}{4}<br>\)</p>
</blockquote>
<p>于是欧拉根据Wallis公式得到如下的结果：</p>
<p>$$<br>\left(\frac{1}{2}\right)! = \frac{\sqrt{\pi}}{2}<br>$$</p>
<p>欧拉发现\((\frac{1}{2})!\)中含有\(\pi\)，而\(\pi\)与圆相关的积分有关系。欧拉猜测\(n!\)一定可以用积分形式表示。</p>
<p>Wallis时代微积分还没问世，当时是使用插值方法做推导计算的，但Wallis公式的推导过程基本上都是在处理积分\(\int_{0}^{1} x^\frac{1}{2} (1-x)^\frac{1}{2} dx\)的问题。</p>
<blockquote>
<p>关于微积分诞生：<br><br>牛顿与莱布尼茨谁先发明的微积分在数学界有很大的争论。不过本人更认可是莱布尼茨，因为微积分的主要思想是他提出的，微分相关概念和符号是他定义的，并沿用至今。<br><br>1684年<a href="http://baike.baidu.com/link?url=TsKq7RAFzLHNnXSqoXKfEosO1ogw_FQnRME1vlTwvwKiJs0hudGCkvWkUJW1MHwpiGPGB7SHD49O2Nmea5m_i63qc9eJSMlDzZ7VAPo7D2p4d6oj9Bb0MVkLfDUaSbqvNCc5Opspb0YHMMGf8PJXaH5kmhwbnefwifkVxETybompTpK4IDEAoP9ZR83kyP4L" target="_blank" rel="external">莱布尼茨</a>发表了关于微积分的重要文献，标志着微积分作为独立学科正式诞生。莱布尼茨是微积分主要思想的独立发明人。当然，不可否认的是牛顿在此前同样做了很多微积分相关工作。</p>
</blockquote>
<p>受Wallis启发，欧拉开始考虑如下一般形式的积分：</p>
<p>$$<br>J(e,n)=\int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>这里，\(n\)为正整数，\(e\)为正实数。利用<strong>分部积分</strong>法，可得：</p>
<p>$$<br>J(e,n)=\frac{n}{e+1}J(e+1, n-1)<br>$$</p>
<blockquote>
<p><strong>关于<a href="http://baike.baidu.com/link?url=2UfHeRl5l_jvgTbPSA4APPlKvrivw6i7iTmXB6-phTsDM-VqyQtxfJyb8EpIFR5r_5vwkjedDtN5japSjucvqq" target="_blank" rel="external">分部积分</a></strong>：<br><br>设\(u=u(x)\)和\(v=v(x)\)均为可导函数，分部积分形式如下：<br></p>
<p>\( \qquad \int u \mathop{v’}dx = \int u dv = [uv] - \int v du \)</p>
</blockquote>
<p>重复迭代上式，可得：</p>
<p>$$<br>J(e,n) = \frac {1\cdot2\cdot3\cdots n}{(e+1)(e+2)\cdots (e+n+1)}<br>$$</p>
<p>于是欧拉得到如下重要的式子：</p>
<p>$$<br>n!=(e+1)(e+2)\cdots (e+n+1) \int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>接下来，欧拉使用一些计算技巧，即取\(e=f/g\) 且 \(f\to1, g\to0\)，然后对上式右边计算极限，得到如下简洁的结果：</p>
<p>$$<br>n! = \int_{0}^{1}(-\log{t})^n dt<br>$$</p>
<p>到此，欧拉成功地把\(n!\)表达为了积分形式！令\(t=e^{-u}\)，可得常见的Gamma函数形式：</p>
<p>$$<br>n! = \int_{0}^{\infty} u^n e^{-u} du<br>$$</p>
<p>注意，此时的\(n\)仍然为正整数，利用上式把<strong>阶乘延伸至实数集</strong>上，就得到Gamma函数的一般形式：</p>
<p>$$<br>\Gamma(x) = \int_{0}^{1} (-\log t)^{x-1}dt = \int_{0}^{\infty} t^{x-1} e^{-t} dt<br>$$</p>
<blockquote>
<p>注：使用MathJax引擎在浏览器上解析时，需要对”_”做转义，应写成下面形式：<br></p>
<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\<span class="keyword">Gamma</span>(x) = \int\_<span class="list">&#123;0&#125;</span>^<span class="list">&#123;1&#125;</span> (-\log t)^<span class="list">&#123;x-1&#125;</span>dt = \int_<span class="list">&#123;0&#125;</span>^<span class="list">&#123;\infty&#125;</span> t^<span class="list">&#123;x-1&#125;</span> e^<span class="list">&#123;-t&#125;</span>dt</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>为什么</strong>\(\Gamma(n)=(n-1)!\)<strong>，而不是</strong>\(\Gamma(n)=n!\)</p>
<p>如果对上述的Gamma函数式做一个调整，将\(t^{x-1}\)替换为\(t^x\)，可得如下函数：</p>
<p>$$<br>\Gamma(x) = \int_{0}^{\infty} t^x e^{-t}dx<br>$$</p>
<p>此时\(\Gamma(n)=n!\)，这才是欧拉最早的Gamma函数定义。但后来欧拉修改了原有Gamma函数的定义，使得\(\Gamma(n)=(n-1)!\)。后来的数学家们在对Gamma函数的进一步研究中，认可了这个定义，沿用至今。</p>
<h4 id="Gamma分布">Gamma分布</h4><ul>
<li>概率密度函数</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {x^{\alpha-1} e^{x/\beta}} {\beta^\alpha \Gamma(\alpha)}, &amp; 0 \leq x \leq \infty; \alpha &gt; 0; \beta &gt; 0 \\\<br>\;\;\quad 0, &amp; others<br>\end{cases}<br>$$</p>
<h3 id="Beta函数－第一类欧拉积分">Beta函数－第一类欧拉积分</h3><p>在@rickjin的《LDA-math》系列中详细地解释了Beta和Dirichlet分布的由来和推导过程，堪称经典。原文见下面链接：</p>
<ul>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%831" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(1)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%832" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(2)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%833" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(3)</a> <br></li>
</ul>
<p>@rickjin在文中从一个<strong>魔鬼的游戏</strong>开始引入。魔鬼撒旦抓走一人，撒旦说：“你们人类很聪明，而我是很仁慈的，和你玩一个游戏，赢了就可以走，否则把灵魂出卖给我。”</p>
<blockquote>
<p>游戏规则：<br><br>我有一个魔盒，上面有一个按钮，你每按一下按钮，就均匀的输出一个[0,1]之间的随机数，我现在按10下，我手上有10个数，你猜第7大的数是什么，偏离不超过0.01就算对。</p>
</blockquote>
<h4 id="数学抽象与推导">数学抽象与推导</h4><p>上述游戏实际在说随机变量\(X_1, X_2, \cdots, X_{10} \sim U(0,1)\), 把这\(n\)个随机变量排序后得到顺序统计量\(X_{(1)}, X_{(2)}, \cdots, X_{(10)}\)，然后问\(X_{(k)}\)的分布是什么？</p>
<blockquote>
<p>因为如果知道随机变量\(X_{(k)}\)分布的概率密度，用概率密度的极值点作为猜测值是最好的策略。</p>
</blockquote>
<p>对于上述游戏而言，\(n=10,k=7\)，\(X_{(k)}\)的分布如何计算？@rickjin在文中列举了通过尝试计算\(X_{(k)}\)落在区间\([x,x+\Delta x]\)的概率，也就是求如下概率值</p>
<p>$$<br>P(x \leq X_{(k)} \leq x + \Delta x) = \;?<br>$$</p>
<p>如果把[0,1]区间分为三个子区间，即\([0,x)、[x,x+\Delta x]和(x+\Delta x,1]\)。考虑简单的情形，假设\(n\)个数中只有一个落在了区间\([x,x+\Delta x]\)内，这个区间内的数\(X_{(k)}\)是第\(k\)大的，则区间\([0,x)\)中应该有\(k-1\)个数，区间\((x+\Delta x,1]\)中应该有\(n-k\)个数。那么，符合上述要求的事件\(E\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E = \\{<br>&amp; X_1 \in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x)\quad (i=2,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<blockquote>
<p>注：</p>
<ol>
<li>符号<code>*</code>在markdown中有特殊含义，若要当作latex环境中的语法用，需要转义，即<code>\ *</code>.</li>
<li>在Mathjax下，<code>$$E = \{x, y\}$$</code>无法正常输出<code>{}</code>, 因为Latex要想输出<code>{}</code>，需要使用<code>\{, \}</code>。而<code>\</code>在markdown中要想正常表法其自身意义也需要再次转义，代码为: <code>$$E = \\{x,y\\}$$</code>。</li>
</ol>
<p>$$E = \\{ x,y\\}$$</p>
</blockquote>
<p>那么事件\(E\)发生的概率，有：</p>
<p>$$<br>\begin{align*}<br>P(E) &amp; = \prod_{i=1}^nP(X_i) \\\<br>&amp; = x^{k-1}(1-x-\Delta x)^{n-k}\Delta x \\\<br>&amp; = x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>\(o(\Delta x)\)表示\(\Delta x\)的高阶无穷小。显然，由于<strong>不同的排列组合</strong>，即\(n\)个数中有一个落在\([x, x+ \Delta x]\)区间的有\(n\)中取法，余下\(n-1\)个数中有\(k-1\)个落在\([0, x)\)区间有\(\binom{n-1}{k-1}\)种组合，所以<strong>与事件E等概率的事件一共有\(n \binom{n-1}{k-1}\)个（当只有1个数落在\([x, x+ \Delta x]\)时）</strong>。</p>
<p>继续考虑复杂一些的情形，假设\(n\)个数中<strong>两个数</strong>落在了区间\([x, x+ \Delta x]\)，此时事件\(E^{‘}\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E’ = \\{<br>&amp; X_1,X_2\in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x) \quad (i=3,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<p>此时，事件\(E^{‘}\)发生的概率为：</p>
<p>$$<br>P(E^{‘}) = x^{k-2}(1-x-\Delta x)^{n-k}(\Delta x)^2 = o(\Delta x)<br>$$</p>
<p>很容易看出，只要落在\([x, x+ \Delta x]\)内的数字<strong>大于1个</strong>，则对应事件的概率就是\(\Delta x\)。于是随机变量\(X_{(k)}\)落在\([x, x+ \Delta x]\)区间的概率：</p>
<p>$$<br>\begin{align*}<br>&amp; P( x \le X_{(k)} \le x+\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}P(E) + o(\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>进一步可得\(X_{(k)}\)的概率密度函数为</p>
<p>$$<br>\begin{align*}<br>f(x) &amp; = \lim_{\Delta x \to 0} \frac {P(x \leq X_{(x)} \leq x + \Delta x)}{\Delta x} \\\<br>&amp; = n \binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\\\<br>&amp; = \frac {n!}{(k-1)! (n-k)!}x^{k-1} (1-x)^{n-k} \quad x \in [0,1]<br>\end{align*}<br>$$</p>
<p>利用上述介绍的Gamma函数，可以把\(f(x)\)表示为</p>
<p>$$<br>f(x) = \frac {\Gamma(n+1)}{\Gamma(k) \Gamma(n-k+1)} x^{k-1} (1-x)^{n-k}<br>$$</p>
<p><strong>\(\Gamma\)函数起源于人们希望把数学计算从整数集合拓展至实数集合</strong>。这里另\(\alpha=k, \beta=n-k+1\)，于是可以得到</p>
<p>$$<br>Beta(x|\alpha,\beta)=f(x)=\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}<br>$$</p>
<p>上式即为一般意义上的Beta分布！</p>
<h4 id="\(\beta\)型概率分布"><br><strong>\(\beta\)型概率分布</strong></h4><ul>
<li>\(\beta\)型随机变量\(X\)的概率密度函数：</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}, &amp; 0 \leq x \leq 1;\;\alpha &gt; 0, \beta &gt; 0 \\\<br>0, &amp; others<br>\end{cases}<br>\quad (**)<br>$$</p>
<blockquote>
<p>关于<strong>Beta函数详细推导</strong><br></p>
<p>$$<br>\begin{align*}<br>f(x;\alpha,\beta) &amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt} \\\<br>&amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{Beta(\alpha,\beta)}<br>\end{align*}<br>$$</p>
<p>其中，</p>
<p>$$<br>Beta(\alpha,\beta)= \int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}<br>$$</p>
<p>$$<br>\Gamma(\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt<br>$$<br>且当\(\alpha\)是正整数时，\(\Gamma(\alpha)=(\alpha-1)!\)</p>
</blockquote>
<ul>
<li>\(\beta\)型随机变量的期望与方差分别是：</li>
</ul>
<p>$$\mu=\frac{\alpha}{\alpha+\beta} \;\qquad\qquad\qquad (期望)$$</p>
<p>$$\sigma^2=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \quad (方差)$$</p>
<ul>
<li>\(\beta\)型分布的期望公式详细推导</li>
</ul>
<p>如果\(p \sim Beta(t|\alpha,\beta)\)，那么\(p\)的期望表示如下：</p>
<p>$$<br>\begin{align*}<br>E(p)<br>&amp; =\int_{0}^{1} t \ast Beta(t|\alpha,\beta)\;dt \\\<br>&amp; = \int_{0}^{1} t \ast \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}t^{\alpha-1}(1-t)^{\beta-1}\;dt \\\<br>&amp; =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt<br>\end{align*}<br>$$</p>
<p>上式右边的积分\(\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt\)恰好对应到概率分布\(Beta(t|\alpha+1,\beta)\)。对于此分布，根据Beta型随机变量的密度函数在\([0,1]\)之间的累积分布等于1，可得:</p>
<p>$$<br>\int_{0}^{1} \frac{\Gamma(\alpha+\beta+1)}{\Gamma(\alpha+1)\Gamma(\beta)} t^\alpha (1-t)^{\beta-1}\;dt=1<br>$$</p>
<p>把上式带入\(E(p)\)计算式，得到 </p>
<p>$$<br>\begin{align*}<br>E(p) &amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \cdot<br>\frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \\\<br>&amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+1)}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \notag \\\<br>&amp; = \frac{\alpha}{\alpha+\beta}<br>\end{align*}<br>$$</p>
<p>这说明，<strong>对于Beta分布的随机变量，其均值可以用\(\frac{\alpha}{\alpha+\beta}\)来估计</strong>。</p>
<ul>
<li>\(\beta\)型概率分布曲线</li>
</ul>
<p><strong>_(此处暂略~)_</strong></p>
<p>回到<strong>魔鬼的游戏</strong>中，\(n=10, k=7\)时，按照密度分布的峰值去猜测是最好的策略。</p>
<p>$$<br>f(x)=\frac{10!}{(6)!(3)!} x^6 (1-x)^3 \quad x \in [0,1]<br>$$</p>
<p>即便按照密度函数分布的峰值作为猜测结果，第一次猜中的概率也不高。<br><br></p>
<p><strong>游戏继续\(\cdots\)</strong></p>
<p>很遗憾，根据上述最好的策略算出来的值竟然有偏差，没猜中，魔鬼微笑着说：“我再仁慈一点，再给你一个机会，你按5下这个机器，你就得到了5个\([0,1]\)之间的随机数，然后我可以告诉你这5个数中的每一个，和我的第7个数相比，谁大谁小，然后你继续猜我手头上的第7大的数是多少。” 这时候该如何猜测呢？</p>
<h3 id="Beta-Binomial共轭"><br><strong>Beta-Binomial共轭</strong></h3><p>魔鬼的两个问题，数学抽象一下，就是：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \in Uniform(0,1)\)，对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\)，我们要猜测\(p=X_{(k)}\)；（第1个问题）</li>
<li>\(Y_1,Y_2, \cdots, Y_m \in Uniform(0,1)\)，\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大；（第2个问题）</li>
</ul>
<p>最后的问题：<strong>\(P(p|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</strong></p>
<p><strong>问题分析</strong></p>
<p>由于\(p=X_(k)\)在\(X_1,X_2, \cdots, X_n\)中是第\(k\)大的，利用\(Y_i\)的信息，可以很容易的推理得到\(p=X_{(k)}\)在\(X_1,X_2, \cdots, X_n,\) \(Y_1, Y_2, \cdots, Y_m \in Uniform(0,1)\)这\((m+n)\)个独立随机变量中是第\(k+m_1\)大的。那么按照<strong>Beta分布（贝塔分布）</strong>小节的推理，此时\(p=X_{(k)}\)的概率密度函数是\(Beta(p|\,k+m_1, n-k+1+m_2)\)。</p>
<p>根据<strong>贝叶斯推理</strong>的逻辑，整理上述过程如下：</p>
<ul>
<li>\(p=X_{(k)}\)是我们需要猜测的参数，并且推导出\(p\)的分布为\(f(p)=Beta(p|k,n-k+1)\)，称为\(p\)的<strong>先验分布</strong>；</li>
<li>数据\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大，\(Y_i\)相当于做了\(m\)次<strong>贝努利试验</strong>，所以\(m_1\)服从二项分布\(B(m,p)\)；</li>
<li>在给定了来自数据提供的\((m_1, m_2)\)的知识后，\(p\)的<strong>后验分布</strong>变为:</li>
</ul>
<p>$$f(p|\,m_1,m_2)=Beta(p|\,k+m_1, n-k+1+m_2)$$</p>
<blockquote>
<p>关于<strong>贝叶斯参数估计</strong>的基本过程：</p>
<p>$$先验分布 ＋ 数据知识 ＝ 后验分布$$</p>
</blockquote>
<p>在这里，贝叶斯分析过程的简单直观的表述就是：</p>
<p>$$Beta(p|\,k, n-k+1) + Count(m_1, m_2) = Beta(p|\,k+m_1,n-k+1+m_2)$$</p>
<p>其中\(m_1,m_2\)对应的是二项分布\(B(m_1+m_2, p)\)的计数。更为一般地数学表述：对于非负实数\(\alpha,\beta\)，存在如下关系</p>
<p>$$<br>Beta(p|\,\alpha, \beta) + Count(m_1, m_2) = Beta(p|\,\alpha+m_1, \beta+m_2) \quad (**)<br>$$</p>
<p>该式描述的就是<strong>Beta-Binomial共轭</strong>。</p>
<blockquote>
<p>关于<strong>Beta-Binomial共轭</strong> <br><br>数据符合<strong>二项分布</strong>的时候，参数的先验分布和后验分布都能保持<strong>Beta分布</strong>的形式，这种形式不变的好处是：我们能够在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续至后验分布中进行解释，同时从先验变换到后验的过程中从数据中补充的知识也容易给出物理上的解释。</p>
</blockquote>
<p>推导过程中可以看到，\(Beta\)分布中的参数\(\alpha,\beta\)都可以理解为物理计数，这两个参数经常被称为<strong>伪计数（pseudo-count）</strong>。\(Beta(p|\,\alpha, \beta)\)可以写成如下表达式：</p>
<p>$$Beta(p|\,1,1) + Count(\alpha-1, \beta-1) = Beta(p|\,\alpha,\beta)$$</p>
<p>其中的<strong>\(Beta(p|\,1,1)\)恰好就是均匀分布\(Uniform(0,1)\)</strong>。</p>
<blockquote>
<p>关于<strong>Beta分布与均匀分布</strong>的关系：</p>
<p>$$Beta(p|\,\alpha=1,\beta=1)= Uniform(a=0,b=1)$$</p>
</blockquote>
<p>对于\((**)\)共轭公式，其实可纯粹从贝叶斯的角度来进行推导。理解过程：</p>
<ul>
<li>假设有一个不均匀的硬币抛出正面的概率为\(p\)，抛\(m\)次后出现正面和反面的次数分别是\(m_1,m_2\)，那么按照传统的<strong>频率学派</strong>观点，\(p\)的估计值应该是\(\hat p = \frac{m_1}{m}\)。<br></li>
<li>而从<strong>贝叶斯学派</strong>的观点来看，开始对硬币不均匀性一无所知，所以应该假设\(p\ \in Uniform(0,1)\)，于是二项分布的计数\(m_1,m_2\)之后，按照贝叶斯公式计算\(p\)的后验分布：</li>
</ul>
<p>$$<br>\begin{align*}<br>P(p|\,m_1,m_2) = &amp; \frac {P(p) \cdot P(m_1,m_2\,|p)}{P(m_1,m_2)} \\\<br>= &amp; \frac {1 \cdot P(m_1,m_2 |p)}{\int_{0}^{1} P(m_1,m_2|t)dt} \\\<br>= &amp; \frac {\binom{m}{m_1}p^{m_1}(1-p)^{m_2}}{\int_{0}^{1} \binom{m}{m_1}t^{m_1}(1-t)^{m_2}dt} \\\<br>= &amp; \frac {p^{m_1}(1-p)^{m_2}}{\int_{0}^{1}t^{m_1}(1-t)^{m_2}dt}<br>\end{align*}<br>$$</p>
<p>得到的后验分布正好是\(Beta(p|\;m_1 + 1, m_2 + 1)\)。</p>
<p>回到<strong>魔鬼的游戏</strong>，如果按出的5个随机数字中，魔鬼告诉你有2个（即\(m_1=2\)）小于他手中第7大的数，那么应该按照如下概率分布的<strong>峰值</strong>做猜测是最好的(\(\alpha+2=9,\beta+3=7\))：</p>
<p>$$<br>Beta(x|9,7)=\frac{15!}{(8)!(6)!}x^8(1-x)^6  \quad x \in [0,1]<br>$$</p>
<p>很幸运，这次猜中了，但是魔鬼开始耍赖，游戏不得不继续 \(\cdots\)</p>
<blockquote>
<p>游戏3新规则：<br><br>魔说道：“这个游戏对你来说太简单了，我要加大点难度，我们重新来一次，<strong>我按20下生成20个随机数，你要同时给我猜第7大和第13大的数是什么？</strong>”</p>
</blockquote>
<p>此时又该如何猜测呢？<br></p>
<h3 id="Dirichlet分布"><strong>Dirichlet分布</strong></h3><p>对于魔鬼变本加厉的新的游戏规则，数学抽象如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\),</li>
<li>排序后对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\),</li>
<li><strong>问: \((X_{(k_1)},X_{(k_1 + k_2)})\)的联合分布是什么？</strong></li>
</ul>
<p>游戏3完全类似游戏1的推导过程，可进行如下的概率计算：</p>
<blockquote>
<p>注：为了公式的简洁性，这里取\(x_3=1-x_1-x_2\),但只有\(x_1,x_2\)是变量。说明符号：<br></p>
<ol>
<li>n: 总的样本数<br></li>
<li>\(X_{(k_1)}\)和\(X_{(k_1 + k_2)}\)将n分割为3段，长度分别为\(k_1, k_2和k_3\)，即<br></li>
</ol>
<p>$$<br>分段结果：<br>\begin{cases}<br>[X_{(1)}, X_{(k-1)}], &amp; 有(k_1-1)个取值，统一记为x_1 \\\<br>[X_{(k_1+1)}, X_{(k_1+k_2)}], &amp; 有(k_2-1)个取值，统一记为x_2 \\\<br>[X_{(k_1+k_2+1)}, X_{(n)}] &amp; 有(n-k_1-k_2)个取值，统一记为x_3<br>\end{cases}<br>$$</p>
</blockquote>
<p>完全类似于游戏1的推导过程，\(X_{(k_1)},X_{(k_1+k_2)}\)的<strong>联合概率</strong>计算如下：</p>
<p>$$<br>\begin{align*}<br>&amp; P\Bigl(X_{(k_1)} \in (x_1, x_1+\Delta x), X_{(k_1+k_2)} \in (x_2, x_2+\Delta x)\Bigr) \\\<br>&amp; \quad = n(n-1)\binom{n-2}{k_1-1,k_2-1}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2 \\\<br>&amp; \quad = \frac{n!}{(k_1-1)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2<br>\end{align*}<br>$$</p>
<blockquote>
<p>式中的\(n(n-1)\binom{n-2}{k_1-1,k_2-1}\)是一个排列组合问题，比较容易理解。</p>
</blockquote>
<p>于是得到\(X_{(k_1)},X_{(k_1+k_2)}\)的联合分布是：</p>
<p>$$<br>\begin{align*}<br>f(x_1,x_2,x_3) &amp; =\frac{n!}{(k_1-)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2} \\\<br>&amp; = \frac{\Gamma(n+1)}{\Gamma(k_1)\Gamma(k_2)\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}<br>\end{align*} \quad (**)<br>$$</p>
<p>而公式\((**)\)则是3维形式的Dirichlet分布，即\(Dir(x_1,x_2,x_3\;|\;k_1,k_2,n-k_1-k_2+1)\)。这里令\(\alpha_1=k_1,\alpha_2=k_2,\alpha_3=n-k_1-k_2+1\)，于是分布密度函数可以写为：</p>
<p>$$<br>f(x_1, x_2, x_3) = \frac{\Gamma(\alpha_1+\alpha_2+\alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}x_2^{\alpha_2-1}x_3^{\alpha_3-1} \qquad (0)<br>$$</p>
<p>公式\((0)\)即为一般形式的3维Dirichlet分布。即使\(\vec{\alpha}=(\alpha_1,\alpha_2,\alpha_3)\)<strong>延伸至非负实数集合</strong>，上述概率分布也是成立的。</p>
<p>在游戏3的基础上还可以往更高的维度上推，譬如\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)中的4、5、…等更多个数，可以得到更高维度的Dirichlet分布和Dirichlet-Multinomial共轭。</p>
<ul>
<li>Dirichlet分布</li>
</ul>
<p>如果\(\vec{p} \sim Dir(\vec{t}|\vec{\alpha})\),那么随机变量\(\vec{p}\)的概率密度函数为</p>
<p>$$<br>\begin{equation}<br>\displaystyle Dir(\overrightarrow{p}|\overrightarrow{\alpha}) =<br>\displaystyle \frac{\Gamma(\sum_{k=1}^{K} \alpha_k)}<br>{\prod_{k=1}^K \Gamma(\alpha_k)} \prod_{k=1}^{K} p_k^{\alpha_k -1}<br>\end{equation}    \qquad (1)<br>$$</p>
<p>对于给定的\(\overrightarrow{p}\)和\(N\)，多项式分布定义为：</p>
<p>$$<br>Multi(\overrightarrow{n}|\overrightarrow{p},N)=<br>\binom{N}{\overrightarrow{n}} \prod_{k=1}^K p_k^{n_k} \qquad (2)<br>$$</p>
<p>而\(Multi(\overrightarrow{n}|\overrightarrow{p},N)\)和\(Dir((\overrightarrow{p}|\overrightarrow{\alpha})\)这两个分布是共轭关系。</p>
<ul>
<li>Dirichlet分布均值</li>
</ul>
<p>$$<br>E(\vec{p}) = \Bigl(\frac{\alpha_1}{\sum_{i=1}^{K}\alpha_i},\frac{\alpha_2}{\sum_{i=1}^{K}\alpha_i}, \cdots, \frac{\alpha_K}{\sum_{i=1}^{K}\alpha_i}\Bigr)<br>$$</p>
<blockquote>
<p>关于<strong>Dirichlet分布与Beta分布</strong>：<br><br>Dirichlet分布是Beta分布在高维度上的推广。当Dirichlet分布维度趋向无限时，便成为Dirichlet过程。<br><br></p>
</blockquote>
<h3 id="Dirichlet-Multinomial共轭"><strong>Dirichlet-Multinomial共轭</strong></h3><p>类似于魔鬼的游戏2，调整一下游戏3，可得游戏4的规则如下：</p>
<blockquote>
<p>游戏4: <br><br>从魔盒中生成m个随机数\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)，魔鬼告诉我们\(Y_i\)和\(X_{(k_1)},X_{(k_1+k_2)}\)相比谁大谁小，然后再次猜测第7大和第13大的数是多少？</p>
</blockquote>
<p>同样，对游戏4进行数学抽象，表示如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\)，排序后对应的顺序统计量为\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)；</li>
<li>令\(p_1=X_{(k_1)},p_2=X_{(k_1+k_2)},p_3=1-p_1-p_2(p_3是为了简洁的数学表达)\)，我们要猜测\(\vec{p}=(p_1,p_2,p_3)\)；</li>
<li>\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\),\(Y_i\)中落到\([0,p_1), [p_1,p_2),[p_2,1]\)三个区间的个数分别是\(m_1,m_2,m_3\),其中\(m=m_1+m_2+m_3\)；</li>
<li>问后验分布\(P(\vec{p}|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</li>
</ul>
<p>为了计算方便，记：</p>
<p>$$<br>\vec{m}=(m_1, m_2, m_3), \quad \vec{k}=(k_1,k_2,n-k_1-k_2+1)<br>$$</p>
<p>从游戏中的信息可以得知：\(p_1,p_2\)在\(X_1,X_2, \cdots, X_n,Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)这\(m+n\)个数中分别成为了第\(k_1+m_1, k_2+m_2\)大的数，于是后验分布\(P(\vec{p}|Y_1,Y_2,\cdots, Y_m)\)应该是\(Dir(\vec{p}|k_1+m_1,k_2+m_2, n-k_1-k_2+1+m_3)\)，即\(Dir(\vec{p}|\vec{k}+\vec{m})\)。按照贝叶斯推理的逻辑，同样可以把上述过程整理如下：</p>
<ul>
<li>首先，根据游戏3需要猜测参数\(\vec{p}=(p_1,p_2,p_3)\)，其先验分布为\(Dir(\vec{p}|\vec{k})\)；</li>
<li>其次，数据\(Y_i\)落到\([0,p_1),[p_1,p_2),[p_2,1]\)三个区间的个数分别为\(m_1,m_2,m_3\)，所以\(\vec{m}=(m_1, m_2, m_3)\)服从多项式分布\(Multi(\vec{m}|\vec{p})\)<strong>（理解这一点很重要）</strong>；</li>
<li>在给定了来自数据提供的知识\(\vec{m}\)后，\(\vec{p}\)的后验分布变为\(Dir(\vec{p}|\vec{k}+\vec{m})\)。</li>
</ul>
<p>以上贝叶斯分析过程的最简单直接的表述：</p>
<p>$$<br>Dir(\overrightarrow{p}|\overrightarrow{k})+Multi(\vec{m})=Dir(\vec{p}|\vec{k}+\vec{m})<br>$$</p>
<p>令\(\overrightarrow{\alpha}=\vec{k}\)，把\(\vec{\alpha}\)从整数集合延拓至实数集合，可以证明如下关系仍然成立：</p>
<p>$$<br>Dir(\vec{p}|\vec{\alpha})+Multi(\vec{m})=Dir(\vec{p}|\vec{\alpha}+\vec{m}) \quad (11)<br>$$</p>
<p>公式\((11)\)描述就是<strong>Dirichlet-Multinomial共轭</strong>。同时，我们可以把Dirichlet分布中的\(\alpha\)都可以理解为物理计数。那么，类似于Beta分布，\(Dir(\vec{p}|\vec{\alpha})\)可做如下分解：</p>
<p>$$<br>Dir(\vec{p}|\vec{1})+Multi(\vec{m}-\vec{1}) = Dir(\vec{p}|\vec{\alpha})<br>$$</p>
<p>这里\(\vec{1}=(1,1,\cdots,1)\),上式同样可以类似的用纯粹贝叶斯的观点推导和解释。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/概率与统计/">概率与统计</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Gamma-Beta-Dirichlet/">Gamma,Beta,Dirichlet</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/stats/prob-stats-chapter3-continuous-random-variable/" title="概率与统计-chapter3-连续随机变量" itemprop="url">概率与统计-chapter3-连续随机变量</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="ZhouYong" target="_blank" itemprop="author">ZhouYong</a>
		
  <p class="article-time">
    <time datetime="2015-06-20T14:34:13.000Z" itemprop="datePublished"> 发表于 2015-06-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="连续随机变量（continuous_random_variable）">连续随机变量（continuous random variable）</h2><ul>
<li>author: zhouyongsdzh@foxmail.com</li>
<li>date: 2015-06-29 22:34:13</li>
<li>weibo: <a href="http://weibo.com/p/1005051707438033/home?" target="_blank" rel="external">@周永_52ML</a></li>
</ul>
<hr>
<p><strong>目录</strong></p>
<ul>
<li>均匀分布</li>
<li>高斯分布</li>
<li>Gamma分布</li>
<li>Erlang分布</li>
<li>指数分布</li>
<li>Beta分布</li>
<li>Dirichlet分布</li>
<li>other<br><br></li>
</ul>
<h3 id="说明">说明</h3><p>可以取连续值的随机变量称为<strong>连续随机变量</strong>（continuous random variable）。</p>
<p>由于是取连续值，那么在任意区间都可以有无穷多个结果。比如高度，可以有1米、2米，也可以在两者之间：1.1米、1.11米，1.688米等无穷多个结果。如此，每个结果取值的可能性都是无穷小。</p>
<p>因此，连续随机变量区别于离散随机变量重要一点：在连续随机变量中，讨论的是时间在<strong>_某个区间_</strong>内发生的概率，即\(P(a &lt; X &lt; b)\)，而不是具体某一取值的概率\(P(X)\)。因为在这种情况下，分到各个结果的概率都无限趋于0。显然，无法用离散随机变量中的<strong>概率质量函数</strong>来描述随机变量的分布。</p>
<p>针对连续随机变量的分布，我们可用以下指标来描述具体的分布：</p>
<ul>
<li><p>累积分布函数（Cumulative Distribution Function，简称CDF）</p>
<p>  累积分布函数本身表示随机变量在一个区间上的概率，所以可直接用于连续随机变量。即：</p>
</li>
</ul>
<p>$$F(x)=P(X \leq x),\quad -\infty &lt; x &lt; \infty$$</p>
<ul>
<li><p>概率密度函数（Probability Density Function，简称PDF）</p>
<p>  根据<code>无穷小</code>的概念，可以得到概率密度函数。在随机变量\(X＝x\)的附近去一个无穷小段，该小段的区间长度为\(dx\)，这无穷小段对应的概率为\(dF\)，那么该点的概率密度为\(dF/dx\)。</p>
<p>  因此，概率密度函数可以代替累积分布函数，表示一个连续随机变量的概率分布。</p>
<p>  $$f(x)=\frac {dF(x)}{dx}$$</p>
<p>  $$F(x)=\int_{-\infty}^{x}f(t)dt$$ </p>
<blockquote>
<p>CDF与PDF之间的关系：PDF是CDF的微分，CDF是PDF在区间\([-\infty, x]\)上的积分。</p>
</blockquote>
</li>
<li><p>均值（Expectation，又称均值Mean）</p>
</li>
<li>方差（Variance）</li>
</ul>
<blockquote>
<p>这里的连续随机变量主要包括：均匀概率分布、高斯分布、Gamma分布等.<br></p>
</blockquote>
<h3 id="均匀分布（Uniform_Distribution）"><strong>均匀分布（Uniform Distribution）</strong></h3><p>假设有一个随机数生成器，产生\([a, b]\)之间的实数\(X)\)（\(a&lt;b\)），每个实数值出现的概率相等，这样的分布被称为均匀分布。那么，随机变量\(X\)服从区间\([a,b]\)上的均匀分布，记作\(X \sim U(a,b)\)。该类型的随机变量称作<strong>均匀随机变量</strong>。</p>
<blockquote>
<p>随机变量\(X\)的密度函数用图可表示为一个长方形，长方形的高是\(1\,/\,(b-a)\)，以保证长方形的面积等于1.</p>
</blockquote>
<ul>
<li>均匀随机变量X的<strong>概率密度函数（PDF）</strong>为：</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {1}{b-a}, &amp; a \leq x \leq b \\\<br>\quad0, &amp; others<br>\end{cases}<br>$$</p>
<ul>
<li>均匀随机变量X的<strong>累积分布函数（CDF）</strong>本身就表示随机变量在\([-\infty, x]\)区间上的概率，是概率密度函数在\([-\infty, x]\)区间上的积分，公式如下：</li>
</ul>
<p>$$<br>F(x)=P(X \leq x)=\int_{-\infty}^{x}f(t)dt=<br>\begin{cases}<br>\quad0, &amp; -\infty &lt; x &lt; a \\\<br>\displaystyle\frac {x-a}{b-a}, &amp; a \leq x \leq b \\\<br>\quad1, &amp; b &lt; x &lt; \infty<br>\end{cases}<br>$$</p>
<ul>
<li>均匀随机变量X的<strong>期望</strong>：</li>
</ul>
<p>$$<br>E(X)=\frac {a+b}{2}<br>$$</p>
<ul>
<li>均匀随机变量X的<strong>方差</strong>：</li>
</ul>
<p>$$<br>D(X)=E(X^2)-E^2(X)=\frac {(b-a)^2}{12}<br>$$</p>
<blockquote>
<p>注：在概率统计学中，几乎所以重要的概率分布都可以从均匀分布\(Uniform(0,1)\)中生成，尤其是在做统计模拟试验中，所有统计分布的随机样本都是通过均匀分布产生的。<br></p>
</blockquote>
<h3 id="高斯分布（Gaussian_Distribution）">高斯分布（Gaussian Distribution）</h3><ul>
<li>概率密度函数</li>
</ul>
<p>$$<br>f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2 \sigma^2} \right)<br>$$</p>
<p><br></p>
<h3 id="Gamma分布（伽马分布，又称\(\Gamma\)型概率分布）">Gamma分布（伽马分布，又称\(\Gamma\)型概率分布）</h3><p>在认识Gamma分布之前，我们先熟悉一下Gamma函数，在@52nlp的<a href="http://www.52nlp.cn/lda-math-%E7%A5%9E%E5%A5%87%E7%9A%84gamma%E5%87%BD%E6%95%B01" target="_blank" rel="external">神奇的Gamma函数</a>一文和<a href="http://baike.baidu.com/link?url=l_mNJgJzx9XbJKf4QBI2zBr3SrEzd6pqx1X-smju8t5vFmzB8xx8ynWew0-2dIrOf9iSuIl5wdaJnTlKFGZbs_" target="_blank" rel="external">百度百科－Gamma函数</a>中，讲的非常清楚。这里仅列出背景、核心公式和说明。</p>
<p><br></p>
<h4 id="Gamma函数"><strong>Gamma函数</strong></h4><ul>
<li><strong>历史由来</strong></li>
</ul>
<p>1728年，<a href="http://baike.baidu.com/view/7386144.htm?fromtitle=%E5%93%A5%E5%BE%B7%E5%B7%B4%E8%B5%AB&amp;fromid=1632207&amp;type=syn" target="_blank" rel="external">哥德巴赫</a>（Goldbach C. 德国数学家，1690-1764）在考虑<strong>数列插值</strong>的问题，通俗的说就是把数列的通项公式定义<strong>从整数集合延伸至实数集合</strong>，例如数列1,4,9,16…..可以用通项公式\(n^2\)自然的表达，即便\(n\)为实数的时候，这个通项公式也可以良好定义。</p>
<p>直观地说，就是可以找到一条平滑的曲线\(f(x)=x^2\)通过所有的整数点\([x, x^2]\)，从而可以把定义在整数集上的公式延伸至实数集合。一天哥德巴赫开始处理<strong>阶乘序列</strong>\(1,2,6,24,120,720,\cdots\)，我们可以计算\(2!,3!\),是否可以计算\(2.5!\)呢？</p>
<p>遗憾的是，哥德巴赫当时无法解决<strong>阶乘的计算从整数集合延伸至实数集合上</strong>的问题，于是写信请教尼古拉斯.伯努利和其弟丹尼尔.伯努利，由于欧拉当时与丹尼尔.伯努利在一起，因此也得知了该问题。而欧拉与1729年完美的解决了这个问题，由此导致Gamma函数的诞生，当时的欧拉只有22岁。</p>
<ul>
<li><strong>Gamma函数发现之旅</strong></li>
</ul>
<p>最早发现\(n!\)差值计算的是丹尼尔.伯努利。他发现：如果\(m,n\)都是正整数，且\(m\to\infty\)，有下述公式成立：</p>
<p>$$<br>\frac {1 \cdot 2 \cdot 3 \cdots m} {(1+n)(2+n) \cdots (m-1+n)} \left(m+ \frac{n}{2}\right)^{n-1} \to n!<br>$$</p>
<p>用此<strong>无穷乘积</strong>的方式可以把\(n!\)定义延伸至实数集合。如\(n=2.5,m\)足够大时，上式基本可近似计算出\(2.5!\)的值。</p>
<p>欧拉偶然地发现\(n!\)可用如下无穷乘积表达：</p>
<p>$$<br>\left[\left(\frac{2}{1}\right)^n \frac{1}{n+1}\right] \left[\left(\frac{3}{2}\right)^n \frac{2}{n+2}\right]<br>\left[\left(\frac{4}{3}\right)^n \frac{3}{n+3}\right]<br>\cdots = n!<br>$$</p>
<p>于是他用一些简单的例子做一些计算，寻找其规律。当\(n=1/2\)时，整理上式可得：</p>
<p>$$<br>\left(\frac{1}{2}\right)!=\sqrt{\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots}<br>$$</p>
<p>此式恰好与著名的<a href="http://baike.baidu.com/link?url=FWbPfOA5BqiQxrCJ4KJnedzJRf6blsxbh2qn1XyGlPvUbGXufoVXf4xie6UMnWNUUAH2ewNaziQug2YzHQ9jpK" target="_blank" rel="external">Wallis公式</a>有关。</p>
<p>注：\( \lim\limits_{k\to\infty} \left(\frac{2^{2k}(k!)^2}{2k!}\right)^2 \frac{1}{2k+1} = \frac{\pi}{2} \qquad（Wallis公式）\) </p>
<p>Wallis公式是关于圆周率的无穷乘积的公式，Wallis在1665年使用<strong>插值方法</strong>计算半圆曲线\(y=\sqrt{x(1-x)}\)下的面积（即直径为1的半圆面积）时，发现\(\pi/4\)等于下式：</p>
<p>$$<br>\frac{2\cdot4}{3\cdot3} \cdot \frac{4\cdot6}{5\cdot5} \cdot \frac{6\cdot8}{7\cdot7} \cdot \frac{8\cdot10}{9\cdot9} \cdots = \frac{\pi}{4}<br>$$</p>
<p>于是欧拉根据Wallis公式得到如下的结果：</p>
<p>$$<br>\left(\frac{1}{2}\right)! = \frac{\sqrt{\pi}}{2}<br>$$</p>
<p>欧拉发现\((\frac{1}{2})!\)中含有\(\pi\)，而\(\pi\)与圆相关的积分有关系。欧拉猜测\(n!\)一定可以用积分形式表示。</p>
<p>Wallis时代微积分还没问世，当时是使用插值方式做推导计算的，但Wallis公式的推导过程基本上是在处理积分\(\int_{0}^{1} x^\frac{1}{2} (1-x)^\frac{1}{2} dx\)。受Wallis启发，欧拉开始考虑如下一般形式的积分：</p>
<p>$$<br>J(e,n)=\int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>这里，\(n\)为正整数，\(e\)为正实数。利用<strong>分部积分</strong>法，可得：</p>
<p>$$<br>J(e,n)=\frac{n}{e+1}J(e+1, n-1)<br>$$</p>
<p>重复迭代上式，可得：</p>
<p>$$<br>J(e,n) = \frac {1\cdot2\cdot3\cdots n}{(e+1)(e+2)\cdots (e+n+1)}<br>$$</p>
<p>于是欧拉得到如下重要的式子：</p>
<p>$$<br>n!=(e+1)(e+2)\cdots (e+n+1) \int_{0}^{1} x^e(1-x)^n dx<br>$$</p>
<p>接下来，欧拉使用一些计算技巧，即取\(e=f/g\) 且 \(f\to1, g\to0\)，然后对上式右边计算极限，得到如下简洁的结果：</p>
<p>$$<br>n! = \int_{0}^{1}(-\log{t})^n dt<br>$$</p>
<p>到此，欧拉成功地把\(n!\)表达为了积分形式！令\(t=e^{-u}\)，可得常见的Gamma函数形式：</p>
<p>$$<br>n! = \int_{0}^{\infty} u^n e^{-u} du<br>$$</p>
<p>注意，此时的\(n\)仍然为正整数，利用上式把<strong>阶乘延伸至实数集</strong>上，就得到Gamma函数的一般形式：</p>
<p>$$<br>\Gamma(x)=\int_{0}^{1} (-\log t)^{x-1}dt = \int_{0}^{\infty} t^{x-1} e^{-t} dt<br>$$</p>
<p>.</p>
<ul>
<li><strong>为什么</strong>\(\Gamma(n)=(n-1)!\)<strong>，而不是</strong>\(\Gamma(n)=n!\)</li>
</ul>
<p>（未完继续…）</p>
<ul>
<li><p>数学家的成就</p>
<ul>
<li><p>哥德巴赫</p>
<blockquote>
<p>哥德巴赫 于1690年3月18日出生于德国（当时是普鲁士）哥尼斯堡的一个富裕家庭。早年在英国牛津大学学习法学。由于喜欢到处旅游，在欧洲各国访问期间结交了伯努利家族，由此对数学产生兴趣。后来又结交了像欧拉等很多著名数学家，并与他们通信交流问题。他在数学上的研究以<strong>数论</strong>为主。</p>
<ol>
<li><p>1728年，提出了实数集上的数列差值问题，该问题在1929年由欧拉解决，并诞生了著名的Gamma函数。</p>
</li>
<li><p>1742年6月7日，在与好友欧拉的一封信中陈述了著名的<strong>哥德巴赫猜想</strong>：任一大于2的偶数都可以写成两个质数之和。</p>
</li>
</ol>
</blockquote>
</li>
<li><p>伯努利</p>
</li>
<li><p>欧拉</p>
<blockquote>
<p>莱昂哈德·欧拉（Leonhard Euler ，1707年4月15日～1783年9月18日），瑞士数学家、自然科学家。1707年出生于瑞士－巴塞尔的牧师家庭，15岁从巴塞尔大学毕业，翌年获得硕士学位。</p>
<p>欧拉是18世纪数学界最杰出的人物之一，不仅在数学界做出伟大贡献，而且把数学应用到了几乎整个物理领域。简述其成就：</p>
<ol>
<li><p><strong>_微分方程_</strong>: 18世纪中叶，欧拉与其它数学家在解决物理问题过程中，创立了<strong>微分方程</strong>学科。偏微分方程的纯数学研究的第一篇论文是欧拉写的《方程的积分法研究》。此外，还提出了<strong>函数用三角级数表示</strong>的方法和<strong>解微分方程的级数法</strong>等。</p>
</li>
<li><p><strong>_微分几何学_</strong>：引入了空间曲线的参数方程，给出了空间曲线曲率半径的解析表达式；1766年出版了《关于曲面上曲线的研究》，建立了<strong>曲面理论</strong>，是微分几何发展史上的里程碑。</p>
</li>
<li><p><strong>_分析学_</strong>：1729年引入了<strong>Gamma函数和Beta函数</strong>，证明了椭圆积分的加法定理，最早引入了<strong>二重积分</strong>。</p>
</li>
<li><p><strong>_数论_</strong>：欧拉的一系列成果奠定了该数学分支。</p>
</li>
</ol>
<p>后人这样评价欧拉：</p>
<p>著名数学家拉普拉斯（Laplace）说：“读读欧拉，他是所有人的老师”。</p>
<p>数学史上公认的4名最伟大的数学家分别是：<strong>阿基米德、牛顿、欧拉和高斯</strong>。阿基米德有“翘起地球”的豪言壮语，牛顿因为苹果闻名世界，高斯少年时就显露出计算天赋，唯独欧拉没有戏剧性的故事让人印象深刻。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="Gamma分布">Gamma分布</h4><ul>
<li><p>概率密度函数:<br>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {x^{\alpha-1} e^{x/\beta}} {\beta^\alpha \Gamma(\alpha)}, &amp; 0 \leq x \leq \infty; \alpha &gt; 0; \beta &gt; 0 \\\<br>\;\;\quad 0, &amp; others<br>\end{cases}<br>$$</p>
</li>
<li><p>概率密度函数</p>
</li>
</ul>
<h3 id="Beta分布（贝塔分布）">Beta分布（贝塔分布）</h3><p>在@rickjin的《LDA-math》系列中详细地解释了Beta和Dirichlet分布的由来和推导过程，堪称经典。拜读后，受益颇深。原文见下面链接：</p>
<ul>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%831" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(1)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%832" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(2)</a> <br></li>
<li><a href="http://www.52nlp.cn/lda-math-%E8%AE%A4%E8%AF%86betadirichlet%E5%88%86%E5%B8%833" target="_blank" rel="external">LDA-math-认识Beta/Dirichlet分布(3)</a> <br></li>
</ul>
<p>@rickjin在文中从一个<strong>魔鬼的游戏</strong>开始引入。魔鬼撒旦抓走一人，撒旦说：“你们人类很聪明，而我是很仁慈的，和你玩一个游戏，赢了就可以走，否则把灵魂出卖给我。”</p>
<blockquote>
<p>游戏规则：<br><br>我有一个魔盒，上面有一个按钮，你每按一下按钮，就均匀的输出一个[0,1]之间的随机数，我现在按10下，我手上有10个数，你猜第7大的数是什么，偏离不超过0.01就算对。</p>
</blockquote>
<h4 id="数学抽象与推导">数学抽象与推导</h4><p>上述游戏实际在说随机变量\(X_1, X_2, \cdots, X_{10} \sim U(0,1)\), 把这\(n\)个随机变量排序后得到顺序统计量\(X_{(1)}, X_{(2)}, \cdots, X_{(10)}\)，然后问\(X_{(k)}\)的分布是什么？</p>
<blockquote>
<p>因为如果知道随机变量\(X_{(k)}\)分布的概率密度，用概率密度的极值点作为猜测值是最好的策略。</p>
</blockquote>
<p>对于上述游戏而言，\(n=10,k=7\)，\(X_{(k)}\)的分布如何计算？@rickjin在文中列举了通过尝试计算\(X_{(k)}\)落在区间\([x,x+\Delta x]\)的概率，也就是求如下概率值</p>
<p>$$<br>P(x \leq X_{(k)} \leq x + \Delta x) = \;?<br>$$</p>
<p>如果把[0,1]区间分为三个子区间，即\([0,x)、[x,x+\Delta x]和(x+\Delta x,1]\)。考虑简单的情形，假设\(n\)个数中只有一个落在了区间\([x,x+\Delta x]\)内，这个区间内的数\(X_{(k)}\)是第\(k\)大的，则区间\([0,x)\)中应该有\(k-1\)个数，区间\((x+\Delta x,1]\)中应该有\(n-k\)个数。那么，符合上述要求的事件\(E\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E = \\{<br>&amp; X_1 \in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x)\quad (i=2,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<blockquote>
<p>注：</p>
<ol>
<li>符号<code>*</code>在markdown中有特殊含义，若要当作latex环境中的语法用，需要转义，即<code>\ *</code>.</li>
<li>在Mathjax下，<code>$$E = \{x, y\}$$</code>无法正常输出<code>{}</code>, 因为Latex要想输出<code>{}</code>，需要使用<code>\{, \}</code>。而<code>\</code>在markdown中要想正常表法其自身意义也需要再次转义，代码为: <code>$$E = \\{x,y\\}$$</code>。</li>
</ol>
<p>$$E = \\{ x,y\\}$$</p>
</blockquote>
<p>那么事件\(E\)发生的概率，有：</p>
<p>$$<br>\begin{align*}<br>P(E) &amp; = \prod_{i=1}^nP(X_i) \\\<br>&amp; = x^{k-1}(1-x-\Delta x)^{n-k}\Delta x \\\<br>&amp; = x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>\(o(\Delta x)\)表示\(\Delta x\)的高阶无穷小。显然，由于<strong>不同的排列组合</strong>，即\(n\)个数中有一个落在\([x, x+ \Delta x]\)区间的有\(n\)中取法，余下\(n-1\)个数中有\(k-1\)个落在\([0, x)\)区间有\(\binom{n-1}{k-1}\)种组合，所以<strong>与事件E等概率的事件一共有\(n \binom{n-1}{k-1}\)个（当只有1个数落在\([x, x+ \Delta x]\)时）</strong>。</p>
<p>继续考虑复杂一些的情形，假设\(n\)个数中<strong>两个数</strong>落在了区间\([x, x+ \Delta x]\)，此时事件\(E^{‘}\)可表示为：</p>
<p>$$<br>\begin{align*}<br>E’ = \\{<br>&amp; X_1,X_2\in [x, x+\Delta x], \\\<br>&amp; X_i \in [0,x) \quad (i=3,\cdots,k), \\\<br>&amp; X_j \in (x+\Delta x,1] \quad (j=k+1,\cdots,n)<br>\\}<br>\end{align*}<br>$$</p>
<p>此时，事件\(E^{‘}\)发生的概率为：</p>
<p>$$<br>P(E^{‘}) = x^{k-2}(1-x-\Delta x)^{n-k}(\Delta x)^2 = o(\Delta x)<br>$$</p>
<p>很容易看出，只要落在\([x, x+ \Delta x]\)内的数字<strong>大于1个</strong>，则对应事件的概率就是\(\Delta x\)。于是随机变量\(X_{(k)}\)落在\([x, x+ \Delta x]\)区间的概率：</p>
<p>$$<br>\begin{align*}<br>&amp; P( x \le X_{(k)} \le x+\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}P(E) + o(\Delta x) \\\<br>&amp; = n\binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\Delta x + o(\Delta x)<br>\end{align*}<br>$$</p>
<p>进一步可得\(X_{(k)}\)的概率密度函数为</p>
<p>$$<br>\begin{align*}<br>f(x) &amp; = \lim_{\Delta x \to 0} \frac {P(x \leq X_{(x)} \leq x + \Delta x)}{\Delta x} \\\<br>&amp; = n \binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\\\<br>&amp; = \frac {n!}{(k-1)! (n-k)!}x^{k-1} (1-x)^{n-k} \quad x \in [0,1]<br>\end{align*}<br>$$</p>
<p>根据<a href="">神奇的Gamma函数、Beta函数系列</a>可知，利用Gamma函数，可以把上述\(f(x)\)表示为</p>
<p>$$<br>f(x) = \frac {\Gamma(n+1)}{\Gamma(k) \Gamma(n-k+1)} x^{k-1} (1-x)^{n-k}<br>$$</p>
<p><strong>\(\Gamma\)函数起源于人们希望把数学计算从整数集合拓展至实数集合</strong>。这里另\(\alpha=k, \beta=n-k+1\)，于是可以得到</p>
<p>$$<br>Beta(x|\alpha,\beta)=f(x)=\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}<br>$$</p>
<p>上式即为一般意义上的Beta分布！</p>
<h4 id="\(\beta\)型概率分布"><br><strong>\(\beta\)型概率分布</strong></h4><ul>
<li>\(\beta\)型随机变量\(X\)的概率密度函数：</li>
</ul>
<p>$$<br>f(x)=<br>\begin{cases}<br>\displaystyle\frac {\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}, &amp; 0 \leq x \leq 1;\;\alpha &gt; 0, \beta &gt; 0 \\\<br>0, &amp; others<br>\end{cases}<br>\quad (**)<br>$$</p>
<blockquote>
<p>关于<strong>Beta函数详细推导</strong><br></p>
<p>$$<br>\begin{align*}<br>f(x;\alpha,\beta) &amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt} \\\<br>&amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{Beta(\alpha,\beta)}<br>\end{align*}<br>$$</p>
<p>其中，</p>
<p>$$<br>Beta(\alpha,\beta)= \int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}<br>$$</p>
</blockquote>
<ul>
<li>\(\beta\)型随机变量的期望与方差分别是：</li>
</ul>
<p>$$\mu=\frac{\alpha}{\alpha+\beta} \;\qquad\qquad\qquad (期望)$$</p>
<p>$$\sigma^2=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \quad (方差)$$</p>
<blockquote>
<p>关于<strong>Gamma函数</strong></p>
<p>$$<br>\Gamma(\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt<br>$$</p>
<p>且当\(\alpha\)是正整数时，\(\Gamma(\alpha)=(\alpha-1)!\)</p>
</blockquote>
<ul>
<li>\(\beta\)型分布的期望公式详细推导</li>
</ul>
<p>如果\(p \sim Beta(t|\alpha,\beta)\)，那么\(p\)的期望表示如下：</p>
<p>$$<br>\begin{align*}<br>E(p)<br>&amp; =\int_{0}^{1} t \ast Beta(t|\alpha,\beta)\;dt \\\<br>&amp; = \int_{0}^{1} t \ast \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}t^{\alpha-1}(1-t)^{\beta-1}\;dt \\\<br>&amp; =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt<br>\end{align*}<br>$$</p>
<p>上式右边的积分\(\int_{0}^{1}t^{\alpha}(1-t)^{\beta-1}\;dt\)恰好对应到概率分布\(Beta(t|\alpha+1,\beta)\)。对于此分布，根据Beta函数在\([0,1]\)之间的累积分布函数等于1，可得:</p>
<p>$$<br>\int_{0}^{1} \frac{\Gamma(\alpha+\beta+1)}{\Gamma(\alpha+1)\Gamma(\beta)} t^\alpha (1-t)^{\beta-1}\;dt=1<br>$$</p>
<p>把上式带入\(E(p)\)计算式，得到 </p>
<p>$$<br>\begin{align*}<br>E(p) &amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \cdot<br>\frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \\\<br>&amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+1)}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \notag \\\<br>&amp; = \frac{\alpha}{\alpha+\beta}<br>\end{align*}<br>$$</p>
<p>这说明，<strong>对于Beta分布的随机变量，其均值可以用\(\frac{\alpha}{\alpha+\beta}\)来估计</strong>。</p>
<ul>
<li>\(\beta\)型概率分布曲线</li>
</ul>
<p><strong>_(此处暂略~)_</strong></p>
<p>回到<strong>魔鬼的游戏</strong>中，\(n=10, k=7\)时，按照密度分布的峰值去猜测是最好的策略。</p>
<p>$$<br>f(x)=\frac{10!}{(6)!(3)!} x^6 (1-x)^3 \quad x \in [0,1]<br>$$</p>
<p>即便按照密度函数分布的峰值作为猜测结果，第一次猜中的概率也不高。<br><br></p>
<p><strong>游戏继续\(\cdots\)</strong></p>
<p>很遗憾，根据上述最好的策略算出来的值竟然有偏差，没猜中，魔鬼微笑着说：“我再仁慈一点，再给你一个机会，你按5下这个机器，你就得到了5个\([0,1]\)之间的随机数，然后我可以告诉你这5个数中的每一个，和我的第7个数相比，谁大谁小，然后你继续猜我手头上的第7大的数是多少。” 这时候该如何猜测呢？</p>
<h3 id="Beta-Binomial共轭"><br><strong>Beta-Binomial共轭</strong></h3><p>魔鬼的两个问题，数学抽象一下，就是：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \in Uniform(0,1)\)，对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\)，我们要猜测\(p=X_{(k)}\)；（第1个问题）</li>
<li>\(Y_1,Y_2, \cdots, Y_m \in Uniform(0,1)\)，\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大；（第2个问题）</li>
</ul>
<p>最后的问题：<strong>\(P(p|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</strong></p>
<p><strong>问题分析</strong></p>
<p>由于\(p=X_(k)\)在\(X_1,X_2, \cdots, X_n\)中是第\(k\)大的，利用\(Y_i\)的信息，可以很容易的推理得到\(p=X_{(k)}\)在\(X_1,X_2, \cdots, X_n,\) \(Y_1, Y_2, \cdots, Y_m \in Uniform(0,1)\)这\((m+n)\)个独立随机变量中是第\(k+m_1\)大的。那么按照<strong>Beta分布（贝塔分布）</strong>小节的推理，此时\(p=X_{(k)}\)的概率密度函数是\(Beta(p|\,k+m_1, n-k+1+m_2)\)。</p>
<p>根据<strong>贝叶斯推理</strong>的逻辑，整理上述过程如下：</p>
<ul>
<li>\(p=X_{(k)}\)是我们需要猜测的参数，并且推导出\(p\)的分布为\(f(p)=Beta(p|k,n-k+1)\)，称为\(p\)的<strong>先验分布</strong>；</li>
<li>数据\(Y_i\)中有\(m_1\)个比\(p\)小，\(m_2\)个比\(p\)大，\(Y_i\)相当于做了\(m\)次<strong>贝努利试验</strong>，所以\(m_1\)服从二项分布\(B(m,p)\)；</li>
<li>在给定了来自数据提供的\((m_1, m_2)\)的知识后，\(p\)的<strong>后验分布</strong>变为:</li>
</ul>
<p>$$f(p|\,m_1,m_2)=Beta(p|\,k+m_1, n-k+1+m_2)$$</p>
<blockquote>
<p>关于<strong>贝叶斯参数估计</strong>的基本过程：</p>
<p>$$先验分布 ＋ 数据知识 ＝ 后验分布$$</p>
</blockquote>
<p>在这里，贝叶斯分析过程的简单直观的表述就是：</p>
<p>$$Beta(p|\,k, n-k+1) + Count(m_1, m_2) = Beta(p|\,k+m_1,n-k+1+m_2)$$</p>
<p>其中\(m_1,m_2\)对应的是二项分布\(B(m_1+m_2, p)\)的计数。更为一般地数学表述：对于非负实数\(\alpha,\beta\)，存在如下关系</p>
<p>$$<br>Beta(p|\,\alpha, \beta) + Count(m_1, m_2) = Beta(p|\,\alpha+m_1, \beta+m_2) \quad (**)<br>$$</p>
<p>该式描述的就是<strong>Beta-Binomial共轭</strong>。</p>
<blockquote>
<p>关于<strong>Beta-Binomial共轭</strong> <br><br>数据符合<strong>二项分布</strong>的时候，参数的先验分布和后验分布都能保持<strong>Beta分布</strong>的形式，这种形式不变的好处是：我们能够在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续至后验分布中进行解释，同时从先验变换到后验的过程中从数据中补充的知识也容易给出物理上的解释。</p>
</blockquote>
<p>推导过程中可以看到，\(Beta\)分布中的参数\(\alpha,\beta\)都可以理解为物理计数，这两个参数经常被称为<strong>伪计数（pseudo-count）</strong>。\(Beta(p|\,\alpha, \beta)\)可以写成如下表达式：</p>
<p>$$Beta(p|\,1,1) + Count(\alpha-1, \beta-1) = Beta(p|\,\alpha,\beta)$$</p>
<p>其中的<strong>\(Beta(p|\,1,1)\)恰好就是均匀分布\(Uniform(0,1)\)</strong>。</p>
<blockquote>
<p>关于<strong>Beta分布与均匀分布</strong>的关系：</p>
<p>$$Beta(p|\,\alpha=1,\beta=1)= Uniform(a=0,b=1)$$</p>
</blockquote>
<p>对于\((**)\)共轭公式，其实可纯粹从贝叶斯的角度来进行推导。理解过程：</p>
<ul>
<li>假设有一个不均匀的硬币抛出正面的概率为\(p\)，抛\(m\)次后出现正面和反面的次数分别是\(m_1,m_2\)，那么按照传统的<strong>频率学派</strong>观点，\(p\)的估计值应该是\(\hat p = \frac{m_1}{m}\)。<br></li>
<li>而从<strong>贝叶斯学派</strong>的观点来看，开始对硬币不均匀性一无所知，所以应该假设\(p\ \in Uniform(0,1)\)，于是二项分布的计数\(m_1,m_2\)之后，按照贝叶斯公式计算\(p\)的后验分布：</li>
</ul>
<p>$$<br>\begin{align*}<br>P(p|\,m_1,m_2) = &amp; \frac {P(p) \cdot P(m_1,m_2\,|p)}{P(m_1,m_2)} \\\<br>= &amp; \frac {1 \cdot P(m_1,m_2 |p)}{\int_{0}^{1} P(m_1,m_2|t)dt} \\\<br>= &amp; \frac {\binom{m}{m_1}p^{m_1}(1-p)^{m_2}}{\int_{0}^{1} \binom{m}{m_1}t^{m_1}(1-t)^{m_2}dt} \\\<br>= &amp; \frac {p^{m_1}(1-p)^{m_2}}{\int_{0}^{1}t^{m_1}(1-t)^{m_2}dt}<br>\end{align*}<br>$$</p>
<p>得到的后验分布正好是\(Beta(p|\;m_1 + 1, m_2 + 1)\)。<strong>_这里真心没看太懂 …_</strong></p>
<p>回到<strong>魔鬼的游戏</strong>，如果按出的5个随机数字中，魔鬼告诉你有2个（即\(m_1=2\)）小于他手中第7大的数，那么应该按照如下概率分布的<strong>峰值</strong>做猜测是最好的(\(\alpha+2=9,\beta+3=7\))：</p>
<p>$$<br>Beta(x|9,7)=\frac{15!}{(8)!(6)!}x^8(1-x)^6  \quad x \in [0,1]<br>$$</p>
<p>很幸运，这次猜中了，但是魔鬼开始耍赖，游戏不得不继续 \(\cdots\)</p>
<blockquote>
<p>游戏3新规则：<br><br>魔说道：“这个游戏对你来说太简单了，我要加大点难度，我们重新来一次，<strong>我按20下生成20个随机数，你要同时给我猜第7大和第13大的数是什么？</strong>”</p>
</blockquote>
<p>这时候又该如何猜测呢？<br></p>
<h3 id="Dirichlet分布（狄利克雷分布）"><strong>Dirichlet分布（狄利克雷分布）</strong></h3><p>对于魔鬼变本加厉的新的游戏规则，数学抽象如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\),</li>
<li>排序后对应的顺序统计量为\(X_{(1)},X_{(2)}, \cdots, X_{(n)}\),</li>
<li><strong>问: \((X_{(k_1)},X_{(k_1 + k_2)})\)的联合分布是什么？</strong></li>
</ul>
<p>游戏3完全类似游戏1的推导过程，可进行如下的概率计算：</p>
<blockquote>
<p>注：为了公式的简洁性，这里取\(x_3=1-x_1-x_2\),但只有\(x_1,x_2\)是变量。说明符号：<br></p>
<ol>
<li>n: 总的样本数<br></li>
<li>\(X_{(k_1)}\)和\(X_{(k_1 + k_2)}\)将n分割为3段，长度分别为\(k_1, k_2和k_3\)，即<br></li>
</ol>
<p>$$<br>分段结果：<br>\begin{cases}<br>[X_{(1)}, X_{(k-1)}], &amp; 有(k_1-1)个取值，统一记为x_1 \\\<br>[X_{(k_1+1)}, X_{(k_1+k_2)}], &amp; 有(k_2-1)个取值，统一记为x_2 \\\<br>[X_{(k_1+k_2+1)}, X_{(n)}] &amp; 有(n-k_1-k_2)个取值，统一记为x_3<br>\end{cases}<br>$$</p>
</blockquote>
<p>完全类似于游戏1的推导过程，\(X_{(k_1)},X_{(k_1+k_2)}\)的<strong>联合概率</strong>计算如下：</p>
<p>$$<br>\begin{align*}<br>&amp; P\Bigl(X_{(k_1)} \in (x_1, x_1+\Delta x), X_{(k_1+k_2)} \in (x_2, x_2+\Delta x)\Bigr) \\\<br>&amp; \quad = n(n-1)\binom{n-2}{k_1-1,k_2-1}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2 \\\<br>&amp; \quad = \frac{n!}{(k_1-1)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\Delta x)^2<br>\end{align*}<br>$$</p>
<blockquote>
<p>式中的\(n(n-1)\binom{n-2}{k_1-1,k_2-1}\)是一个排列组合问题，比较容易理解。</p>
</blockquote>
<p>于是得到\(X_{(k_1)},X_{(k_1+k_2)}\)的联合分布是：</p>
<p>$$<br>\begin{align*}<br>f(x_1,x_2,x_3) &amp; =\frac{n!}{(k_1-)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2} \\\<br>&amp; = \frac{\Gamma(n+1)}{\Gamma(k_1)\Gamma(k_2)\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}<br>\end{align*} \quad (**)<br>$$</p>
<p>而公式\((**)\)则是3维形式的Dirichlet分布，即\(Dir(x_1,x_2,x_3\;|\;k_1,k_2,n-k_1-k_2+1)\)。这里令\(\alpha_1=k_1,\alpha_2=k_2,\alpha_3=n-k_1-k_2+1\)，于是分布密度函数可以写为：</p>
<p>$$<br>f(x_1, x_2, x_3) = \frac{\Gamma(\alpha_1+\alpha_2+\alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}x_1^{\alpha_1-1}x_2^{\alpha_2-1}x_3^{\alpha_3-1} \qquad (0)<br>$$</p>
<p>公式\((0)\)即为一般形式的3维Dirichlet分布。即使\(\vec{\alpha}=(\alpha_1,\alpha_2,\alpha_3)\)<strong>延伸至非负实数集合</strong>，上述概率分布也是成立的。</p>
<p>在游戏3的基础上还可以往更高的维度上推，譬如\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)中的4、5、…等更多个数，可以得到更高维度的Dirichlet分布和Dirichlet-Multinomial共轭。</p>
<ul>
<li>Dirichlet分布</li>
</ul>
<p>如果\(\vec{p} \sim Dir(\vec{t}|\vec{\alpha})\),那么随机变量\(\vec{p}\)的概率密度函数为</p>
<p>$$<br>\begin{equation}<br>\displaystyle Dir(\overrightarrow{p}|\overrightarrow{\alpha}) =<br>\displaystyle \frac{\Gamma(\sum_{k=1}^{K} \alpha_k)}<br>{\prod_{k=1}^K \Gamma(\alpha_k)} \prod_{k=1}^{K} p_k^{\alpha_k -1}<br>\end{equation}    \qquad (1)<br>$$</p>
<p>对于给定的\(\overrightarrow{p}\)和\(N\)，多项式分布定义为：</p>
<p>$$<br>Multi(\overrightarrow{n}|\overrightarrow{p},N)=<br>\binom{N}{\overrightarrow{n}} \prod_{k=1}^K p_k^{n_k} \qquad (2)<br>$$</p>
<p>而\(Multi(\overrightarrow{n}|\overrightarrow{p},N)\)和\(Dir((\overrightarrow{p}|\overrightarrow{\alpha})\)这两个分布是共轭关系。</p>
<ul>
<li>Dirichlet分布均值</li>
</ul>
<p>$$<br>E(\vec{p}) = \Bigl(\frac{\alpha_1}{\sum_{i=1}^{K}\alpha_i},\frac{\alpha_2}{\sum_{i=1}^{K}\alpha_i}, \cdots, \frac{\alpha_K}{\sum_{i=1}^{K}\alpha_i}\Bigr)<br>$$</p>
<blockquote>
<p>关于<strong>Dirichlet分布与Beta分布</strong>：<br><br>Dirichlet分布是Beta分布在高维度上的推广。当Dirichlet分布维度趋向无限时，便成为Dirichlet过程。<br><br></p>
</blockquote>
<h3 id="Dirichlet-Multinomial共轭"><strong>Dirichlet-Multinomial共轭</strong></h3><p>类似于魔鬼的游戏2，调整一下游戏3，可得游戏4的规则如下：</p>
<blockquote>
<p>游戏4: <br><br>从魔盒中生成m个随机数\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)，魔鬼告诉我们\(Y_i\)和\(X_{(k_1)},X_{(k_1+k_2)}\)相比谁大谁小，然后再次猜测第7大和第13大的数是多少？</p>
</blockquote>
<p>同样，对游戏4进行数学抽象，表示如下：</p>
<ul>
<li>\(X_1,X_2, \cdots, X_n \sim Uniform(0,1)\)，排序后对应的顺序统计量为\(X_{(1)},X_{(2)},\cdots,X_{(n)}\)；</li>
<li>令\(p_1=X_{(k_1)},p_2=X_{(k_1+k_2)},p_3=1-p_1-p_2(p_3是为了简洁的数学表达)\)，我们要猜测\(\vec{p}=(p_1,p_2,p_3)\)；</li>
<li>\(Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\),\(Y_i\)中落到\([0,p_1), [p_1,p_2),[p_2,1]\)三个区间的个数分别是\(m_1,m_2,m_3\),其中\(m=m_1+m_2+m_3\)；</li>
<li>问后验分布\(P(\vec{p}|Y_1,Y_2, \cdots, Y_m)\)的分布是什么？</li>
</ul>
<p>为了计算方便，记：</p>
<p>$$<br>\vec{m}=(m_1, m_2, m_3), \quad \vec{k}=(k_1,k_2,n-k_1-k_2+1)<br>$$</p>
<p>从游戏中的信息可以得知：\(p_1,p_2\)在\(X_1,X_2, \cdots, X_n,Y_1,Y_2, \cdots, Y_m \sim Uniform(0,1)\)这\(m+n\)个数中分别成为了第\(k_1+m_1, k_2+m_2\)大的数，于是后验分布\(P(\vec{p}|Y_1,Y_2,\cdots, Y_m)\)应该是\(Dir(\vec{p}|k_1+m_1,k_2+m_2, n-k_1-k_2+1+m_3)\)，即\(Dir(\vec{p}|\vec{k}+\vec{m})\)。按照贝叶斯推理的逻辑，同样可以把上述过程整理如下：</p>
<ul>
<li>首先，根据游戏3需要猜测参数\(\vec{p}=(p_1,p_2,p_3)\)，其先验分布为\(Dir(\vec{p}|\vec{k})\)；</li>
<li>其次，数据\(Y_i\)落到\([0,p_1),[p_1,p_2),[p_2,1]\)三个区间的个数分别为\(m_1,m_2,m_3\)，所以\(\vec{m}=(m_1, m_2, m_3)\)服从多项式分布\(Multi(\vec{m}|\vec{p})\)<strong>（理解这一点很重要）</strong>；</li>
<li>在给定了来自数据提供的知识\(\vec{m}\)后，\(\vec{p}\)的后验分布变为\(Dir(\vec{p}|\vec{k}+\vec{m})\)。</li>
</ul>
<p>以上贝叶斯分析过程的最简单直接的表述：</p>
<p>$$<br>Dir(\overrightarrow{p}|\overrightarrow{k})+Multi(\vec{m})=Dir(\vec{p}|\vec{k}+\vec{m})<br>$$</p>
<p>令\(\overrightarrow{\alpha}=\vec{k}\)，把\(\vec{\alpha}\)从整数集合延拓至实数集合，可以证明如下关系仍然成立：</p>
<p>$$<br>Dir(\vec{p}|\vec{\alpha})+Multi(\vec{m})=Dir(\vec{p}|\vec{\alpha}+\vec{m}) \quad (11)<br>$$</p>
<p>公式\((11)\)描述就是<strong>Dirichlet-Multinomial共轭</strong>。同时，我们可以把Dirichlet分布中的\(\alpha\)都可以理解为物理计数。那么，类似于Beta分布，\(Dir(\vec{p}|\vec{\alpha})\)可做如下分解：</p>
<p>$$<br>Dir(\vec{p}|\vec{1})+Multi(\vec{m}-\vec{1}) = Dir(\vec{p}|\vec{\alpha})<br>$$</p>
<p>这里\(\vec{1}=(1,1,\cdots,1)\),上式同样可以类似的用纯粹贝叶斯的观点推导和解释。</p>
<p>数学</p>
<hr>
<p>表格(改日修改语法)</p>
<p>$$<br>\begin{array}{c|lcr}<br>n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\<br>\hline<br>1 &amp; 0.24 &amp; 1 &amp; 125 \\<br>2 &amp; -1 &amp; 189 &amp; -8 \\<br>3 &amp; -20 &amp; 2000 &amp; 1+10i \\<br>\end{array}<br>$$</p>
<p>$$<br>f(x)=<br>\begin{cases}<br>n/2, &amp; \text{if \(n\) is even} \\<br>3n+1, &amp; \text{if \(n\) is odd}<br>\end{cases}<br>$$</p>
<p>f(x)=<br>\begin{cases}<br>n/2, &amp; \text{if \(n\) is even} \\\<br>3n+1, &amp; \text{if \(n\) is odd}<br>\end{cases}</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/概率与统计/">概率与统计</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/连续随机变量/">连续随机变量</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/OpenMIT/" title="OpenMIT">OpenMIT<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/分布式机器学习/" title="分布式机器学习">分布式机器学习<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习/图网络/" title="图网络">图网络<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/强化学习与智能决策/" title="强化学习与智能决策">强化学习与智能决策<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/概率与统计/" title="概率与统计">概率与统计<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/深入浅出机器学习/" title="深入浅出机器学习">深入浅出机器学习<sup>9</sup></a></li>
		  
		
		  
			<li><a href="/categories/深度学习/" title="深度学习">深度学习<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/编程语言与技术/" title="编程语言与技术">编程语言与技术<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/计算广告学/" title="计算广告学">计算广告学<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Agent/" title="Agent">Agent<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/参数服务器/" title="参数服务器">参数服务器<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Attention/" title="Attention">Attention<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/DP/" title="DP">DP<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Evalation/" title="Policy Evalation">Policy Evalation<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Improvement/" title="Policy Improvement">Policy Improvement<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Policy-Iteration/" title="Policy Iteration">Policy Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Value-Iteration/" title="Value Iteration">Value Iteration<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Greedy-Policy/" title="Greedy Policy">Greedy Policy<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/连续随机变量/" title="连续随机变量">连续随机变量<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/MDP/" title="MDP">MDP<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Markov-Decision-Process/" title="Markov Decision Process">Markov Decision Process<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/RL/" title="RL">RL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Environments/" title="Environments">Environments<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/GD/" title="GD">GD<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/FTRL/" title="FTRL">FTRL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaGrad/" title="AdaGrad">AdaGrad<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/AdaDelta/" title="AdaDelta">AdaDelta<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Adam/" title="Adam">Adam<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello, Welcome to CAML technology sharing platform.  <br/>
			I&#39;m Zhou Yong, engaged in algorithms work on computational advertising and machine learning.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1707438033" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/ComputationalAds" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:zhouyongsdzh@foxmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2021 
		
		<a href="/about" target="_blank" title="ZhouYong">ZhouYong</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
 </html>
